{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital', 'occupation', 'relationship', 'race', 'gender', 'capgain', 'caploss', 'hours', 'country', 'income']\n",
    "df_train = pd.read_csv('adult.data', names=cols, sep=\",\")\n",
    "df_test = pd.read_csv('adult.test', names=cols, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing** (categorical to numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def preprocess(df):\n",
    "    df.isin(['?']).sum(axis=0)\n",
    "\n",
    "    # replace missing values (?) to nan and then drop the columns\n",
    "    df['country'] = df['country'].replace('?',np.nan)\n",
    "    df['workclass'] = df['workclass'].replace('?',np.nan)\n",
    "    df['occupation'] = df['occupation'].replace('?',np.nan)\n",
    "\n",
    "    # dropping the NaN rows now\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    df['income'] = df['income'].map({'<=50K': 0, '>50K': 1}).astype(int)\n",
    "    df['age'] = df['age'].apply(lambda x : 1 if x >= 45 else 0) # 1 if old, 0 if young\n",
    "    df['workclass'] = df['workclass'].map({'Never-worked': 0, 'Without-pay': 1, 'State-gov': 2, 'Local-gov': 3, 'Federal-gov': 4, 'Self-emp-inc': 5, 'Self-emp-not-inc': 6, 'Private': 7}).astype(int)\n",
    "    df['education'] = df['education'].map({'Preschool': 0, '1st-4th': 1, '5th-6th': 2, '7th-8th': 3, '9th': 4, '10th': 5, '11th': 6, '12th': 7, 'HS-grad': 8, 'Some-college': 9, 'Bachelors': 10, 'Prof-school': 11, 'Assoc-acdm': 12, 'Assoc-voc': 13, 'Masters': 14, 'Doctorate': 15}).astype(int)\n",
    "    df['marital'] = df['marital'].map({'Married-civ-spouse': 1, 'Divorced': 0, 'Never-married': 0, 'Separated': 0, 'Widowed': 0, 'Married-spouse-absent': 1, 'Married-AF-spouse': 1}).astype(int)\n",
    "    df['relationship'] = df['relationship'].map({'Wife': 1 , 'Own-child': 0 , 'Husband': 1, 'Not-in-family': 0, 'Other-relative': 0, 'Unmarried': 0}).astype(int)\n",
    "    df['race'] = df['race'].map({'White': 1, 'Asian-Pac-Islander': 0, 'Amer-Indian-Eskimo': 0, 'Other': 0, 'Black': 0}).astype(int)\n",
    "    df['gender'] = df['gender'].map({'Male': 1, 'Female': 0}).astype(int)\n",
    "    \n",
    "    # process hours\n",
    "    df.loc[(df['hours'] <= 40), 'hours'] = 0\n",
    "    df.loc[(df['hours'] > 40), 'hours'] = 1\n",
    "\n",
    "    df = df.drop(columns=['fnlwgt', 'education.num', 'occupation', 'country', 'capgain', 'caploss'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df_train = preprocess(df_train)\n",
    "df_test = preprocess(df_test)\n",
    "\n",
    "X_train = copy.deepcopy(df_train)\n",
    "X_train = X_train.drop(columns=['income'])\n",
    "y_train = df_train['income']\n",
    "\n",
    "X_test = copy.deepcopy(df_test)\n",
    "X_test = X_test.drop(columns=['income'])\n",
    "y_test = df_test['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Protected, privileged**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protected: 'gender'=0\n",
    "# privileged: 'gender'=1\n",
    "\n",
    "# protected: 'age'=0\n",
    "# privileged: 'age'=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size=500\n",
    "# X_train = X_train[0:size]\n",
    "# y_train = y_train[0:size]\n",
    "\n",
    "X_train_orig = copy.deepcopy(X_train)\n",
    "X_test_orig = copy.deepcopy(X_test)\n",
    "\n",
    "# Scale data: regularization penalty default: ‘l2’, ‘lbfgs’ solvers support only l2 penalties. \n",
    "# Regularization makes the predictor dependent on the scale of the features.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute fairness metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFairness(y_pred, X_test, y_test, metric): \n",
    "    fairnessMetric = 0\n",
    "    protected_idx = X_test[X_test['gender']==0].index\n",
    "    numProtected = len(protected_idx)\n",
    "    privileged_idx = X_test[X_test['gender']==1].index\n",
    "    numPrivileged = len(privileged_idx)\n",
    "        \n",
    "    p_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        p_protected += y_pred[protected_idx[i]][1]\n",
    "    p_protected /= len(protected_idx)\n",
    "    \n",
    "    p_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        p_privileged += y_pred[privileged_idx[i]][1]\n",
    "    p_privileged /= len(privileged_idx)\n",
    "    \n",
    "    # statistical parity difference\n",
    "    statistical_parity = p_protected - p_privileged\n",
    "    \n",
    "    # equality of opportunity, or \n",
    "    # true positive rate parity\n",
    "    # P(Y=1 | Y=1, G=0)- P(Y=1 | Y=1, G=1)\n",
    "    true_positive_protected = 0\n",
    "    actual_positive_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            actual_positive_protected += 1\n",
    "#             if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "            true_positive_protected += y_pred[protected_idx[i]][1]\n",
    "    tpr_protected = true_positive_protected/actual_positive_protected\n",
    "    \n",
    "    true_positive_privileged = 0\n",
    "    actual_positive_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            actual_positive_privileged += 1\n",
    "#             if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "            true_positive_privileged += y_pred[privileged_idx[i]][1]\n",
    "    tpr_privileged = true_positive_privileged/actual_positive_privileged\n",
    "    \n",
    "    tpr_parity = tpr_protected - tpr_privileged\n",
    "    \n",
    "    # equalized odds or TPR parity + FPR parity\n",
    "    # false positive rate parity\n",
    "    \n",
    "    # predictive parity\n",
    "    p_o1_y1_s1 = 0\n",
    "    p_o1_s1 = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "#         if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "        p_o1_s1 += y_pred[protected_idx[i]][1]\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            p_o1_y1_s1 += y_pred[protected_idx[i]][1]\n",
    "    ppv_protected = p_o1_y1_s1/p_o1_s1\n",
    "    \n",
    "    p_o1_y1_s0 = 0\n",
    "    p_o1_s0 = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "#         if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "        p_o1_s0 += y_pred[privileged_idx[i]][1]\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            p_o1_y1_s0 += y_pred[privileged_idx[i]][1]\n",
    "    ppv_privileged = p_o1_y1_s0/p_o1_s0\n",
    "    \n",
    "    predictive_parity = ppv_protected - ppv_privileged\n",
    "    \n",
    "    if (metric == 0):\n",
    "        fairnessMetric = statistical_parity\n",
    "    elif (metric == 1):\n",
    "        fairnessMetric = tpr_parity\n",
    "    elif (metric == 2):\n",
    "        fairnessMetric = predictive_parity\n",
    "        \n",
    "    return fairnessMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Influence of points computed using ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_influence(X_train, y_train, X_test, X_test_orig, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    spd_0 = computeFairness(y_pred, X_test_orig, y_test, 0)\n",
    "\n",
    "    delta_spd = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_removed = np.delete(X_train, i, 0)\n",
    "        y_removed = y_train.drop(index=i, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        delta_spd_i = computeFairness(y_pred, X_test_orig, y_test, 0) - spd_0\n",
    "        delta_spd.append(delta_spd_i)\n",
    "    \n",
    "    return delta_spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function** (Log loss for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if (y_pred[i][1] != 0 and y_pred[i][0] != 0):\n",
    "            loss += - y_true[i] * math.log(y_pred[i][1]) - (1 - y_true[i]) * math.log(y_pred[i][0])\n",
    "    loss /= len(y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Accuracy** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def computeAccuracy(y_true, y_pred):\n",
    "    accuracy = 0\n",
    "    for i in range(len(y_true)):\n",
    "        idx = y_true[i]\n",
    "        if (y_pred[i][idx] > y_pred[i][1 - idx]):\n",
    "            accuracy += 1\n",
    "#         accuracy += y_pred[i][idx]\n",
    "    accuracy /= len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of loss function at z with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_L_del_theta_i(num_params, y_true, x, y_pred):\n",
    "#     del_L_del_theta = np.ones((num_params, 1)) * ((1 - y_true) * y_pred[1] - y_true * y_pred[0])\n",
    "    del_L_del_theta = np.ones((num_params, 1)) * (- y_true + y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_L_del_theta[j] *=  x[j-1]\n",
    "    return del_L_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessian: Second-order partial derivative of loss function with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hessian_one_point(num_params, x, y_pred):\n",
    "#     H = np.ones((num_params, num_params)) * (y_pred[0] * y_pred[1])\n",
    "#     for i in range(1, num_params):\n",
    "#         for j in range(i + 1):\n",
    "#             if j == 0:\n",
    "#                 H[i][j] *= x[i-1]\n",
    "#             else:\n",
    "#                 H[i][j] *= x[i-1] * x[j-1] \n",
    "#     i_lower = np.tril_indices(num_params, -1)\n",
    "#     H.T[i_lower] = H[i_lower]     \n",
    "#     return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of $P(y \\mid \\textbf{x})$ with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_f_del_theta_i(num_params, x, y_pred):\n",
    "    del_f_del_theta = np.ones((num_params, 1)) * (y_pred[0] * y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_f_del_theta[j] *=  x[j-1]\n",
    "    return del_f_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($Statistical parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(SPD)/del(theta)\n",
    "def del_spd_del_theta(num_params, X_test_orig, X_test, y_pred):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    numPrivileged = X_test_orig['gender'].sum()\n",
    "    numProtected = len(X_test_orig) - numPrivileged\n",
    "    for i in range(len(X_test)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[i], y_pred[i])\n",
    "        if X_test_orig.iloc[i]['gender'] == 1: #privileged\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "        elif X_test_orig.iloc[i]['gender'] == 0:\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_privileged /= numPrivileged\n",
    "    del_f_protected /= numProtected\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($TPR parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(TPR_parity)/del(theta)\n",
    "def del_tpr_parity_del_theta(num_params, X_test_orig, X_test, y_pred, y_test):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    \n",
    "    protected_idx = X_test_orig[X_test_orig['gender']==0].index\n",
    "    privileged_idx = X_test_orig[X_test_orig['gender']==1].index\n",
    "\n",
    "    actual_positive_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            actual_positive_privileged += 1\n",
    "#             if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "            del_f_i = del_f_del_theta_i(num_params, X_test[privileged_idx[i]], y_pred[privileged_idx[i]])\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "    del_f_privileged /= actual_positive_privileged\n",
    "    \n",
    "    actual_positive_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            actual_positive_protected += 1\n",
    "#             if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "            del_f_i = del_f_del_theta_i(num_params, X_test[protected_idx[i]], y_pred[protected_idx[i]])\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_protected /= actual_positive_protected\n",
    "\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($Predictive parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(Predictive_parity)/del(theta)\n",
    "def del_predictive_parity_del_theta(num_params, X_test_orig, X_test, y_pred, y_test):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    \n",
    "    protected_idx = X_test_orig[X_test_orig['gender']==0].index\n",
    "    privileged_idx = X_test_orig[X_test_orig['gender']==1].index\n",
    "\n",
    "    u_dash_protected = np.zeros((num_params, 1))\n",
    "    v_protected = 0\n",
    "    v_dash_protected = np.zeros((num_params, 1))\n",
    "    u_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[protected_idx[i]], y_pred[protected_idx[i]])\n",
    "#         if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "        v_protected += y_pred[protected_idx[i]][1]\n",
    "        v_dash_protected = np.add(v_dash_protected, del_f_i)\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            u_dash_protected = np.add(u_dash_protected, del_f_i)\n",
    "            u_protected += y_pred[protected_idx[i]][1]\n",
    "    del_f_protected = (u_dash_protected * v_protected - u_protected * v_dash_protected)/(v_protected * v_protected)\n",
    "    \n",
    "    u_dash_privileged = np.zeros((num_params, 1))\n",
    "    v_privileged = 0\n",
    "    v_dash_privileged = np.zeros((num_params, 1))\n",
    "    u_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[privileged_idx[i]], y_pred[privileged_idx[i]])\n",
    "#         if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "        v_privileged += y_pred[privileged_idx[i]][1]\n",
    "        v_dash_privileged = np.add(v_dash_privileged, del_f_i)\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            u_dash_privileged = np.add(u_dash_privileged, del_f_i)\n",
    "            u_privileged += y_pred[privileged_idx[i]][1]\n",
    "    del_f_privileged = (u_dash_privileged * v_privileged - u_privileged * v_dash_privileged)/(v_privileged * v_privileged)\n",
    "    \n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic estimation of Hessian vector product (involving del fairness): $H_{\\theta}^{-1}v = H_{\\theta}^{-1}\\nabla_{\\theta}f(z, \\theta) = v + [I - \\nabla_{\\theta}^2L(z_{s_j}, \\theta^*)]H_{\\theta}^{-1}v$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uniformly sample t points from training data \n",
    "# def hessian_vector_product(num_params, n, size, v, hessian_all_points):\n",
    "#     if (size > n):\n",
    "#         size = n\n",
    "#     sample = random.sample(range(n), size)\n",
    "#     hinv_v = copy.deepcopy(v)\n",
    "#     for idx in range(size):\n",
    "#         i = sample[idx]\n",
    "#         hessian_i = hessian_all_points[i]\n",
    "#         hinv_v = np.matmul(np.subtract(np.identity(num_params), hessian_i), hinv_v)\n",
    "#         hinv_v = np.add(hinv_v, v)\n",
    "#     return hinv_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def first_order_influence(del_L_del_theta, hinv_v, n):\n",
    "#     infs = []\n",
    "#     for i in range(n):\n",
    "#         inf = -np.dot(del_L_del_theta[i].transpose(), hinv_v)\n",
    "#         inf *= -1/n\n",
    "#         infs.append(inf[0][0].tolist())\n",
    "#     return infs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second-order influence computation for a group of points in subset U**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def second_order_influence(X_train, U, size, del_L_del_theta, hessian_all_points):\n",
    "#     u = len(U)\n",
    "#     s = len(X_train)\n",
    "#     p = u/s\n",
    "#     c1 = (1 - 2*p)/(s * (1-p)**2)\n",
    "#     c2 = 1/((s * (1-p))**2)\n",
    "#     num_params = len(del_L_del_theta[0])\n",
    "#     del_L_del_theta_hinv = np.zeros((num_params, 1))\n",
    "#     del_L_del_theta_sum = np.zeros((num_params, 1))\n",
    "#     hessian_U = np.zeros((num_params, num_params))\n",
    "#     for i in range(u):\n",
    "#         idx = U[i]\n",
    "#         hessian_U = np.add(hessian_U, s * hessian_all_points[idx])\n",
    "#         del_L_del_theta_sum = np.add(del_L_del_theta_sum, del_L_del_theta[idx])\n",
    "    \n",
    "#     hinv_del_L_del_theta= np.matmul(hinv_exact, del_L_del_theta_sum)\n",
    "#     hinv_hessian_U = np.matmul(hinv_exact, hessian_U)\n",
    "#     term1 = c1 * hinv_del_L_del_theta\n",
    "#     term2 = c2 * np.matmul(hinv_hessian_U, hinv_del_L_del_theta)\n",
    "#     sum_term = np.add(term1, term2)\n",
    "#     return sum_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics: Initial state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial statistical parity:  -0.20037734868606946\n",
      "Initial TPR parity:  -0.1735781937702311\n",
      "Initial predictive parity:  -0.16703955662661102\n",
      "Initial loss:  0.39634603985000877\n",
      "Initial accuracy:  0.8065737051792828\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "num_params = len(clf.coef_.transpose()) + 1 #weights and intercept; params: clf.coef_, clf.intercept_\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "y_pred_train = clf.predict_proba(X_train)\n",
    "\n",
    "spd_0 = computeFairness(y_pred_test, X_test_orig, y_test, 0)\n",
    "print(\"Initial statistical parity: \", spd_0)\n",
    "\n",
    "tpr_parity_0 = computeFairness(y_pred_test, X_test_orig, y_test, 1)\n",
    "print(\"Initial TPR parity: \", tpr_parity_0)\n",
    "\n",
    "predictive_parity_0 = computeFairness(y_pred_test, X_test_orig, y_test, 2)\n",
    "print(\"Initial predictive parity: \", predictive_parity_0)\n",
    "\n",
    "loss_0 = logistic_loss(y_test, y_pred_test)\n",
    "print(\"Initial loss: \", loss_0)\n",
    "\n",
    "accuracy_0 = computeAccuracy(y_test, y_pred_test)\n",
    "print(\"Initial accuracy: \", accuracy_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-compute: (1) Hessian (2) del_L_del_theta for each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del_L_del_theta = []\n",
    "# for i in range(int(len(X_train))):\n",
    "#     del_L_del_theta.insert(i, del_L_del_theta_i(num_params, y_train[i], X_train[i], y_pred_train[i]))\n",
    "\n",
    "# hessian_all_points = []\n",
    "# for i in range(len(X_train)):\n",
    "#     hessian_all_points.insert(i, hessian_one_point(num_params, X_train[i], y_pred_train[i])\n",
    "#                               /len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Select delta fairness function depending on selected metric*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 0\n",
    "if metric == 0:\n",
    "    v1 = del_spd_del_theta(num_params, X_test_orig, X_test, y_pred_test)\n",
    "elif metric == 1:\n",
    "    v1 = del_tpr_parity_del_theta(num_params, X_test_orig, X_test, y_pred_test, y_test)\n",
    "elif metric == 2:\n",
    "    v1 = del_predictive_parity_del_theta(num_params, X_test_orig, X_test, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*H^{-1} computation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hexact = 1\n",
    "# if hexact == 1: \n",
    "#     H_exact = np.zeros((num_params, num_params))\n",
    "#     for i in range(len(X_train)):\n",
    "#         H_exact = np.add(H_exact, hessian_all_points[i])\n",
    "#     hinv_exact = np.linalg.pinv(H_exact) \n",
    "#     hinv_v = np.matmul(hinv_exact, v1)\n",
    "# else: #using Hessian vector product\n",
    "#     size_hvp = int(len(X_train) * .01)\n",
    "#     hinv_v = hessian_vector_product(num_params, len(X_train), size_hvp, v1, hessian_all_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence of each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infs_1 = first_order_influence(del_L_del_theta, hinv_v, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_L_del_delta_i(num_params, x, y_pred, params, y_true):\n",
    "    del_L_del_delta = np.ones((num_params - 1, num_params)) * (y_pred[0] * y_pred[1])\n",
    "    for i in range(num_params - 1):\n",
    "        for j in range(num_params):\n",
    "            del_L_del_delta[i][j] *=  params[i]\n",
    "            if j != 0:\n",
    "                del_L_del_delta[i][j] *=  x[j - 1]\n",
    "                if j == i:\n",
    "                    del_L_del_delta[i][j] += y_pred[1] - y_true\n",
    "            \n",
    "    return del_L_del_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair(idx, numIter, learningRate):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_test = clf.predict_proba(X_test)\n",
    "    \n",
    "    X_p = copy.deepcopy(X_train[idx])\n",
    "    y_p_true = copy.deepcopy(y_train[idx]) \n",
    "\n",
    "    del_L_del_delta = np.zeros((num_params - 1, num_params))\n",
    "\n",
    "    random.seed(0) # seed random number generator\n",
    "    delta_new = np.zeros((num_params - 1, 1))\n",
    "    for i in range(len(delta_new)):\n",
    "        delta_new[i] = random.random()\n",
    "    \n",
    "    threshold = 0.05\n",
    "    delta_old = -1 * np.ones((num_params - 1, 1))\n",
    "#     v1 = del_spd_del_theta(num_params, X_test_orig, X_test, y_pred_test)\n",
    "\n",
    "    num_iter = 0\n",
    "    del_L_del_theta_Xp = np.zeros((num_params, 1))\n",
    "    for i in range(len(idx)):\n",
    "        del_L_del_theta_Xp = np.add(del_L_del_theta_Xp, del_L_del_theta_i(num_params, y_train[idx[i]], X_train[idx[i]], y_pred_train[i]))\n",
    "    \n",
    "    obj_old = np.dot(del_L_del_theta_Xp.transpose(), v1)\n",
    "    obj_new = obj_old + 0.1\n",
    "    print(obj_old, obj_new)\n",
    "    \n",
    "#     while (np.linalg.norm(np.subtract(delta_old, delta_new)) > threshold):\n",
    "#         print(np.linalg.norm(np.subtract(delta_old, delta_new)))\n",
    "    while (obj_new > obj_old):\n",
    "#     while True:\n",
    "#     while ((num_iter < 100) or (obj_new - obj_old > threshold)) :\n",
    "        obj_old = obj_new\n",
    "#         print(num_iter)\n",
    "        num_iter += 1\n",
    "        for i in range(len(idx)):\n",
    "            x = np.zeros((len(X_train[idx[i]]), 1))\n",
    "            for j in range(len(x)):\n",
    "                x[j] = X_p[i][j]\n",
    "            x = np.add(x, delta_new)\n",
    "            x_ = [x[i][0] for i in range(len(x))]\n",
    "            y_pred = clf.predict_proba([x_])\n",
    "            del_L_del_delta_i_ = del_L_del_delta_i(num_params, x, y_pred[0], clf.coef_[0], y_train[idx[i]])/len(idx)\n",
    "            del_L_del_delta = np.add(del_L_del_delta, del_L_del_delta_i_)\n",
    "        \n",
    "        delta_old = delta_new\n",
    "        delta_new = np.add(delta_new, (learningRate/num_iter) * np.dot(del_L_del_delta, v1))\n",
    "        \n",
    "        X_train_perturbed = copy.deepcopy(X_train)\n",
    "        for i in range(len(idx)):\n",
    "            for j in range(len(delta_new)):\n",
    "                X_train_perturbed[idx[i]][j] += delta_new[j]\n",
    "            \n",
    "        del_L_del_theta_Xp = np.zeros((num_params, 1))\n",
    "        for i in range(len(idx)):\n",
    "            del_L_del_theta_Xp = np.add(del_L_del_theta_Xp, del_L_del_theta_i(num_params, y_train[idx[i]], X_train_perturbed[idx[i]], y_pred_train[idx[i]]))\n",
    "\n",
    "        obj_new = np.dot(del_L_del_theta_Xp.transpose(), v1)\n",
    "        print(obj_old, obj_new)\n",
    "#         if (numIter>=10):\n",
    "#         if ((num_iter > 10) & (obj_new - obj_old < threshold)):\n",
    "#         if ((num_iter > 2) & (obj_new < obj_old)):\n",
    "        if ((obj_new < obj_old)):\n",
    "#             print((obj_new - obj_old))\n",
    "            return delta_old\n",
    "    \n",
    "    return delta_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40.37473093]] [[40.47473093]]\n",
      "[[40.47473093]] [[8.29286928]]\n"
     ]
    }
   ],
   "source": [
    "idx = X_train_orig[\n",
    "    (X_train_orig['gender'] == 0)\n",
    "    & (X_train_orig['relationship'] == 0)\n",
    "    & (X_train_orig['education'] == 12)\n",
    "    & (X_train_orig['race'] == 1)\n",
    "    ].index \n",
    "# v_pert = repair(idx, numIter, learningRate)\n",
    "v_pert = repair(idx, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennis/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_hessian_update_strategy.py:182: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  warn('delta_grad == 0.0. Check if the approximated '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-61b8a6cccfe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumCols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     res = minimize(f, x0, method='trust-constr', \n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;31m#                jac=rosen_der, hess=rosen_hess,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#                constraints=[linear_constraint, nonlinear_constraint],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    628\u001b[0m                                constraints, callback=callback, **options)\n\u001b[1;32m    629\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trust-constr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         return _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[0m\u001b[1;32m    631\u001b[0m                                             \u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                                             callback=callback, **options)\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_trustregion_constr/minimize_trustregion_constr.py\u001b[0m in \u001b[0;36m_minimize_trustregion_constr\u001b[0;34m(fun, x0, args, grad, hess, hessp, bounds, constraints, xtol, gtol, barrier_tol, sparse_jacobian, callback, maxiter, verbose, finite_diff_rel_step, initial_constr_penalty, initial_tr_radius, initial_barrier_parameter, initial_barrier_tolerance, factorization_method, disp)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tr_interior_point'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         _, result = tr_interior_point(\n\u001b[0m\u001b[1;32m    510\u001b[0m             \u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlagrangian_hess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0mn_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_ineq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_eq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_trustregion_constr/tr_interior_point.py\u001b[0m in \u001b[0;36mtr_interior_point\u001b[0;34m(fun, grad, lagr_hess, n_vars, n_ineq, n_eq, constr, jac, x0, fun0, grad0, constr_ineq0, jac_ineq0, constr_eq0, jac_eq0, stop_criteria, enforce_feasibility, xtol, state, initial_barrier_parameter, initial_tolerance, initial_penalty, initial_trust_radius, factorization_method)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;31m# Compute initial values for next iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mfun0_subprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstr0_subprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_and_constraints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mgrad0_subprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac0_subprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_and_jacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;31m# Get x and s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_trustregion_constr/tr_interior_point.py\u001b[0m in \u001b[0;36mgradient_and_jacobian\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# Compute first derivatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mJ_eq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ_ineq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Return gradient and Jacobian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         return (self._compute_gradient(g),\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_trustregion_constr/canonical_constraint.py\u001b[0m in \u001b[0;36mjac\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0meq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mless\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgreater\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arrayXint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arrayXslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# row.ndim == 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m_get_arrayXslice\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arrayXarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_major_index_fancy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_submatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_get_submatrix\u001b[0;34m(self, major, minor, copy)\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0mi0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0mj0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mj0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mM\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mj1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_process_slice\u001b[0;34m(sl, num)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0mi0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0mi0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'slicing with step != 1 not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res_0 = []\n",
    "res_1 = []\n",
    "res_2 = []\n",
    "res_3 = []\n",
    "res_4 = []\n",
    "res_5 = []\n",
    "res_6 = []\n",
    "res_7 = []\n",
    "\n",
    "X_train_copy = copy.deepcopy(X_train)\n",
    "numCols = len(X_train[0])\n",
    "for ix in range(len(idx)):\n",
    "    X_train_pert = np.zeros((len(X_train[idx[ix]]), 1))\n",
    "    for i in range(len(X_train[idx[ix]])):\n",
    "        X_train_pert[i] = X_train[idx[ix]][i] + v_pert[i]\n",
    "\n",
    "    x0 = np.random.rand(1,numCols)\n",
    "    mins = []\n",
    "    maxs = []\n",
    "    numCols = len(X_train[0])\n",
    "    for i in range(numCols):\n",
    "        mins.insert(i, min(X_train[:,i]))\n",
    "        maxs.insert(i, max(X_train[:,i]))\n",
    "\n",
    "    from scipy.optimize import Bounds, minimize\n",
    "    bounds = Bounds(mins, maxs)\n",
    "\n",
    "    f = lambda x: np.linalg.norm(x - X_train_pert)\n",
    "\n",
    "    x0 = np.random.rand(numCols)\n",
    "    res = minimize(f, x0, method='trust-constr', \n",
    "    #                jac=rosen_der, hess=rosen_hess,\n",
    "    #                constraints=[linear_constraint, nonlinear_constraint],\n",
    "                   options={'verbose': 0}, bounds=bounds)\n",
    "    \n",
    "    for i in range(len(X_train_copy[idx[ix]])):\n",
    "        X_train_copy[idx[ix]][i] += res.x[i]\n",
    "        \n",
    "    res_x_inv_transform = sc.inverse_transform(res.x, copy=None)\n",
    "    res_0.insert(ix, res_x_inv_transform[0])\n",
    "    res_1.insert(ix, res_x_inv_transform[1])\n",
    "    res_2.insert(ix, res_x_inv_transform[2])\n",
    "    res_3.insert(ix, res_x_inv_transform[3])\n",
    "    res_4.insert(ix, res_x_inv_transform[4])\n",
    "    res_5.insert(ix, res_x_inv_transform[5])\n",
    "    res_6.insert(ix, res_x_inv_transform[6])\n",
    "    res_7.insert(ix, res_x_inv_transform[7])\n",
    "#     print(res_x_inv_transform[10], res_x_inv_transform[4], res_x_inv_transform[13], \n",
    "#           res_x_inv_transform[28], res_x_inv_transform[29], res_x_inv_transform[30],\n",
    "#          res_x_inv_transform[17], res_x_inv_transform[18], res_x_inv_transform[19], res_x_inv_transform[20],\n",
    "#          res_x_inv_transform[21], res_x_inv_transform[22], res_x_inv_transform[23], res_x_inv_transform[24],\n",
    "#          res_x_inv_transform[25], res_x_inv_transform[26],\n",
    "#           '\\n')\n",
    "\n",
    "clf.fit(X_train_copy, y_train)\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "\n",
    "print(computeFairness(y_pred_test, X_test_orig, y_test, 0)/spd_0 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "print(statistics.mode(res_6), statistics.mode(res_4)\n",
    "      , statistics.mode(res_2)\n",
    "      , statistics.mode(res_0)\n",
    "     )\n",
    "\n",
    "print(computeFairness(y_pred_test, X_test_orig, y_test, 0)/spd_0 - 1)\n",
    "# print(computeFairness(y_pred_test, X_test_orig, y_test, 0))\n",
    "# print(spd_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
