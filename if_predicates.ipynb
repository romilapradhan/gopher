{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital', 'occupation', 'relationship', 'race', 'gender', 'capgain', 'caploss', 'hours', 'country', 'income']\n",
    "df_train = pd.read_csv('adult.data', names=cols, sep=\",\")\n",
    "df_test = pd.read_csv('adult.test', names=cols, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    " def one_hot_encode(df):\n",
    "    df.isin(['?']).sum(axis=0)\n",
    "\n",
    "    # replace missing values (?) to nan and then drop the columns\n",
    "    df['country'] = df['country'].replace('?',np.nan)\n",
    "    df['workclass'] = df['workclass'].replace('?',np.nan)\n",
    "    df['occupation'] = df['occupation'].replace('?',np.nan)\n",
    "\n",
    "    # dropping the NaN rows now\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    df['income'] = df['income'].map({'<=50K': 0, '>50K': 1}).astype(int)\n",
    "    df = pd.concat([df, pd.get_dummies(df['gender'], prefix='gender')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['race'], prefix='race')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['marital'], prefix='marital')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['workclass'], prefix='workclass')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['relationship'], prefix='relationship')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['occupation'], prefix='occupation')],axis=1)\n",
    "\n",
    "    df = df.drop(columns=['workclass', 'gender', 'fnlwgt', 'education', 'occupation', \\\n",
    "                      'relationship', 'marital', 'race', 'country', 'capgain', \\\n",
    "                      'caploss'])\n",
    "    return df\n",
    "\n",
    "# one-hot encoding (for regression mdoels)\n",
    "df_train = one_hot_encode(df_train)\n",
    "df_test = one_hot_encode(df_test)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Protected, privileged**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protected: 'gender_Female'=1\n",
    "# privileged: 'gender_Male'=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns='income')\n",
    "y_train = df_train['income']\n",
    "\n",
    "X_test = df_test.drop(columns='income')\n",
    "y_test = df_test['income']\n",
    "\n",
    "X_train_orig = copy.deepcopy(X_train)\n",
    "X_test_orig = copy.deepcopy(X_test)\n",
    "\n",
    "# Scale data: regularization penalty default: ‘l2’, ‘lbfgs’ solvers support only l2 penalties. \n",
    "# Regularization makes the predictor dependent on the scale of the features.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute statistical parity difference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFairness(y_pred, X_test): \n",
    "    protected_idx = X_test[X_test['gender_Female']==1].index\n",
    "    numProtected = len(protected_idx)\n",
    "    privileged_idx = X_test[X_test['gender_Male']==1].index\n",
    "    numPrivileged = len(privileged_idx)\n",
    "    \n",
    "    p_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        p_protected += y_pred[protected_idx[i]][1]\n",
    "    p_protected /= len(protected_idx)\n",
    "    \n",
    "    p_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        p_privileged += y_pred[privileged_idx[i]][1]\n",
    "    p_privileged /= len(privileged_idx)\n",
    "    \n",
    "    spd = p_protected - p_privileged\n",
    "    return spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Influence of points computed using ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_influence(X_train, y_train, X_test, X_test_orig):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    spd_0 = computeFairness(y_pred, X_test_orig)\n",
    "\n",
    "    delta_spd = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_removed = np.delete(X_train, i, 0)\n",
    "        y_removed = y_train.drop(index=i, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        delta_spd_i = computeFairness(y_pred, X_test_orig) - spd_0\n",
    "        delta_spd.append(delta_spd_i)\n",
    "    \n",
    "    return delta_spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function** (Log loss for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if (y_pred[i][1] != 0 and y_pred[i][0] != 0):\n",
    "            loss += - y_true[i] * math.log(y_pred[i][1]) - (1 - y_true[i]) * math.log(y_pred[i][0])\n",
    "    loss /= len(y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Accuracy** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAccuracy(y_true, y_pred):\n",
    "    accuracy = 0\n",
    "    for i in range(len(y_true)):\n",
    "        idx = y_true[i]\n",
    "        accuracy += y_pred[i][idx]\n",
    "    accuracy /= len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of loss function at z with respect to model parameters**\n",
    "\n",
    "(Pre-computed for all training points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_L_del_theta_i(num_params, y_true, x, y_pred):\n",
    "#     del_L_del_theta = np.ones((num_params, 1)) * ((1 - y_true) * y_pred[1] - y_true * y_pred[0])\n",
    "    del_L_del_theta = np.ones((num_params, 1)) * (- y_true + y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_L_del_theta[j] *=  x[j-1]\n",
    "    return del_L_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessian: Second-order partial derivative of loss function with respect to model parameters**\n",
    "\n",
    "(Pre-computed for all training points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_one_point(num_params, x, y_pred):\n",
    "    H = np.ones((num_params, num_params)) * (y_pred[0] * y_pred[1])\n",
    "    for i in range(1, num_params):\n",
    "        for j in range(i+1):\n",
    "            if j == 0:\n",
    "                H[i][j] *= x[i-1]\n",
    "            else:\n",
    "                H[i][j] *= x[i-1] * x[j-1] \n",
    "    i_lower = np.tril_indices(num_params, -1)\n",
    "    H.T[i_lower] = H[i_lower]     \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of $P(y \\mid \\textbf{x})$ with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_f_del_theta_i(num_params, x, y_pred):\n",
    "    del_f_del_theta = np.ones((num_params, 1)) * (y_pred[0] * y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_f_del_theta[j] *=  x[j-1]\n",
    "    return del_f_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($Statistical parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(SPD)/del(theta)\n",
    "def del_spd_del_theta(num_params, X_test_orig, X_test, y_pred):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    numProtected = X_test_orig['gender_Female'].sum()\n",
    "    numPrivileged = X_test_orig['gender_Male'].sum()\n",
    "    for i in range(len(X_test)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[i], y_pred[i])\n",
    "        if X_test_orig.iloc[i]['gender_Male'] == 1: #privileged\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "        elif X_test_orig.iloc[i]['gender_Female'] == 1:\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_privileged /= numPrivileged\n",
    "    del_f_protected /= numProtected\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic estimation of Hessian vector product (involving del fairness): $H_{\\theta}^{-1}v = H_{\\theta}^{-1}\\nabla_{\\theta}f(z, \\theta) = v + [I - \\nabla_{\\theta}^2L(z_{s_j}, \\theta^*)]H_{\\theta}^{-1}v$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformly sample t points from training data \n",
    "def hessian_vector_product(num_params, n, size, v, hessian_all_points):\n",
    "    if (size > n):\n",
    "        size = n\n",
    "    sample = random.sample(range(n), size)\n",
    "    hinv_v = copy.deepcopy(v)\n",
    "    for idx in range(size):\n",
    "        i = sample[idx]\n",
    "        hessian_i = hessian_all_points[i]\n",
    "        hinv_v = np.matmul(np.subtract(np.identity(num_params), hessian_i), hinv_v)\n",
    "        hinv_v = np.add(hinv_v, v)\n",
    "    return hinv_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order_influence(del_L_del_theta, hinv_v, n):\n",
    "    infs = []\n",
    "    for i in range(n):\n",
    "        inf = -np.dot(del_L_del_theta[i].transpose(), hinv_v)\n",
    "        inf *= -1/n\n",
    "        infs.append(inf[0][0].tolist())\n",
    "    return infs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second-order Influence function computation**\n",
    "\n",
    "(For any group of points U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_influence(X_train, v1, U, size, del_L_del_theta, hessian_all_points):\n",
    "    u = len(U) \n",
    "    s = len(X_train)\n",
    "    p = u/s\n",
    "    c1 = (1 - 2*p)/(s * (1-p)**2)\n",
    "    c2 = 1/((s * (1-p))**2)\n",
    "    num_params = len(v1)\n",
    "    \n",
    "    del_L_del_theta_hinv = np.zeros((num_params, 1))\n",
    "    del_L_del_theta_sum = np.zeros((num_params, 1))\n",
    "    hessian_all = np.zeros((num_params, num_params))\n",
    "    for i in range(u):\n",
    "        idx = U[i]\n",
    "        del_L_del_theta_hinv = np.add(del_L_del_theta_hinv, hessian_vector_product(num_params, s, size, del_L_del_theta[idx], hessian_all_points))\n",
    "        hessian_all = np.add(hessian_all, hessian_all_points[idx])\n",
    "        del_L_del_theta_sum = np.add(del_L_del_theta_sum, del_L_del_theta[idx])\n",
    "    \n",
    "    term1 = c1 * del_L_del_theta_sum\n",
    "    term2 = c2 * np.dot(hessian_all, del_L_del_theta_hinv)\n",
    "\n",
    "    I = np.dot(v1.transpose(), (term1 + term2))\n",
    "    return (I*(-1)) # multiplied by -1 because removing these points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics: Initial state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial fairness:  -0.20059371090978573\n",
      "Initial loss:  0.360972684923813\n",
      "Initial accuracy:  0.7683939044369612\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.0001\n",
    "clf.fit(X_train, y_train)\n",
    "num_params = len(clf.coef_.transpose()) + 1 #weights and intercept; params: clf.coef_, clf.intercept_\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "y_pred_train = clf.predict_proba(X_train)\n",
    "    \n",
    "spd_0 = computeFairness(y_pred_test, X_test_orig)\n",
    "print(\"Initial fairness: \", spd_0)\n",
    "\n",
    "loss_0 = logistic_loss(y_test, y_pred_test)\n",
    "print(\"Initial loss: \", loss_0)\n",
    "\n",
    "accuracy_0 = computeAccuracy(y_test, y_pred_test)\n",
    "print(\"Initial accuracy: \", accuracy_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-compute: (1) Hessian (2) del_L_del_theta for each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_L_del_theta = []\n",
    "for i in range(int(len(X_train))):\n",
    "    del_L_del_theta.insert(i, del_L_del_theta_i(num_params, y_train[i], X_train[i], y_pred_train[i]))\n",
    "\n",
    "hessian_all_points = []\n",
    "for i in range(len(X_train)):\n",
    "    hessian_all_points.insert(i, hessian_one_point(num_params, X_train[i], y_pred_train[i])\n",
    "                              /len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute: (1) First-order influence, (2) Ground truth influence of each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth influence\n",
    "# spdgt = ground_truth_influence(X_train, y_train, X_test, X_test_orig)\n",
    "# with open('delta_spd_ground_truth_v0.txt', 'w') as filehandle:\n",
    "#     for listitem in delta_spd:\n",
    "#         filehandle.write('%s\\n' % listitem)\n",
    "gt_spd = pd.read_csv('delta_spd_ground_truth_v0.txt', names=[\"Values\"], sep=\",\")\n",
    "gt_spd = gt_spd.values.tolist()\n",
    "spdgt=[]\n",
    "for i in range(len(gt_spd)):\n",
    "    spdgt.append(gt_spd[i][0])\n",
    "sort_index = np.argsort(spdgt)[::-1][:len(spdgt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation between 1st order inf and ground truth inf:  0.9222355399390547\n",
      "Pearson correlation coefficient between 1st order inf and ground truth inf:  0.8903340052474299\n"
     ]
    }
   ],
   "source": [
    "size_hvp = int(len(X_train) * .001)\n",
    "# Hessian vector product H^{-1}v, v = del_fairness\n",
    "v1 = del_spd_del_theta(num_params, X_test_orig, X_test, y_pred_test)\n",
    "# v = del_L_del_theta[3]\n",
    "hinv_v = hessian_vector_product(num_params, len(X_train), size_hvp, v1, hessian_all_points)\n",
    "\n",
    "infs_1 = first_order_influence(del_L_del_theta, hinv_v, len(X_train))\n",
    "print(\"Spearman rank correlation between 1st order inf and ground truth inf: \", \n",
    "      stats.spearmanr(spdgt, infs_1)[0])\n",
    "print(\"Pearson correlation coefficient between 1st order inf and ground truth inf: \", \n",
    "      stats.pearsonr(spdgt, infs_1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Space Partitioner for reducing bias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfluenceOfSet(indices, f, X_df, y_df, X_test_df, X_test_orig_df, method): \n",
    "    del_f = 0\n",
    "    if (method == 1):\n",
    "        X = X_df.drop(index=indices, inplace=False)\n",
    "        y = y_df.drop(index=indices, inplace=False)\n",
    "        if len(y.unique()) < 2:\n",
    "            return 0\n",
    "        clf.fit(X, y)\n",
    "        y_pred = clf.predict_proba(X_test_df)\n",
    "        del_f = computeFairness(y_pred, X_test_orig) - f\n",
    "    elif (method == 2):\n",
    "        for i in range(len(indices)):\n",
    "            del_f += infs_1[indices[i]]\n",
    "    elif (method == 3):\n",
    "        del_f = second_order_influence(X_df.to_numpy(), v1, indices, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "#     del_f = del_f * 100/f\n",
    "    return  round(del_f, 2)\n",
    "\n",
    "def getSplitGain(numLeft, numRight, infLeft, infRight):\n",
    "    gain = max(infLeft, infRight)\n",
    "#     gain = abs(infLeft) + abs(infRight)\n",
    "#     gain = infLeft + infRight\n",
    "    return gain\n",
    "\n",
    "def getAttribute(cols, X_df, y_df, X_test_df, X_train_orig, X_test_orig, method):\n",
    "    splitCol, splitVal, score, left, right, leftInf, rightInf, count = None, np.Inf, 0, None, None, 0, 0, 0\n",
    "    for col in cols:\n",
    "#         print(\"Column: \", col)\n",
    "        if \n",
    "        vals = X_train_orig[col].unique()\n",
    "        vals.sort()\n",
    "        mid = []\n",
    "        for i in range(len(vals) - 1):\n",
    "            mid.append(np.mean(vals[i:i+2]))\n",
    "        for val in mid:\n",
    "#             print(val)\n",
    "            idxLeft = X_train_orig[X_train_orig[col] <= val].index\n",
    "            idxRight = X_train_orig[X_train_orig[col] > val].index\n",
    "            infLeft = getInfluenceOfSet(idxLeft, spd_0, X_df, y_df, X_test_df, X_test_orig, method)\n",
    "            infRight = getInfluenceOfSet(idxRight, spd_0, X_df, y_df, X_test_df, X_test_orig, method)\n",
    "            gain = getSplitGain(len(idxLeft), len(idxRight), infLeft, infRight)\n",
    "#             if abs(gain) > abs(score):\n",
    "            if gain > score:\n",
    "                print(\"Column passed: \", col)\n",
    "                print(\"Gain: \", gain)\n",
    "                splitCol, splitVal, score = col, val, gain \n",
    "                left, right = idxLeft, idxRight\n",
    "                if method==1:\n",
    "                    leftInf, rightInf = infLeft, infRight\n",
    "                else:\n",
    "                    leftInf = getInfluenceOfSet(idxLeft, spd_0, X_df, y_df, X_test_df, X_test_orig, 1)\n",
    "                    rightInf = getInfluenceOfSet(idxRight, spd_0, X_df, y_df, X_test_df, X_test_orig, 1)\n",
    "                count = len(X_train_orig)\n",
    "    return {'col':splitCol, 'val':splitVal, 'count':count,\n",
    "            'idxLeft':left, 'idxRight':right, \n",
    "            'leftInf':leftInf, 'rightInf':rightInf\n",
    "            }\n",
    "\n",
    "def partition(node, maxDepth, minSize, depth, cols, \n",
    "              X_train, y_train, X_train_orig, X_test, X_test_orig, method):\n",
    "    print(\"Depth: \", depth)\n",
    "    X_left = X_train.drop(index=node['idxLeft'], inplace=False)\n",
    "    y_left = y_train.drop(index=node['idxLeft'], inplace=False)\n",
    "    X_train_orig_left = X_train_orig.drop(index=node['idxLeft'], inplace=False)\n",
    "    X_right = X_train.drop(index=node['idxRight'], inplace=False)\n",
    "    y_right = y_train.drop(index=node['idxRight'], inplace=False)\n",
    "    X_train_orig_right = X_train_orig.drop(index=node['idxRight'], inplace=False)\n",
    "    del(node['idxLeft'])\n",
    "    del(node['idxRight'])\n",
    "    if len(X_left)==0 or len(X_right)==0:\n",
    "        node['leftInf'] = node['rightInf'] = node['leftInf'] + node['rightInf']\n",
    "        return\n",
    "    if depth >= maxDepth:\n",
    "        node['leftInf'], node['rightInf'] = node['leftInf'], node['rightInf']\n",
    "        return\n",
    "    if len(X_left) <= minSize:\n",
    "        node['leftInf'] = node['leftInf']\n",
    "    else:\n",
    "        node['left'] = getAttribute(cols, X_left, y_left, X_test, X_train_orig_left, X_test_orig, method)\n",
    "        partition(node['left'], maxDepth, minSize, depth + 1, cols, \n",
    "                  X_train, y_train, X_train_orig_left, X_test, X_test_orig, method)\n",
    "    if len(X_right) <= minSize:\n",
    "        node['rightInf'] = node['rightInf']\n",
    "    else:\n",
    "        node['right'] = getAttribute(cols, X_right, y_right, X_test, X_train_orig_right, X_test_orig, method)\n",
    "        partition(node['right'], maxDepth, minSize, depth + 1, cols, \n",
    "                  X_train, y_train, X_train_orig_right, X_test, X_test_orig, method)\n",
    "\n",
    "def buildTree(X_train, maxDepth, minSize, method):\n",
    "    cols = copy.deepcopy(X_train_orig.columns).tolist()\n",
    "    cols_continuous = ['age', 'hours', 'education.num']\n",
    "    X_train = pd.DataFrame(data=X_train, columns=cols)\n",
    "    cols = list(set(cols)-set(cols_continuous))\n",
    "    root = getAttribute(cols, X_train, y_train, X_test, X_train_orig, X_test_orig, method)\n",
    "    partition(root, maxDepth, minSize, 1, cols, \n",
    "              X_train, y_train, X_train_orig, X_test, X_test_orig, method)\n",
    "    return root\n",
    "\n",
    "method = 1\n",
    "dtree = buildTree(X_train, 2, 20, method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital', 'occupation', 'relationship', 'race', 'gender', 'capgain', 'caploss', 'hours', 'country', 'income']\n",
    "df_train = pd.read_csv('adult.data', names=cols, sep=\",\")\n",
    "df_test = pd.read_csv('adult.test', names=cols, sep=\",\")\n",
    "\n",
    "def preprocess(df):\n",
    "    df.isin(['?']).sum(axis=0)\n",
    "\n",
    "    # replace missing values (?) to nan and then drop the columns\n",
    "    df['country'] = df['country'].replace('?',np.nan)\n",
    "    df['workclass'] = df['workclass'].replace('?',np.nan)\n",
    "    df['occupation'] = df['occupation'].replace('?',np.nan)\n",
    "\n",
    "    # dropping the NaN rows now\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    df['income'] = df['income'].map({'<=50K': 0, '>50K': 1}).astype(int)\n",
    "    df = df.drop(columns=['fnlwgt', 'education.num', 'country', 'capgain', 'caploss'])\n",
    "    return df\n",
    "\n",
    "df_train = preprocess(df_train)\n",
    "df_test = preprocess(df_test)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "X_train_ = df_train.drop(columns='income')\n",
    "y_train_ = df_train['income']\n",
    "\n",
    "X_test_ = df_test.drop(columns='income')\n",
    "y_test_ = df_test['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Gain:  0.001\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Gain:  0.012\n",
      "Column passed:  marital\n",
      "Val:  Married-civ-spouse\n",
      "Gain:  0.172\n",
      "Depth:  1\n",
      "Column passed:  race\n",
      "Val:  White\n",
      "Gain:  0.008\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Gain:  0.145\n",
      "Depth:  2\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Gain:  0.192\n",
      "Depth:  2\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Gain:  0.012\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Gain:  0.023\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Gain:  0.115\n",
      "Depth:  2\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Gain:  0.001\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Gain:  0.01\n",
      "Column passed:  workclass\n",
      "Val:  Private\n",
      "Gain:  0.016\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Gain:  0.101\n",
      "Depth:  2\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Gain:  0.003\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Gain:  0.014\n",
      "Column passed:  workclass\n",
      "Val:  Private\n",
      "Gain:  0.015\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Gain:  0.095\n",
      "Depth:  2\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Gain:  0.011\n",
      "Column passed:  workclass\n",
      "Val:  Private\n",
      "Gain:  0.013\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Gain:  0.074\n",
      "Depth:  2\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Gain:  0.005\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Gain:  0.015\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Gain:  0.085\n",
      "Depth:  2\n"
     ]
    }
   ],
   "source": [
    "def computeFairness(y_pred, X_test): \n",
    "    protected_idx = X_test[X_test['gender']=='Female'].index\n",
    "    numProtected = len(protected_idx)\n",
    "    privileged_idx = X_test[X_test['gender']=='Male'].index\n",
    "    numPrivileged = len(privileged_idx)\n",
    "    \n",
    "    p_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        p_protected += y_pred[protected_idx[i]][1]\n",
    "    p_protected /= len(protected_idx)\n",
    "    \n",
    "    p_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        p_privileged += y_pred[privileged_idx[i]][1]\n",
    "    p_privileged /= len(privileged_idx)\n",
    "    \n",
    "    spd = p_protected - p_privileged\n",
    "    return spd\n",
    "\n",
    "def getInfluenceOfSet(indices, f, X_train, y_train, X_test, X_test_, method): \n",
    "    del_f = 0\n",
    "    if (method == 1):\n",
    "        X = X_train.drop(index=indices, inplace=False)\n",
    "        y = y_train.drop(index=indices, inplace=False)\n",
    "        if len(y.unique()) < 2:\n",
    "            return 0\n",
    "        clf.fit(X, y)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        del_f = computeFairness(y_pred, X_test_) - f\n",
    "#     elif (method == 2):\n",
    "#         for i in range(len(indices)):\n",
    "#             del_f += infs_1[indices[i]]\n",
    "#     elif (method == 3):\n",
    "#         del_f = second_order_influence(X_df.to_numpy(), v1, indices, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "#     del_f = del_f * 100/f\n",
    "    return  round(del_f, 3)\n",
    "\n",
    "def getSplitVal(infs):\n",
    "    return (np.argmax(np.asarray(infs)))\n",
    "\n",
    "def getSplitAttribute(cols, cols_continuous, X_train, y_train, X_test, X_train_, X_test_, method):\n",
    "    splitCol, splitVal, score, numRows = None, None, 0, 0\n",
    "    for col in cols:\n",
    "        if col not in cols_continuous:\n",
    "            vals = X_train_[col].unique()\n",
    "            infs = []\n",
    "            for val in vals:\n",
    "                idx = X_train_[X_train_[col] == val].index\n",
    "                infs.append(getInfluenceOfSet(idx, spd_0, X_train, y_train, X_test, X_test_, method))\n",
    "                ix = getSplitVal(infs)\n",
    "                if infs[ix] > score:\n",
    "                    print(\"Column passed: \", col)\n",
    "                    print(\"Val: \", val)\n",
    "                    print(\"Gain: \", infs[ix])\n",
    "                    splitCol, score, splitVal = col, infs[ix], val\n",
    "                    numRows = len(X_train_)\n",
    "#         else:\n",
    "#         vals = X_train_orig[col].unique()\n",
    "#         vals.sort()\n",
    "#         mid = []\n",
    "#         for i in range(len(vals) - 1):\n",
    "#             mid.append(np.mean(vals[i:i+2]))\n",
    "#         for val in mid:\n",
    "# #             print(val)\n",
    "#             idxLeft = X_train_orig[X_train_orig[col] <= val].index\n",
    "#             idxRight = X_train_orig[X_train_orig[col] > val].index\n",
    "#             infLeft = getInfluenceOfSet(idxLeft, spd_0, X_df, y_df, X_test_df, X_test_orig, method)\n",
    "#             infRight = getInfluenceOfSet(idxRight, spd_0, X_df, y_df, X_test_df, X_test_orig, method)\n",
    "#             gain = getSplitGain(infLeft, infRight)\n",
    "# #             if abs(gain) > abs(score):\n",
    "#             if gain > score:\n",
    "#                 print(\"Column passed: \", col)\n",
    "#                 print(\"Gain: \", gain)\n",
    "#                 splitCol, splitVal, score = col, val, gain \n",
    "#                 left, right = idxLeft, idxRight\n",
    "#                 if method==1:\n",
    "#                     leftInf, rightInf = infLeft, infRight\n",
    "#                 else:\n",
    "#                     leftInf = getInfluenceOfSet(idxLeft, spd_0, X_df, y_df, X_test_df, X_test_orig, 1)\n",
    "#                     rightInf = getInfluenceOfSet(idxRight, spd_0, X_df, y_df, X_test_df, X_test_orig, 1)\n",
    "#                 count = len(X_train_orig)\n",
    "    return {'col':splitCol, 'numRows':numRows}\n",
    "\n",
    "def partition(node, maxDepth, minSize, depth, cols, cols_continuous, \n",
    "              X_train_, y_train_, X_train, X_test_, X_test, method):\n",
    "    print(\"Depth: \", depth)\n",
    "    col = node['col']\n",
    "    if depth >= maxDepth or node['numRows'] < minSize:\n",
    "        node['child'] = None\n",
    "        return\n",
    "    if col not in cols_continuous:\n",
    "        vals = X_train_[col].unique()\n",
    "        child = [None] * len(vals)\n",
    "        for i in range(len(vals)):\n",
    "            idx = X_train_[X_train_[col] == vals[i]].index \n",
    "            X = X_train.drop(index=idx, inplace=False)\n",
    "            y = y_train.drop(index=idx, inplace=False)\n",
    "            X_ = X_train_.drop(index=idx, inplace=False)\n",
    "            if len(X) < minSize:\n",
    "                node['child'] = None\n",
    "            else:\n",
    "                cols_ = copy.deepcopy(cols)\n",
    "                cols_.remove(col)\n",
    "                child[i] = getSplitAttribute(cols_, cols_continuous, \n",
    "                                             X, y, X_test, X_, X_test_, method)\n",
    "                partition(child[i], maxDepth, minSize, depth + 1, cols_, cols_continuous, \n",
    "                  X_, y, X, X_test_, X_test, method)\n",
    "    node['children'] = child\n",
    "\n",
    "def buildTree(X_train_, X_train, maxDepth, minSize, method):\n",
    "    cols = copy.deepcopy(X_train_.columns).tolist()\n",
    "    cols_continuous = ['age', 'hours']\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X_train_orig.columns)\n",
    "    cols = list(set(cols) - set(cols_continuous))\n",
    "    root = getSplitAttribute(cols, cols_continuous, X_train, y_train, X_test, X_train_, X_test_, method)\n",
    "    partition(root, maxDepth, minSize, 1, cols, cols_continuous,\n",
    "              X_train_, y_train_, X_train, X_test_, X_test, method)\n",
    "    return root\n",
    "\n",
    "method = 1\n",
    "dtree = buildTree(X_train_, X_train, 2, 20, method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'col': 'marital',\n",
       " 'numRows': 30162,\n",
       " 'children': [{'col': 'gender', 'numRows': 20436, 'child': None},\n",
       "  {'col': 'occupation', 'numRows': 16097, 'child': None},\n",
       "  {'col': 'gender', 'numRows': 25948, 'child': None},\n",
       "  {'col': 'gender', 'numRows': 29792, 'child': None},\n",
       "  {'col': 'gender', 'numRows': 29223, 'child': None},\n",
       "  {'col': 'gender', 'numRows': 30141, 'child': None},\n",
       "  {'col': 'gender', 'numRows': 29335, 'child': None}]}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Never-married', 'Married-civ-spouse', 'Divorced',\n",
       "       'Married-spouse-absent', 'Separated', 'Married-AF-spouse',\n",
       "       'Widowed'], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_['marital'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking ground truth, first-order and second-order influences for a set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5414\n",
      "Ground truth influence:  -0.0066603768891713155\n"
     ]
    }
   ],
   "source": [
    "predicates = ['marital_Never-married', 'gender_Male']\n",
    "# predicates = ['occupation_Priv-house-serv']\n",
    "# predicates = ['gender_Male']\n",
    "\n",
    "idx = X_train_orig[(X_train_orig[predicates[0]] == 1)\n",
    "                   & (X_train_orig[predicates[1]] == 1) \n",
    "#                    & (X_train_orig[predicates[2]] == 1)\n",
    "#                    & (X_train_orig[predicates[3]] == 1)\n",
    "                  ].index \n",
    "print(len(idx))\n",
    "X = np.delete(X_train, idx, 0)\n",
    "y = y_train.drop(index=idx, inplace=False)\n",
    "clf.fit(X, y)\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "print(\"Ground truth influence: \", computeFairness(y_pred_test, X_test_) - spd_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column:  marital_Married-civ-spouse\n",
      "Ground truth influence:  0.17231515705421363\n",
      "Column:  workclass_Self-emp-inc\n",
      "Ground truth influence:  0.002050567117793084\n",
      "Column:  occupation_Tech-support\n",
      "Ground truth influence:  0.0014802543322554873\n",
      "Column:  marital_Widowed\n",
      "Ground truth influence:  0.0032918920780070915\n",
      "Column:  occupation_Armed-Forces\n",
      "Ground truth influence:  -0.0001606351330849798\n",
      "Column:  occupation_Handlers-cleaners\n",
      "Ground truth influence:  -0.0030429891187805924\n",
      "Column:  marital_Divorced\n",
      "Ground truth influence:  0.011514899383314126\n",
      "Column:  occupation_Sales\n",
      "Ground truth influence:  0.006589790823944591\n",
      "Column:  gender_Female\n",
      "Ground truth influence:  0.0843803045900777\n",
      "Column:  marital_Married-spouse-absent\n",
      "Ground truth influence:  -6.562463764650528e-05\n",
      "Column:  occupation_Priv-house-serv\n",
      "Ground truth influence:  0.0035155109378589566\n",
      "Column:  occupation_Protective-serv\n",
      "Ground truth influence:  0.0016340498266165793\n",
      "Column:  occupation_Machine-op-inspct\n",
      "Ground truth influence:  -0.0009462273701826285\n",
      "Column:  relationship_Own-child\n",
      "Ground truth influence:  -0.0017590046578270235\n",
      "Column:  marital_Never-married\n",
      "Ground truth influence:  -0.007759890940738784\n",
      "Column:  workclass_State-gov\n",
      "Ground truth influence:  -0.0017816691742466029\n",
      "Column:  occupation_Farming-fishing\n",
      "Ground truth influence:  -0.005765441508572833\n",
      "Column:  race_Black\n",
      "Ground truth influence:  0.001994781236516918\n",
      "Column:  marital_Married-AF-spouse\n",
      "Ground truth influence:  -0.00011379073603426382\n",
      "Column:  marital_Separated\n",
      "Ground truth influence:  0.0018376263209688437\n",
      "Column:  workclass_Without-pay\n",
      "Ground truth influence:  -0.0002075749550743411\n",
      "Column:  workclass_Local-gov\n",
      "Ground truth influence:  0.0013416282098084875\n",
      "Column:  workclass_Self-emp-not-inc\n",
      "Ground truth influence:  -0.007413520413749708\n",
      "Column:  relationship_Husband\n",
      "Ground truth influence:  -0.011446196580878448\n",
      "Column:  occupation_Exec-managerial\n",
      "Ground truth influence:  0.011607201733277311\n",
      "Column:  relationship_Unmarried\n",
      "Ground truth influence:  0.002548424148924533\n",
      "Column:  occupation_Transport-moving\n",
      "Ground truth influence:  -0.0015573142141178664\n",
      "Column:  relationship_Wife\n",
      "Ground truth influence:  -0.034951093353242146\n",
      "Column:  occupation_Prof-specialty\n",
      "Ground truth influence:  0.0015910986911271685\n",
      "Column:  workclass_Federal-gov\n",
      "Ground truth influence:  0.0024088247367832216\n",
      "Column:  relationship_Not-in-family\n",
      "Ground truth influence:  -0.006172196252192258\n",
      "Column:  gender_Male\n",
      "Ground truth influence:  0.08002045761292803\n",
      "Column:  race_Other\n",
      "Ground truth influence:  -0.0011475500891248158\n",
      "Column:  relationship_Other-relative\n",
      "Ground truth influence:  0.00016214904688605092\n",
      "Column:  workclass_Private\n",
      "Ground truth influence:  0.016080198794457323\n",
      "Column:  occupation_Other-service\n",
      "Ground truth influence:  -0.0006243881911789928\n",
      "Column:  occupation_Adm-clerical\n",
      "Ground truth influence:  0.0013694625777754266\n",
      "Column:  race_Asian-Pac-Islander\n",
      "Ground truth influence:  -0.001052484592632713\n",
      "Column:  race_Amer-Indian-Eskimo\n",
      "Ground truth influence:  -0.0005114668816826873\n",
      "Column:  occupation_Craft-repair\n",
      "Ground truth influence:  -0.00036035964267222553\n",
      "Column:  race_White\n",
      "Ground truth influence:  0.005329785983341245\n"
     ]
    }
   ],
   "source": [
    "# ground truth predicates\n",
    "predicates = ['marital_Married-civ-spouse']\n",
    "predicates = ['occupation_Priv-house-serv']\n",
    "# predicates = ['gender_Male']\n",
    "\n",
    "cols = copy.deepcopy(X_train_orig.columns).tolist()\n",
    "cols_continuous = ['age', 'hours', 'education.num']\n",
    "cols = list(set(cols)-set(cols_continuous))\n",
    "for i in range(len(cols)):\n",
    "    print(\"Column: \", cols[i])\n",
    "    idx = X_train_orig[(X_train_orig[cols[i]] == 1)].index\n",
    "    X = np.delete(X_train, idx, 0)\n",
    "    y = y_train.drop(index=idx, inplace=False)\n",
    "    clf.fit(X, y)\n",
    "    y_pred_test = clf.predict_proba(X_test)\n",
    "    print(\"Ground truth influence: \", computeFairness(y_pred_test, X_test_orig) - spd_0)\n",
    "\n",
    "# idx = X_train_orig[(X_train_orig[predicates[0]] == 1)\n",
    "# #                    & (X_train_orig[predicates[1]] == 1) \n",
    "# #                    & (X_train_orig[predicates[2]] == 1)\n",
    "# #                    & (X_train_orig[predicates[3]] == 1)\n",
    "#                   ].index \n",
    "\n",
    "# print(len(idx))\n",
    "\n",
    "# X = np.delete(X_train, idx, 0)\n",
    "# y = y_train.drop(index=idx, inplace=False)\n",
    "# clf.fit(X, y)\n",
    "# y_pred_test = clf.predict_proba(X_test)\n",
    "# print(\"Ground truth influence: \", computeFairness(y_pred_test, X_test_orig) - spd_0)\n",
    "\n",
    "# del_f = 0\n",
    "# for i in range(len(idx)):\n",
    "#     del_f += infs_1[idx[i]]\n",
    "# print(\"First-order influence: \", del_f)\n",
    "\n",
    "# print(\"Second-order influence: \", second_order_influence(X_train, hinv_v, idx, size_hvp, del_L_del_theta, hessian_all_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
