{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital', 'occupation', 'relationship', 'race', 'gender', 'capgain', 'caploss', 'hours', 'country', 'income']\n",
    "df_train = pd.read_csv('adult.data', names=cols, sep=\",\")\n",
    "df_test = pd.read_csv('adult.test', names=cols, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def one_hot_encode(df):\n",
    "    df.isin(['?']).sum(axis=0)\n",
    "\n",
    "    # replace missing values (?) to nan and then drop the columns\n",
    "    df['country'] = df['country'].replace('?',np.nan)\n",
    "    df['workclass'] = df['workclass'].replace('?',np.nan)\n",
    "    df['occupation'] = df['occupation'].replace('?',np.nan)\n",
    "\n",
    "    # dropping the NaN rows now\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    df['income'] = df['income'].map({'<=50K': 0, '>50K': 1}).astype(int)\n",
    "    df = pd.concat([df, pd.get_dummies(df['gender'], prefix='gender')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['race'], prefix='race')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['marital'], prefix='marital')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['workclass'], prefix='workclass')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['relationship'], prefix='relationship')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['occupation'], prefix='occupation')],axis=1)\n",
    "\n",
    "    df = df.drop(columns=['workclass', 'gender', 'fnlwgt', 'education', 'occupation', \\\n",
    "                      'relationship', 'marital', 'race', 'country', 'capgain', \\\n",
    "                      'caploss'])\n",
    "    return df\n",
    "\n",
    "# one-hot encoding (for regression mdoels)\n",
    "df_train = one_hot_encode(df_train)\n",
    "df_test = one_hot_encode(df_test)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Protected, privileged**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protected: 'gender_Female'=1\n",
    "# privileged: 'gender_Male'=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns='income')\n",
    "y_train = df_train['income']\n",
    "\n",
    "X_test = df_test.drop(columns='income')\n",
    "y_test = df_test['income']\n",
    "\n",
    "X_train_orig = copy.deepcopy(X_train)\n",
    "X_test_orig = copy.deepcopy(X_test)\n",
    "\n",
    "# Scale data: regularization penalty default: ‘l2’, ‘lbfgs’ solvers support only l2 penalties. \n",
    "# Regularization makes the predictor dependent on the scale of the features.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute fairness metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFairness(y_pred, X_test, y_test, metric): \n",
    "    fairnessMetric = 0\n",
    "    protected_idx = X_test[X_test['gender_Female']==1].index\n",
    "    numProtected = len(protected_idx)\n",
    "    privileged_idx = X_test[X_test['gender_Male']==1].index\n",
    "    numPrivileged = len(privileged_idx)\n",
    "    \n",
    "    p_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        p_protected += y_pred[protected_idx[i]][1]\n",
    "    p_protected /= len(protected_idx)\n",
    "    \n",
    "    p_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        p_privileged += y_pred[privileged_idx[i]][1]\n",
    "    p_privileged /= len(privileged_idx)\n",
    "    \n",
    "    # statistical parity difference\n",
    "    statistical_parity = p_protected - p_privileged\n",
    "    \n",
    "    # equalized odds \n",
    "    # true positive rate parity\n",
    "    # P(Y=1 | Y=1, G=0)- P(Y=1 | Y=1, G=1)\n",
    "    true_positive_protected = 0\n",
    "    actual_positive_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            actual_positive_protected += 1\n",
    "            if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "                true_positive_protected += 1\n",
    "    tpr_protected = true_positive_protected/actual_positive_protected\n",
    "    \n",
    "    true_positive_privileged = 0\n",
    "    actual_positive_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            actual_positive_privileged += 1\n",
    "            if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "                true_positive_privileged += 1\n",
    "    tpr_privileged = true_positive_privileged/actual_positive_privileged\n",
    "    \n",
    "    tpr_parity = tpr_protected - tpr_privileged\n",
    "    \n",
    "    if (metric == 0):\n",
    "        fairnessMetric = statistical_parity\n",
    "    elif (metric == 1):\n",
    "        fairnessMetric = tpr_parity\n",
    "    \n",
    "    # false positive rate parity\n",
    "    \n",
    "    # predictive parity\n",
    "    \n",
    "    return fairnessMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Influence of points computed using ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_influence(X_train, y_train, X_test, X_test_orig, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    spd_0 = computeFairness(y_pred, X_test_orig, y_test, 0)\n",
    "\n",
    "    delta_spd = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_removed = np.delete(X_train, i, 0)\n",
    "        y_removed = y_train.drop(index=i, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        delta_spd_i = computeFairness(y_pred, X_test_orig, y_test, 0) - spd_0\n",
    "        delta_spd.append(delta_spd_i)\n",
    "    \n",
    "    return delta_spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function** (Log loss for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if (y_pred[i][1] != 0 and y_pred[i][0] != 0):\n",
    "            loss += - y_true[i] * math.log(y_pred[i][1]) - (1 - y_true[i]) * math.log(y_pred[i][0])\n",
    "    loss /= len(y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Accuracy** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAccuracy(y_true, y_pred):\n",
    "    accuracy = 0\n",
    "    for i in range(len(y_true)):\n",
    "        idx = y_true[i]\n",
    "        if (y_pred[i][idx] > y_pred[i][1 - idx]):\n",
    "            accuracy += 1\n",
    "#         accuracy += y_pred[i][idx]\n",
    "    accuracy /= len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of loss function at z with respect to model parameters**\n",
    "\n",
    "(Pre-computed for all training points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_L_del_theta_i(num_params, y_true, x, y_pred):\n",
    "#     del_L_del_theta = np.ones((num_params, 1)) * ((1 - y_true) * y_pred[1] - y_true * y_pred[0])\n",
    "    del_L_del_theta = np.ones((num_params, 1)) * (- y_true + y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_L_del_theta[j] *=  x[j-1]\n",
    "    return del_L_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessian: Second-order partial derivative of loss function with respect to model parameters**\n",
    "\n",
    "(Pre-computed for all training points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_one_point(num_params, x, y_pred):\n",
    "    H = np.ones((num_params, num_params)) * (y_pred[0] * y_pred[1])\n",
    "    for i in range(1, num_params):\n",
    "        for j in range(i+1):\n",
    "            if j == 0:\n",
    "                H[i][j] *= x[i-1]\n",
    "            else:\n",
    "                H[i][j] *= x[i-1] * x[j-1] \n",
    "    i_lower = np.tril_indices(num_params, -1)\n",
    "    H.T[i_lower] = H[i_lower]     \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of $P(y \\mid \\textbf{x})$ with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_f_del_theta_i(num_params, x, y_pred):\n",
    "    del_f_del_theta = np.ones((num_params, 1)) * (y_pred[0] * y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_f_del_theta[j] *=  x[j-1]\n",
    "    return del_f_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($Statistical parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(SPD)/del(theta)\n",
    "def del_spd_del_theta(num_params, X_test_orig, X_test, y_pred):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    numProtected = X_test_orig['gender_Female'].sum()\n",
    "    numPrivileged = X_test_orig['gender_Male'].sum()\n",
    "    for i in range(len(X_test)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[i], y_pred[i])\n",
    "        if X_test_orig.iloc[i]['gender_Male'] == 1: #privileged\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "        elif X_test_orig.iloc[i]['gender_Female'] == 1:\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_privileged /= numPrivileged\n",
    "    del_f_protected /= numProtected\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($TPR parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(TPR_parity)/del(theta)\n",
    "def del_tpr_parity_del_theta(num_params, X_test_orig, X_test, y_pred, y_test):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    \n",
    "    protected_idx = X_test[X_test['gender_Female']==1].index\n",
    "    privileged_idx = X_test[X_test['gender_Male']==1].index\n",
    "    \n",
    "    p_y_1_protected = 0\n",
    "    p_y_1_privileged = 0\n",
    "    \n",
    "    true_positive_protected = 0\n",
    "    actual_positive_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            actual_positive_protected += 1\n",
    "            if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "                true_positive_protected += 1\n",
    "    tpr_protected = true_positive_protected/actual_positive_protected\n",
    "    \n",
    "    num_positive_privileged = 0\n",
    "    num_positive_protected = 0\n",
    "    for i in range(len(X_test)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[i], y_pred[i])\n",
    "        if X_test_orig.iloc[i]['gender_Male'] == 1: #privileged\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "        elif X_test_orig.iloc[i]['gender_Female'] == 1:\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_privileged =\n",
    "    del_f_protected =\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic estimation of Hessian vector product (involving del fairness): $H_{\\theta}^{-1}v = H_{\\theta}^{-1}\\nabla_{\\theta}f(z, \\theta) = v + [I - \\nabla_{\\theta}^2L(z_{s_j}, \\theta^*)]H_{\\theta}^{-1}v$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformly sample t points from training data \n",
    "def hessian_vector_product(num_params, n, size, v, hessian_all_points):\n",
    "    if (size > n):\n",
    "        size = n\n",
    "    sample = random.sample(range(n), size)\n",
    "    hinv_v = copy.deepcopy(v)\n",
    "    for idx in range(size):\n",
    "        i = sample[idx]\n",
    "        hessian_i = hessian_all_points[i]\n",
    "        hinv_v = np.matmul(np.subtract(np.identity(num_params), hessian_i), hinv_v)\n",
    "        hinv_v = np.add(hinv_v, v)\n",
    "    return hinv_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order_influence(del_L_del_theta, hinv_v, n):\n",
    "    infs = []\n",
    "    for i in range(n):\n",
    "        inf = -np.dot(del_L_del_theta[i].transpose(), hinv_v)\n",
    "        inf *= -1/n\n",
    "        infs.append(inf[0][0].tolist())\n",
    "    return infs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second-order Influence function computation**\n",
    "\n",
    "(For any group of points U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_influence(X_train, v1, U, size, del_L_del_theta, hessian_all_points):\n",
    "    u = len(U) \n",
    "    s = len(X_train)\n",
    "    p = u/s\n",
    "    c1 = (1 - 2*p)/(s * (1-p)**2)\n",
    "    c2 = 1/((s * (1-p))**2)\n",
    "    num_params = len(v1)\n",
    "    \n",
    "    del_L_del_theta_hinv = np.zeros((num_params, 1))\n",
    "    del_L_del_theta_sum = np.zeros((num_params, 1))\n",
    "    hessian_all = np.zeros((num_params, num_params))\n",
    "    for i in range(u):\n",
    "        idx = U[i]\n",
    "        del_L_del_theta_hinv = np.add(del_L_del_theta_hinv, hessian_vector_product(num_params, s, size, del_L_del_theta[idx], hessian_all_points))\n",
    "        hessian_all = np.add(hessian_all, hessian_all_points[idx])\n",
    "        del_L_del_theta_sum = np.add(del_L_del_theta_sum, del_L_del_theta[idx])\n",
    "    \n",
    "    term1 = c1 * del_L_del_theta_sum\n",
    "    term2 = c2 * np.dot(hessian_all, del_L_del_theta_hinv)\n",
    "\n",
    "    I = np.dot(v1.transpose(), (term1 + term2))\n",
    "    return (I*(-1)) # multiplied by -1 because removing these points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics: Initial state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial fairness:  -0.16734289130157864\n",
      "Initial loss:  0.360972684923813\n",
      "Initial accuracy:  0.8289508632138114\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.0001\n",
    "clf.fit(X_train, y_train)\n",
    "num_params = len(clf.coef_.transpose()) + 1 #weights and intercept; params: clf.coef_, clf.intercept_\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "y_pred_train = clf.predict_proba(X_train)\n",
    "    \n",
    "spd_0 = computeFairness(y_pred_test, X_test_orig, y_test, 1)\n",
    "print(\"Initial fairness: \", spd_0)\n",
    "\n",
    "loss_0 = logistic_loss(y_test, y_pred_test)\n",
    "print(\"Initial loss: \", loss_0)\n",
    "\n",
    "accuracy_0 = computeAccuracy(y_test, y_pred_test)\n",
    "print(\"Initial accuracy: \", accuracy_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-compute: (1) Hessian (2) del_L_del_theta for each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_L_del_theta = []\n",
    "for i in range(int(len(X_train))):\n",
    "    del_L_del_theta.insert(i, del_L_del_theta_i(num_params, y_train[i], X_train[i], y_pred_train[i]))\n",
    "\n",
    "hessian_all_points = []\n",
    "for i in range(len(X_train)):\n",
    "    hessian_all_points.insert(i, hessian_one_point(num_params, X_train[i], y_pred_train[i])\n",
    "                              /len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute: (1) First-order influence, (2) Ground truth influence of each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth influence\n",
    "# spdgt = ground_truth_influence(X_train, y_train, X_test, X_test_orig, y_test)\n",
    "# with open('delta_spd_ground_truth_v0.txt', 'w') as filehandle:\n",
    "#     for listitem in delta_spd:\n",
    "#         filehandle.write('%s\\n' % listitem)\n",
    "gt_spd = pd.read_csv('delta_spd_ground_truth_v0.txt', names=[\"Values\"], sep=\",\")\n",
    "gt_spd = gt_spd.values.tolist()\n",
    "spdgt=[]\n",
    "for i in range(len(gt_spd)):\n",
    "    spdgt.append(gt_spd[i][0])\n",
    "sort_index = np.argsort(spdgt)[::-1][:len(spdgt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation between 1st order inf and ground truth inf:  0.9222355399390547\n",
      "Pearson correlation coefficient between 1st order inf and ground truth inf:  0.8903340052474299\n"
     ]
    }
   ],
   "source": [
    "size_hvp = int(len(X_train) * .001)\n",
    "# Hessian vector product H^{-1}v, v = del_fairness\n",
    "v1 = del_spd_del_theta(num_params, X_test_orig, X_test, y_pred_test)\n",
    "# v = del_L_del_theta[3]\n",
    "hinv_v = hessian_vector_product(num_params, len(X_train), size_hvp, v1, hessian_all_points)\n",
    "\n",
    "infs_1 = first_order_influence(del_L_del_theta, hinv_v, len(X_train))\n",
    "print(\"Spearman rank correlation between 1st order inf and ground truth inf: \", \n",
    "      stats.spearmanr(spdgt, infs_1)[0])\n",
    "print(\"Pearson correlation coefficient between 1st order inf and ground truth inf: \", \n",
    "      stats.pearsonr(spdgt, infs_1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Space Partitioner for reducing bias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital', 'occupation', 'relationship', 'race', 'gender', 'capgain', 'caploss', 'hours', 'country', 'income']\n",
    "df_train = pd.read_csv('adult.data', names=cols, sep=\",\")\n",
    "df_test = pd.read_csv('adult.test', names=cols, sep=\",\")\n",
    "\n",
    "def preprocess(df):\n",
    "    df.isin(['?']).sum(axis=0)\n",
    "\n",
    "    # replace missing values (?) to nan and then drop the columns\n",
    "    df['country'] = df['country'].replace('?',np.nan)\n",
    "    df['workclass'] = df['workclass'].replace('?',np.nan)\n",
    "    df['occupation'] = df['occupation'].replace('?',np.nan)\n",
    "\n",
    "    # dropping the NaN rows now\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    df['income'] = df['income'].map({'<=50K': 0, '>50K': 1}).astype(int)\n",
    "    df = df.drop(columns=['fnlwgt', 'education.num', 'country', 'capgain', 'caploss'])\n",
    "    return df\n",
    "\n",
    "df_train = preprocess(df_train)\n",
    "df_test = preprocess(df_test)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "X_train_ = df_train.drop(columns='income')\n",
    "y_train_ = df_train['income']\n",
    "\n",
    "X_test_ = df_test.drop(columns='income')\n",
    "y_test_ = df_test['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Column passed:  gender\n",
      "Val:  Female\n",
      "Column passed:  marital\n",
      "Val:  Married-civ-spouse\n",
      "{'splitCol': 'marital', 'numRows': 30162, 'infs': [-0.208, -0.028, -0.189, -0.201, -0.199, -0.201, -0.197], 'vals': ['Never-married', 'Married-civ-spouse', 'Divorced', 'Married-spouse-absent', 'Separated', 'Married-AF-spouse', 'Widowed'], 'idxs': [20436, 16097, 25948, 29792, 29223, 30141, 29335]}\n",
      "Depth:  1\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Depth:  2\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Depth:  2\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Depth:  2\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Depth:  2\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Depth:  2\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Depth:  2\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Depth:  2\n"
     ]
    }
   ],
   "source": [
    "def computeFairness(y_pred, X_test): \n",
    "    protected_idx = X_test[X_test['gender']=='Female'].index\n",
    "    numProtected = len(protected_idx)\n",
    "    privileged_idx = X_test[X_test['gender']=='Male'].index\n",
    "    numPrivileged = len(privileged_idx)\n",
    "    \n",
    "    p_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        p_protected += y_pred[protected_idx[i]][1]\n",
    "    p_protected /= len(protected_idx)\n",
    "    \n",
    "    p_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        p_privileged += y_pred[privileged_idx[i]][1]\n",
    "    p_privileged /= len(privileged_idx)\n",
    "    \n",
    "    spd = p_protected - p_privileged\n",
    "    return spd\n",
    "\n",
    "def getInfluenceOfSet(indices, f, X_train, y_train, X_test, X_test_, method): \n",
    "    del_f = 0\n",
    "    if (method == 1):\n",
    "        X = X_train.drop(index=indices, inplace=False)\n",
    "        y = y_train.drop(index=indices, inplace=False)\n",
    "        if len(y.unique()) < 2:\n",
    "            return 0\n",
    "        clf.fit(X, y)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        del_f = computeFairness(y_pred, X_test_)\n",
    "#     elif (method == 2):\n",
    "#         for i in range(len(indices)):\n",
    "#             del_f += infs_1[indices[i]]\n",
    "#     elif (method == 3):\n",
    "#         del_f = second_order_influence(X_df.to_numpy(), v1, indices, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "#     del_f = del_f * 100/f\n",
    "    return  round(del_f, 3)\n",
    "\n",
    "def getSplitVal(infs):\n",
    "    return (np.argmax(np.asarray(infs)))\n",
    "\n",
    "def getSplitAttribute(cols, cols_continuous, X_train, y_train, X_test, X_train_, X_test_, method):\n",
    "    splitCol, numRows, score = None, len(X_train_), np.Inf\n",
    "    infs = []\n",
    "    vals = []\n",
    "    idxs = []\n",
    "    for i in range(len(cols)):\n",
    "        col = cols[i]\n",
    "        infs_i = []\n",
    "        vals_i = []\n",
    "        idxs_i = []\n",
    "        if col not in cols_continuous:\n",
    "            colVals = X_train_[col].unique()\n",
    "            for val in colVals:\n",
    "                idx = X_train_[X_train_[col] == val].index\n",
    "                idxs_i.append(len(X_train)-len(idx))\n",
    "                infs_i.append(getInfluenceOfSet(idx, spd_0, X_train, y_train, X_test, X_test_, method))\n",
    "                vals_i.append(val)\n",
    "                ix = getSplitVal(infs_i)\n",
    "                if (abs(infs_i[ix]) < abs(spd_0)) and (abs(infs_i[ix]) < score):\n",
    "                    print(\"Column passed: \", col)\n",
    "                    print(\"Val: \", val)\n",
    "                    splitCol = col\n",
    "                    splitIdx = i\n",
    "                    score = abs(infs_i[ix])\n",
    "        infs.append(infs_i)\n",
    "        vals.append(vals_i)\n",
    "        idxs.append(idxs_i)\n",
    "    return {'splitCol':splitCol, 'numRows':numRows, \n",
    "            'infs': infs[splitIdx], 'vals': vals[splitIdx], 'idxs': idxs[splitIdx]}\n",
    "#         else:\n",
    "#         vals = X_train_orig[col].unique()\n",
    "#         vals.sort()\n",
    "#         mid = []\n",
    "#         for i in range(len(vals) - 1):\n",
    "#             mid.append(np.mean(vals[i:i+2]))\n",
    "#         for val in mid:\n",
    "# #             print(val)\n",
    "#             idxLeft = X_train_orig[X_train_orig[col] <= val].index\n",
    "#             idxRight = X_train_orig[X_train_orig[col] > val].index\n",
    "#             infLeft = getInfluenceOfSet(idxLeft, spd_0, X_df, y_df, X_test_df, X_test_orig, method)\n",
    "#             infRight = getInfluenceOfSet(idxRight, spd_0, X_df, y_df, X_test_df, X_test_orig, method)\n",
    "#             gain = getSplitGain(infLeft, infRight)\n",
    "# #             if abs(gain) > abs(score):\n",
    "#             if gain > score:\n",
    "#                 print(\"Column passed: \", col)\n",
    "#                 print(\"Gain: \", gain)\n",
    "#                 splitCol, splitVal, score = col, val, gain \n",
    "#                 left, right = idxLeft, idxRight\n",
    "#                 if method==1:\n",
    "#                     leftInf, rightInf = infLeft, infRight\n",
    "#                 else:\n",
    "#                     leftInf = getInfluenceOfSet(idxLeft, spd_0, X_df, y_df, X_test_df, X_test_orig, 1)\n",
    "#                     rightInf = getInfluenceOfSet(idxRight, spd_0, X_df, y_df, X_test_df, X_test_orig, 1)\n",
    "#                 count = len(X_train_orig)    \n",
    "\n",
    "def partition(node, maxDepth, minSize, depth, cols, cols_continuous, \n",
    "              X_train_, y_train_, X_train, X_test_, X_test, method):\n",
    "    print(\"Depth: \", depth)\n",
    "    if depth >= maxDepth or node['numRows'] < minSize:\n",
    "        node['children'] = None\n",
    "        return\n",
    "    col = node['splitCol']\n",
    "    if col not in cols_continuous:\n",
    "        vals = X_train_[col].unique()\n",
    "        child = [None] * len(vals)\n",
    "        for i in range(len(vals)):\n",
    "            idx = X_train_[X_train_[col] == vals[i]].index \n",
    "            X = X_train.drop(index=idx, inplace=False)\n",
    "            y = y_train.drop(index=idx, inplace=False)\n",
    "            X_ = X_train_.drop(index=idx, inplace=False)\n",
    "            if len(X) < minSize:\n",
    "                node['children'] = None\n",
    "            else:\n",
    "                cols_ = copy.deepcopy(cols)\n",
    "                cols_.remove(col)\n",
    "                child[i] = getSplitAttribute(cols_, cols_continuous,\n",
    "                                             X, y, X_test, X_, X_test_, method)\n",
    "                child[i]['col'] = col\n",
    "                child[i]['val'] = vals[i]\n",
    "                partition(child[i], maxDepth, minSize, depth + 1, cols_, cols_continuous, \n",
    "                  X_, y, X, X_test_, X_test, method)\n",
    "    node['children'] = child\n",
    "\n",
    "def buildTree(X_train_, X_train, maxDepth, minSize, method):\n",
    "    cols = copy.deepcopy(X_train_.columns).tolist()\n",
    "    cols_continuous = ['age', 'hours']\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X_train_orig.columns)\n",
    "    cols = list(set(cols) - set(cols_continuous))\n",
    "    root = getSplitAttribute(cols, cols_continuous,\n",
    "                             X_train, y_train, X_test, X_train_, X_test_, method)\n",
    "    print(root)\n",
    "    partition(root, maxDepth, minSize, 1, cols, cols_continuous,\n",
    "              X_train_, y_train_, X_train, X_test_, X_test, method)\n",
    "    return root\n",
    "\n",
    "method = 1\n",
    "dtree = buildTree(X_train_, X_train, 2, 20, method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'splitCol': 'marital',\n",
       " 'numRows': 30162,\n",
       " 'infs': [-0.208, -0.028, -0.189, -0.201, -0.199, -0.201, -0.197],\n",
       " 'vals': ['Never-married',\n",
       "  'Married-civ-spouse',\n",
       "  'Divorced',\n",
       "  'Married-spouse-absent',\n",
       "  'Separated',\n",
       "  'Married-AF-spouse',\n",
       "  'Widowed'],\n",
       " 'idxs': [20436, 16097, 25948, 29792, 29223, 30141, 29335],\n",
       " 'children': [{'splitCol': 'gender',\n",
       "   'numRows': 20436,\n",
       "   'infs': [-0.055, -0.093],\n",
       "   'vals': ['Male', 'Female'],\n",
       "   'idxs': [5470, 14966],\n",
       "   'col': 'marital',\n",
       "   'val': 'Never-married',\n",
       "   'children': None},\n",
       "  {'splitCol': 'occupation',\n",
       "   'numRows': 16097,\n",
       "   'infs': [-0.009,\n",
       "    -0.029,\n",
       "    -0.041,\n",
       "    -0.012,\n",
       "    -0.02,\n",
       "    -0.013,\n",
       "    -0.029,\n",
       "    -0.025,\n",
       "    -0.026,\n",
       "    -0.026,\n",
       "    -0.041,\n",
       "    -0.042,\n",
       "    -0.027,\n",
       "    -0.028],\n",
       "   'vals': ['Adm-clerical',\n",
       "    'Handlers-cleaners',\n",
       "    'Other-service',\n",
       "    'Prof-specialty',\n",
       "    'Sales',\n",
       "    'Farming-fishing',\n",
       "    'Machine-op-inspct',\n",
       "    'Exec-managerial',\n",
       "    'Tech-support',\n",
       "    'Craft-repair',\n",
       "    'Protective-serv',\n",
       "    'Transport-moving',\n",
       "    'Armed-Forces',\n",
       "    'Priv-house-serv'],\n",
       "   'idxs': [13347,\n",
       "    15204,\n",
       "    13584,\n",
       "    14125,\n",
       "    14144,\n",
       "    15683,\n",
       "    15100,\n",
       "    14500,\n",
       "    15584,\n",
       "    14591,\n",
       "    15832,\n",
       "    15507,\n",
       "    16091,\n",
       "    15969],\n",
       "   'col': 'marital',\n",
       "   'val': 'Married-civ-spouse',\n",
       "   'children': None},\n",
       "  {'splitCol': 'gender',\n",
       "   'numRows': 25948,\n",
       "   'infs': [-0.085, -0.092],\n",
       "   'vals': ['Male', 'Female'],\n",
       "   'idxs': [7253, 18695],\n",
       "   'col': 'marital',\n",
       "   'val': 'Divorced',\n",
       "   'children': None},\n",
       "  {'splitCol': 'gender',\n",
       "   'numRows': 29792,\n",
       "   'infs': [-0.099, -0.127],\n",
       "   'vals': ['Male', 'Female'],\n",
       "   'idxs': [9593, 20199],\n",
       "   'col': 'marital',\n",
       "   'val': 'Married-spouse-absent',\n",
       "   'children': None},\n",
       "  {'splitCol': 'gender',\n",
       "   'numRows': 29223,\n",
       "   'infs': [-0.106, -0.112],\n",
       "   'vals': ['Male', 'Female'],\n",
       "   'idxs': [9208, 20015],\n",
       "   'col': 'marital',\n",
       "   'val': 'Separated',\n",
       "   'children': None},\n",
       "  {'splitCol': 'gender',\n",
       "   'numRows': 30141,\n",
       "   'infs': [-0.126, -0.128],\n",
       "   'vals': ['Male', 'Female'],\n",
       "   'idxs': [9770, 20371],\n",
       "   'col': 'marital',\n",
       "   'val': 'Married-AF-spouse',\n",
       "   'children': None},\n",
       "  {'splitCol': 'gender',\n",
       "   'numRows': 29335,\n",
       "   'infs': [-0.116, -0.117],\n",
       "   'vals': ['Male', 'Female'],\n",
       "   'idxs': [9096, 20239],\n",
       "   'col': 'marital',\n",
       "   'val': 'Widowed',\n",
       "   'children': None}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking ground truth, first-order and second-order influences for a set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Ground truth influence:  -0.20043790718523896\n",
      "-0.20059371090978573\n"
     ]
    }
   ],
   "source": [
    "predicates = ['marital_Married-civ-spouse', 'occupation_Armed-Forces']\n",
    "# predicates = ['marital_Widowed']\n",
    "# predicates = ['gender_Male']\n",
    "\n",
    "idx = X_train_orig[(X_train_orig[predicates[0]] == 1)\n",
    "                   & (X_train_orig[predicates[1]] == 1) \n",
    "#                    & (X_train_orig[predicates[2]] == 1)\n",
    "#                    & (X_train_orig[predicates[3]] == 1)\n",
    "                  ].index \n",
    "print(len(idx))\n",
    "X = np.delete(X_train, idx, 0)\n",
    "y = y_train.drop(index=idx, inplace=False)\n",
    "clf.fit(X, y)\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "print(\"Ground truth influence: \", computeFairness(y_pred_test, X_test_, y_test, 0))\n",
    "print(spd_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
