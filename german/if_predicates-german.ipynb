{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['status', 'duration', 'credit_hist', 'purpose', 'credit_amt', 'savings', 'employment', 'install_rate', 'personal_status', 'debtors', 'residence', 'property', 'age', 'install_plans', 'housing', 'num_credits', 'job', 'num_liable', 'telephone', 'foreign_worker', 'credit']\n",
    "df = pd.read_table('german.data', names=cols, sep=\" \", index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing** (categorical to numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    " def preprocess(df):\n",
    "    df['status'] = df['status'].map({'A11': 0, 'A12': 1, 'A13': 2, 'A14': 3}).astype(int)\n",
    "    \n",
    "    df.loc[(df['duration'] <= 12), 'duration'] = 0\n",
    "    df.loc[(df['duration'] > 12) & (df['duration'] <= 24), 'duration'] = 1\n",
    "    df.loc[(df['duration'] > 24) & (df['duration'] <= 36), 'duration'] = 2\n",
    "    df.loc[(df['duration'] > 36), 'duration'] = 3    \n",
    "    \n",
    "    df['credit_hist'] = df['credit_hist'].map({'A34': 0, 'A33': 1, 'A32': 2, 'A31': 3, 'A30': 4}).astype(int)    \n",
    "    df = pd.concat([df, pd.get_dummies(df['purpose'], prefix='purpose')],axis=1)\n",
    "\n",
    "    df.loc[(df['credit_amt'] <= 2000), 'credit_amt'] = 0\n",
    "    df.loc[(df['credit_amt'] > 2000) & (df['credit_amt'] <= 5000), 'credit_amt'] = 1\n",
    "    df.loc[(df['credit_amt'] > 5000), 'credit_amt'] = 2    \n",
    "    \n",
    "    df['savings'] = df['savings'].map({'A61': 0, 'A62': 1, 'A63': 2, 'A64': 3, 'A65': 4}).astype(int)\n",
    "    df['employment'] = df['employment'].map({'A71': 0, 'A72': 1, 'A73': 2, 'A74': 3, 'A75': 4}).astype(int)    \n",
    "    df['gender'] = df['personal_status'].map({'A91': 1, 'A92': 0, 'A93': 1, 'A94': 1, 'A95': 0}).astype(int)\n",
    "    df['debtors'] = df['debtors'].map({'A101': 0, 'A102': 1, 'A103': 2}).astype(int)\n",
    "    df['property'] = df['property'].map({'A121': 3, 'A122': 2, 'A123': 1, 'A124': 0}).astype(int)        \n",
    "    df['age'] = df['age'].apply(lambda x : 1 if x >= 45 else 0) # 1 if old, 0 if young\n",
    "    df['install_plans'] = df['install_plans'].map({'A141': 1, 'A142': 1, 'A143': 0}).astype(int)\n",
    "    df = pd.concat([df, pd.get_dummies(df['housing'], prefix='housing')],axis=1)\n",
    "    df['job'] = df['job'].map({'A171': 0, 'A172': 1, 'A173': 2, 'A174': 3}).astype(int)    \n",
    "    df['telephone'] = df['telephone'].map({'A191': 0, 'A192': 1}).astype(int)\n",
    "    df['foreign_worker'] = df['foreign_worker'].map({'A201': 1, 'A202': 0}).astype(int)\n",
    "    \n",
    "    df['credit'] = df['credit'].replace(2, 0) #1 = Good, 2= Bad credit risk\n",
    "\n",
    "#     process age\n",
    "#     df.loc[(df['age'] >= 15) & (df['age'] <= 24) , 'age'] = 0\n",
    "#     df.loc[(df['age'] >= 25) & (df['age'] <= 34) , 'age'] = 1\n",
    "#     df.loc[(df['age'] >= 35) & (df['age'] <= 44) , 'age'] = 2\n",
    "#     df.loc[(df['age'] >= 45) & (df['age'] <= 54) , 'age'] = 3\n",
    "#     df.loc[(df['age'] >= 55) & (df['age'] <= 64) , 'age'] = 4\n",
    "#     df.loc[(df['age'] >= 65) , 'age'] = 5\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess(df)\n",
    "\n",
    "y = df['credit']\n",
    "df = df.drop(columns=['purpose', 'personal_status', 'housing', 'credit'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=1)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Protected, privileged**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protected: 'gender'=0\n",
    "# privileged: 'gender'=1\n",
    "\n",
    "# protected: 'age'=0\n",
    "# privileged: 'age'=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size=500\n",
    "# X_train = X_train[0:size]\n",
    "# y_train = y_train[0:size]\n",
    "\n",
    "X_train_orig = copy.deepcopy(X_train)\n",
    "X_test_orig = copy.deepcopy(X_test)\n",
    "\n",
    "# Scale data: regularization penalty default: ‘l2’, ‘lbfgs’ solvers support only l2 penalties. \n",
    "# Regularization makes the predictor dependent on the scale of the features.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute fairness metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFairness(y_pred, X_test, y_test, metric): \n",
    "    fairnessMetric = 0\n",
    "    protected_idx = X_test[X_test['age']==0].index\n",
    "    numProtected = len(protected_idx)\n",
    "    privileged_idx = X_test[X_test['age']==1].index\n",
    "    numPrivileged = len(privileged_idx)\n",
    "        \n",
    "    p_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        p_protected += y_pred[protected_idx[i]][1]\n",
    "    p_protected /= len(protected_idx)\n",
    "    \n",
    "    p_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        p_privileged += y_pred[privileged_idx[i]][1]\n",
    "    p_privileged /= len(privileged_idx)\n",
    "    \n",
    "    # statistical parity difference\n",
    "    statistical_parity = p_protected - p_privileged\n",
    "    \n",
    "    # equality of opportunity, or \n",
    "    # true positive rate parity\n",
    "    # P(Y=1 | Y=1, G=0)- P(Y=1 | Y=1, G=1)\n",
    "    true_positive_protected = 0\n",
    "    actual_positive_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            actual_positive_protected += 1\n",
    "#             if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "            true_positive_protected += y_pred[protected_idx[i]][1]\n",
    "    tpr_protected = true_positive_protected/actual_positive_protected\n",
    "    \n",
    "    true_positive_privileged = 0\n",
    "    actual_positive_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            actual_positive_privileged += 1\n",
    "#             if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "            true_positive_privileged += y_pred[privileged_idx[i]][1]\n",
    "    tpr_privileged = true_positive_privileged/actual_positive_privileged\n",
    "    \n",
    "    tpr_parity = tpr_protected - tpr_privileged\n",
    "    \n",
    "    # equalized odds or TPR parity + FPR parity\n",
    "    # false positive rate parity\n",
    "    \n",
    "    # predictive parity\n",
    "    p_o1_y1_s1 = 0\n",
    "    p_o1_s1 = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "#         if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "        p_o1_s1 += y_pred[protected_idx[i]][1]\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            p_o1_y1_s1 += y_pred[protected_idx[i]][1]\n",
    "    ppv_protected = p_o1_y1_s1/p_o1_s1\n",
    "    \n",
    "    p_o1_y1_s0 = 0\n",
    "    p_o1_s0 = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "#         if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "        p_o1_s0 += y_pred[privileged_idx[i]][1]\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            p_o1_y1_s0 += y_pred[privileged_idx[i]][1]\n",
    "    ppv_privileged = p_o1_y1_s0/p_o1_s0\n",
    "    \n",
    "    predictive_parity = ppv_protected - ppv_privileged\n",
    "    \n",
    "    if (metric == 0):\n",
    "        fairnessMetric = statistical_parity\n",
    "    elif (metric == 1):\n",
    "        fairnessMetric = tpr_parity\n",
    "    elif (metric == 2):\n",
    "        fairnessMetric = predictive_parity\n",
    "        \n",
    "    return fairnessMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Influence of points computed using ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_influence(X_train, y_train, X_test, X_test_orig, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    spd_0 = computeFairness(y_pred, X_test_orig, y_test, 0)\n",
    "\n",
    "    delta_spd = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_removed = np.delete(X_train, i, 0)\n",
    "        y_removed = y_train.drop(index=i, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        delta_spd_i = computeFairness(y_pred, X_test_orig, y_test, 0) - spd_0\n",
    "        delta_spd.append(delta_spd_i)\n",
    "    \n",
    "    return delta_spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function** (Log loss for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if (y_pred[i][1] != 0 and y_pred[i][0] != 0):\n",
    "            loss += - y_true[i] * math.log(y_pred[i][1]) - (1 - y_true[i]) * math.log(y_pred[i][0])\n",
    "    loss /= len(y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Accuracy** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def computeAccuracy(y_true, y_pred):\n",
    "    accuracy = 0\n",
    "    for i in range(len(y_true)):\n",
    "        idx = y_true[i]\n",
    "        if (y_pred[i][idx] > y_pred[i][1 - idx]):\n",
    "            accuracy += 1\n",
    "#         accuracy += y_pred[i][idx]\n",
    "    accuracy /= len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of loss function at z with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_L_del_theta_i(num_params, y_true, x, y_pred):\n",
    "#     del_L_del_theta = np.ones((num_params, 1)) * ((1 - y_true) * y_pred[1] - y_true * y_pred[0])\n",
    "    del_L_del_theta = np.ones((num_params, 1)) * (- y_true + y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_L_del_theta[j] *=  x[j-1]\n",
    "    return del_L_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessian: Second-order partial derivative of loss function with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_one_point(num_params, x, y_pred):\n",
    "    H = np.ones((num_params, num_params)) * (y_pred[0] * y_pred[1])\n",
    "    for i in range(1, num_params):\n",
    "        for j in range(i + 1):\n",
    "            if j == 0:\n",
    "                H[i][j] *= x[i-1]\n",
    "            else:\n",
    "                H[i][j] *= x[i-1] * x[j-1] \n",
    "    i_lower = np.tril_indices(num_params, -1)\n",
    "    H.T[i_lower] = H[i_lower]     \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of $P(y \\mid \\textbf{x})$ with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_f_del_theta_i(num_params, x, y_pred):\n",
    "    del_f_del_theta = np.ones((num_params, 1)) * (y_pred[0] * y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_f_del_theta[j] *=  x[j-1]\n",
    "    return del_f_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($Statistical parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(SPD)/del(theta)\n",
    "def del_spd_del_theta(num_params, X_test_orig, X_test, y_pred):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    numPrivileged = X_test_orig['age'].sum()\n",
    "    numProtected = len(X_test_orig) - numPrivileged\n",
    "    for i in range(len(X_test)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[i], y_pred[i])\n",
    "        if X_test_orig.iloc[i]['age'] == 1: #privileged\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "        elif X_test_orig.iloc[i]['age'] == 0:\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_privileged /= numPrivileged\n",
    "    del_f_protected /= numProtected\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($TPR parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(TPR_parity)/del(theta)\n",
    "def del_tpr_parity_del_theta(num_params, X_test_orig, X_test, y_pred, y_test):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    \n",
    "    protected_idx = X_test_orig[X_test_orig['age']==0].index\n",
    "    privileged_idx = X_test_orig[X_test_orig['age']==1].index\n",
    "\n",
    "    actual_positive_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            actual_positive_privileged += 1\n",
    "#             if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "            del_f_i = del_f_del_theta_i(num_params, X_test[privileged_idx[i]], y_pred[privileged_idx[i]])\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "    del_f_privileged /= actual_positive_privileged\n",
    "    \n",
    "    actual_positive_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            actual_positive_protected += 1\n",
    "#             if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "            del_f_i = del_f_del_theta_i(num_params, X_test[protected_idx[i]], y_pred[protected_idx[i]])\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_protected /= actual_positive_protected\n",
    "\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($Predictive parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(Predictive_parity)/del(theta)\n",
    "def del_predictive_parity_del_theta(num_params, X_test_orig, X_test, y_pred, y_test):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    \n",
    "    protected_idx = X_test_orig[X_test_orig['age']==0].index\n",
    "    privileged_idx = X_test_orig[X_test_orig['age']==1].index\n",
    "\n",
    "    u_dash_protected = np.zeros((num_params, 1))\n",
    "    v_protected = 0\n",
    "    v_dash_protected = np.zeros((num_params, 1))\n",
    "    u_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[protected_idx[i]], y_pred[protected_idx[i]])\n",
    "#         if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "        v_protected += y_pred[protected_idx[i]][1]\n",
    "        v_dash_protected = np.add(v_dash_protected, del_f_i)\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            u_dash_protected = np.add(u_dash_protected, del_f_i)\n",
    "            u_protected += y_pred[protected_idx[i]][1]\n",
    "    del_f_protected = (u_dash_protected * v_protected - u_protected * v_dash_protected)/(v_protected * v_protected)\n",
    "    \n",
    "    u_dash_privileged = np.zeros((num_params, 1))\n",
    "    v_privileged = 0\n",
    "    v_dash_privileged = np.zeros((num_params, 1))\n",
    "    u_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[privileged_idx[i]], y_pred[privileged_idx[i]])\n",
    "#         if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "        v_privileged += y_pred[privileged_idx[i]][1]\n",
    "        v_dash_privileged = np.add(v_dash_privileged, del_f_i)\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            u_dash_privileged = np.add(u_dash_privileged, del_f_i)\n",
    "            u_privileged += y_pred[privileged_idx[i]][1]\n",
    "    del_f_privileged = (u_dash_privileged * v_privileged - u_privileged * v_dash_privileged)/(v_privileged * v_privileged)\n",
    "    \n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic estimation of Hessian vector product (involving del fairness): $H_{\\theta}^{-1}v = H_{\\theta}^{-1}\\nabla_{\\theta}f(z, \\theta) = v + [I - \\nabla_{\\theta}^2L(z_{s_j}, \\theta^*)]H_{\\theta}^{-1}v$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformly sample t points from training data \n",
    "def hessian_vector_product(num_params, n, size, v, hessian_all_points):\n",
    "    if (size > n):\n",
    "        size = n\n",
    "    sample = random.sample(range(n), size)\n",
    "    hinv_v = copy.deepcopy(v)\n",
    "    for idx in range(size):\n",
    "        i = sample[idx]\n",
    "        hessian_i = hessian_all_points[i]\n",
    "        hinv_v = np.matmul(np.subtract(np.identity(num_params), hessian_i), hinv_v)\n",
    "        hinv_v = np.add(hinv_v, v)\n",
    "    return hinv_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order_influence(del_L_del_theta, hinv_v, n):\n",
    "    infs = []\n",
    "    for i in range(n):\n",
    "        inf = -np.dot(del_L_del_theta[i].transpose(), hinv_v)\n",
    "        inf *= -1/n\n",
    "        infs.append(inf[0][0].tolist())\n",
    "    return infs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second-order influence computation for a group of points in subset U**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_influence(X_train, U, size, del_L_del_theta, hessian_all_points):\n",
    "    u = len(U)\n",
    "    s = len(X_train)\n",
    "    p = u/s\n",
    "    c1 = (1 - 2*p)/(s * (1-p)**2)\n",
    "    c2 = 1/((s * (1-p))**2)\n",
    "    num_params = len(del_L_del_theta[0])\n",
    "    del_L_del_theta_hinv = np.zeros((num_params, 1))\n",
    "    del_L_del_theta_sum = np.zeros((num_params, 1))\n",
    "    hessian_U = np.zeros((num_params, num_params))\n",
    "    for i in range(u):\n",
    "        idx = U[i]\n",
    "        hessian_U = np.add(hessian_U, s * hessian_all_points[idx])\n",
    "        del_L_del_theta_sum = np.add(del_L_del_theta_sum, del_L_del_theta[idx])\n",
    "    \n",
    "    hinv_del_L_del_theta= np.matmul(hinv_exact, del_L_del_theta_sum)\n",
    "    hinv_hessian_U = np.matmul(hinv_exact, hessian_U)\n",
    "    term1 = c1 * hinv_del_L_del_theta\n",
    "    term2 = c2 * np.matmul(hinv_hessian_U, hinv_del_L_del_theta)\n",
    "    sum_term = np.add(term1, term2)\n",
    "    return sum_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics: Initial state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial statistical parity:  -0.11218759952324076\n",
      "Initial TPR parity:  -0.08639444415122699\n",
      "Initial predictive parity:  -0.09396577739948753\n",
      "Initial loss:  0.5063649711386483\n",
      "Initial accuracy:  0.755\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "num_params = len(clf.coef_.transpose()) + 1 #weights and intercept; params: clf.coef_, clf.intercept_\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "y_pred_train = clf.predict_proba(X_train)\n",
    "\n",
    "spd_0 = computeFairness(y_pred_test, X_test_orig, y_test, 0)\n",
    "print(\"Initial statistical parity: \", spd_0)\n",
    "\n",
    "tpr_parity_0 = computeFairness(y_pred_test, X_test_orig, y_test, 1)\n",
    "print(\"Initial TPR parity: \", tpr_parity_0)\n",
    "\n",
    "predictive_parity_0 = computeFairness(y_pred_test, X_test_orig, y_test, 2)\n",
    "print(\"Initial predictive parity: \", predictive_parity_0)\n",
    "\n",
    "loss_0 = logistic_loss(y_test, y_pred_test)\n",
    "print(\"Initial loss: \", loss_0)\n",
    "\n",
    "accuracy_0 = computeAccuracy(y_test, y_pred_test)\n",
    "print(\"Initial accuracy: \", accuracy_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-compute: (1) Hessian (2) del_L_del_theta for each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_L_del_theta = []\n",
    "for i in range(int(len(X_train))):\n",
    "    del_L_del_theta.insert(i, del_L_del_theta_i(num_params, y_train[i], X_train[i], y_pred_train[i]))\n",
    "\n",
    "hessian_all_points = []\n",
    "for i in range(len(X_train)):\n",
    "    hessian_all_points.insert(i, hessian_one_point(num_params, X_train[i], y_pred_train[i])\n",
    "                              /len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Select delta fairness function depending on selected metric*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 2\n",
    "if metric == 0:\n",
    "    v1 = del_spd_del_theta(num_params, X_test_orig, X_test, y_pred_test)\n",
    "elif metric == 1:\n",
    "    v1 = del_tpr_parity_del_theta(num_params, X_test_orig, X_test, y_pred_test, y_test)\n",
    "elif metric == 2:\n",
    "    v1 = del_predictive_parity_del_theta(num_params, X_test_orig, X_test, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*H^{-1} computation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexact = 1\n",
    "if hexact == 1: \n",
    "    H_exact = np.zeros((num_params, num_params))\n",
    "    for i in range(len(X_train)):\n",
    "        H_exact = np.add(H_exact, hessian_all_points[i])\n",
    "    hinv_exact = np.linalg.pinv(H_exact) \n",
    "    hinv_v = np.matmul(hinv_exact, v1)\n",
    "else: #using Hessian vector product\n",
    "    size_hvp = int(len(X_train) * .01)\n",
    "    hinv_v = hessian_vector_product(num_params, len(X_train), size_hvp, v1, hessian_all_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence of each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "infs_1 = first_order_influence(del_L_del_theta, hinv_v, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking ground truth, first-order and second-order influences for a set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical parity \n",
      "#Rows removed:  67\n",
      "#Rows left:  733\n",
      "Ground truth influence of subset (on statistical parity):  0.0450982036707821\n",
      "Ground truth influence of subset (on tpr parity):  0.021053889053461172\n",
      "Ground truth influence of subset (on predictive parity):  0.019288838268330277\n",
      "First-order influence:  0.04166645112626196\n",
      "Second-order influence:  0.047501068997333265\n",
      "Ground truth statistical parity after removing subset:  -0.08610961744429924\n",
      "Ground truth tpr parity after removing subset:  -0.23207230495614894\n",
      "Ground truth predictive parity after removing subset:  -0.23207230495614894\n",
      "Loss after removing subset:  0.4968544758453933\n",
      "Accuracy after removing subset:  0.755\n"
     ]
    }
   ],
   "source": [
    "if metric == 0:\n",
    "    print(\"Statistical parity \")\n",
    "elif metric == 1:\n",
    "    print(\"True positive rate parity \")\n",
    "elif metric == 2:\n",
    "    print(\"Predictive parity\")\n",
    "    \n",
    "active = 1\n",
    "if active:\n",
    "    predicates = [ 'age']\n",
    "    idx=X_train_orig.index \n",
    "    for pred in predicates:\n",
    "       idx0 = X_train_orig[(X_train_orig[pred] == 1)].index \n",
    "       idx=idx.intersection(idx0)\n",
    "      \n",
    "    print(\"#Rows removed: \", len(idx))\n",
    "    print(\"#Rows left: \", len(X_train) - len(idx))\n",
    "    X = np.delete(X_train, idx, 0)\n",
    "    y = y_train.drop(index=idx, inplace=False)\n",
    "    clf.fit(X, y)\n",
    "    y_pred_test = clf.predict_proba(X_test)\n",
    "    print(\"Ground truth influence of subset (on statistical parity): \", computeFairness(y_pred_test, X_test_orig, y_test, 0) - spd_0)\n",
    "    print(\"Ground truth influence of subset (on tpr parity): \", computeFairness(y_pred_test, X_test_orig, y_test, 1) - tpr_parity_0)\n",
    "    print(\"Ground truth influence of subset (on predictive parity): \", computeFairness(y_pred_test, X_test_orig, y_test, 2) - predictive_parity_0)\n",
    "\n",
    "    del_f_1 = 0\n",
    "    for i in range(len(idx)):\n",
    "        del_f_1 += infs_1[idx[i]]\n",
    "    print(\"First-order influence: \", del_f_1)\n",
    "\n",
    "    size_hvp = 1\n",
    "    params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "    del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "    print(\"Second-order influence: \", del_f_2)\n",
    "    \n",
    "    spd_1 = computeFairness(y_pred_test, X_test_orig, y_test, 0)\n",
    "    print(\"Ground truth statistical parity after removing subset: \", spd_1)\n",
    "    \n",
    "    tpr_parity_1 = computeFairness(y_pred_test, X_test_orig, y_test, 1)\n",
    "    print(\"Ground truth tpr parity after removing subset: \", tpr_parity_1)\n",
    "\n",
    "    predictive_parity_1 = computeFairness(y_pred_test, X_test_orig, y_test, 1)\n",
    "    print(\"Ground truth predictive parity after removing subset: \", predictive_parity_1)\n",
    "\n",
    "    loss_1 = logistic_loss(y_test, y_pred_test)\n",
    "    print(\"Loss after removing subset: \", loss_1)\n",
    "\n",
    "    accuracy_1 = computeAccuracy(y_test, y_pred_test)\n",
    "    print(\"Accuracy after removing subset: \", accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.20044233691822827 -0.17393358242162493 0.3597551530603338 0.8310092961487384\n",
      "-0.1225835505238218 -0.01100904749147602 0.3789946670357515 0.8205179282868525\n"
     ]
    }
   ],
   "source": [
    "print(spd_0, tpr_parity_0, preditive_parity_0, loss_0, accuracy_0)\n",
    "print(spd_1, tpr_parity_1, predictive_parity_1, loss_1, accuracy_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fairness: Ground-truth subset influence vs. computed subset influences: Random subset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-truth subset, Add 1st-order inf individual, Second-order subset influence\n"
     ]
    }
   ],
   "source": [
    "print(\"Ground-truth subset, Add 1st-order inf individual, Second-order subset influence\")\n",
    "sampleSize = int(.2 * len(X_train))\n",
    "for i in range(100):\n",
    "    idx = random.sample(range(1, len(X_train)), sampleSize) \n",
    "    \n",
    "    # Ground truth subset influence\n",
    "    X = np.delete(X_train, idx, 0)\n",
    "    y = y_train.drop(index=idx, inplace=False)\n",
    "    clf.fit(X, y)\n",
    "    y_pred_test = clf.predict_proba(X_test)\n",
    "    inf_gt = computeFairness(y_pred_test, X_test_orig, y_test, 0) - spd_0\n",
    "    \n",
    "    # First-order subset influence\n",
    "    del_f_1 = 0\n",
    "    for i in range(len(idx)):\n",
    "        del_f_1 += infs_1[idx[i]]\n",
    "    \n",
    "    # Second-order subset influence\n",
    "    size_hvp = 1\n",
    "    params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "    del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "    \n",
    "#     print(inf_gt, del_f_1, del_f_2, sep=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fairness: Ground-truth subset influence vs. computed subset influences: Coherent subset** \n",
    "\n",
    "(by coherent, we mean group of data points that share some properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute, Value, Ground-truth subset, Add 1st-order inf individual, Second-order subset influence, %rowsRemoved, Accuracy\n",
      "status, 3, 0.004439428399044432, 0.0010781288227304144, 0.004297819415713518, 0.39125, 0.755\n",
      "status, 2, -0.001210510917164065, -0.000992893112670721, -0.0010752234934097368, 0.06125, 0.765\n",
      "status, 0, 0.006830357499232886, 0.0019490983748598548, 0.004974289135839417, 0.27625, 0.75\n",
      "status, 1, -0.0014147502762921205, -0.002296310557521478, -0.0023602352786543388, 0.27125, 0.725\n",
      "duration, 1, 0.0011286600324059703, -0.0016256360172768834, 0.0015999614820016122, 0.42375, 0.745\n",
      "duration, 0, 0.00274381819014069, 0.002507299164637925, 0.0037139881905710577, 0.35125, 0.73\n",
      "duration, 2, 0.003873438120326367, 0.0029603629558223885, 0.004024410841070304, 0.1425, 0.76\n",
      "duration, 3, -0.0038772852854099815, -0.004104002575785356, -0.00425150315466431, 0.0825, 0.76\n",
      "credit_hist, 2, 0.002839912891081209, 0.0007969517669278989, 0.004263316088178447, 0.53, 0.74\n",
      "credit_hist, 0, -0.01047844200070458, -0.006334203563064673, -0.009351122975267512, 0.2875, 0.755\n",
      "credit_hist, 4, 0.0009962327355886869, 0.0009350441986633715, 0.001073284224889187, 0.04, 0.76\n",
      "credit_hist, 3, 0.0008244740693928998, 0.0010160311212018863, 0.0011589989341657665, 0.05, 0.755\n",
      "credit_hist, 1, 0.004543840914713493, 0.003324200003669588, 0.0038840941450419744, 0.0925, 0.77\n",
      "credit_amt, 0, -0.004187933031241764, -0.00010276114181848796, 0.001895000478153117, 0.44, 0.71\n",
      "credit_amt, 1, -0.010196842128261774, -0.007092560614421053, -0.00763611179628589, 0.38125, 0.77\n",
      "credit_amt, 2, 0.007067725448877349, 0.006933345283637608, 0.009983553057597048, 0.17875, 0.73\n",
      "savings, 4, 0.007506819794179109, 0.004069452312681432, 0.005822307134195958, 0.1875, 0.755\n",
      "savings, 1, 0.0005709028974734132, 0.00018445933248685515, 0.000706054246924037, 0.09625, 0.755\n",
      "savings, 2, 4.910693273185718e-05, 9.13454651433842e-05, 4.192662016291762e-05, 0.0625, 0.76\n",
      "savings, 0, -0.020840263867311193, -0.005882445689695267, -0.019651057957707597, 0.6025, 0.71\n",
      "savings, 3, 0.001152118055214979, 0.0012752121067816718, 0.0012770969118108544, 0.05125, 0.755\n",
      "employment, 3, -0.0007013356785832148, -0.0008547465051368758, -0.00038418226372244433, 0.17125, 0.75\n",
      "employment, 4, 0.009537573974937374, 0.0059599592635360766, 0.009747318499025222, 0.2525, 0.775\n",
      "employment, 2, 0.005667756800330159, 0.0008425061932540041, 0.0010361642368996032, 0.34375, 0.765\n",
      "employment, 0, -0.005649896613918681, -0.004564277338634108, -0.005998244892052862, 0.0675, 0.76\n",
      "employment, 1, -0.003375103717227934, -0.001645418085621026, -0.003003037378420126, 0.165, 0.745\n",
      "install_rate, 4, -0.005626381217371024, -0.0020898290931453166, -0.0039164531896954, 0.47625, 0.75\n",
      "install_rate, 1, 0.009298734752174909, 0.008266945919408459, 0.010200670829369466, 0.13875, 0.75\n",
      "install_rate, 2, -0.006831716788984643, -0.004281560606508836, -0.005832048053193466, 0.23125, 0.745\n",
      "install_rate, 3, -0.002233770225823495, -0.0021575326923562385, -0.002669579675763893, 0.15375, 0.755\n",
      "debtors, 0, 0.08512702672244477, 0.003201005025095974, 0.09355542687766034, 0.90625, 0.56\n",
      "debtors, 2, 0.005256069827338283, -0.000542762247534656, 0.0007207779570328904, 0.0525, 0.745\n",
      "debtors, 1, -0.0014744882900694645, -0.002920219250163252, -0.002459755093485, 0.04125, 0.765\n",
      "residence, 4, -0.001171084119686694, -9.256928544352323e-06, 0.0014149205917232443, 0.40875, 0.755\n",
      "residence, 3, 0.014182163281894589, 0.006924881597674756, 0.00944934130167509, 0.1575, 0.76\n",
      "residence, 1, 0.0009572788936973442, -0.00018066796662970244, 0.00013624934840381162, 0.1225, 0.755\n",
      "residence, 2, -0.009757043249011943, -0.0069969331751026315, -0.009404305249792065, 0.31125, 0.75\n",
      "property, 2, 0.0039023051025464106, 0.0035082644520370745, 0.004543894420618975, 0.22125, 0.75\n",
      "property, 1, -0.012243630798137173, -0.00729192859944058, -0.012051931862783308, 0.34, 0.735\n",
      "property, 3, 0.0018315988583930132, 0.0012038215239718437, 0.0026462786049025683, 0.28125, 0.75\n",
      "property, 0, 0.0067904520508335064, 0.002317866150829728, 0.0029060555511125456, 0.1575, 0.725\n",
      "age, 0, 0.004437598430625189, 0.001131906507488968, -0.009303153949036316, 0.79875, 0.655\n",
      "age, 1, -0.00467027488612648, -0.0013938829800908968, -0.002761383466883775, 0.20125, 0.755\n",
      "install_plans, 0, 0.023498746679190052, 0.0056705643746807225, -0.0009542511620843144, 0.805, 0.735\n",
      "install_plans, 1, -0.009534209915012859, -0.0059325408472826555, -0.009100724268905661, 0.195, 0.75\n",
      "num_credits, 1, 0.00520258793676831, 0.0012097084673896399, 0.004357209045065241, 0.63125, 0.73\n",
      "num_credits, 2, -0.004142916492611448, -0.0035538564818145255, -0.003898212475321108, 0.3375, 0.75\n",
      "num_credits, 3, 0.002161356045991325, 0.002192756402479459, 0.0022582966376898956, 0.025, 0.75\n",
      "num_credits, 4, -0.00010274222028905378, -0.00011058486065650125, -0.00010258703550792985, 0.00625, 0.75\n",
      "job, 2, -0.011642779070911158, -0.0029145764342094383, -0.009858512503331703, 0.63625, 0.735\n",
      "job, 3, 0.02408246370700262, 0.0032664321548209396, 0.005619653232656471, 0.15125, 0.75\n",
      "job, 0, 5.879319365209579e-05, -0.00034023765634201444, -0.0001392197538473759, 0.0225, 0.76\n",
      "job, 1, -0.0038527653626458935, -0.0002735945368714188, -0.0015926137224403858, 0.19, 0.75\n",
      "num_liable, 1, -0.042350513242867205, -0.009778711597247818, -0.04444915698191914, 0.855, 0.7\n",
      "num_liable, 2, 0.01135637133866163, 0.00951673512464589, 0.011746119140088211, 0.145, 0.77\n",
      "telephone, 0, -0.012094655240614816, 0.00023021955235110657, -0.0011157906129262746, 0.60375, 0.71\n",
      "telephone, 1, -0.0036669381892022734, -0.0004921960249530323, -0.0014718291790905298, 0.39625, 0.755\n",
      "foreign_worker, 1, -0.03164234658662601, -0.001834411713712125, 0.06532727412862169, 0.9625, 0.675\n",
      "foreign_worker, 0, 0.0011280712936443393, 0.0015724352411101961, 0.001816820991637196, 0.0375, 0.745\n",
      "purpose_A40, 1, -0.0038497379468549475, 0.002505728748384488, 0.0020970140867794455, 0.245, 0.75\n",
      "purpose_A40, 0, -0.030361858336555803, -0.0027677052209864154, -0.022290948740010665, 0.755, 0.71\n",
      "purpose_A41, 0, -0.028484031313833147, -0.0021676241064893928, -0.014329573181008939, 0.8925, 0.755\n",
      "purpose_A41, 1, -0.005315109411203833, 0.0019056476338874656, 0.002149560724781745, 0.1075, 0.77\n",
      "purpose_A410, 0, -0.049816992016361916, -0.001780689293527471, 0.9298625465645712, 0.9875, 0.62\n",
      "purpose_A410, 1, 0.0018779493971238237, 0.0015187128209255432, 0.001714461411947105, 0.0125, 0.76\n",
      "purpose_A42, 0, -0.022712520622305443, 0.0010632928360362246, 0.0002474730722489774, 0.81875, 0.725\n",
      "purpose_A42, 1, -0.0017769847514945702, -0.0013252693086381541, -0.0018496774245363072, 0.18125, 0.73\n",
      "purpose_A43, 0, 0.026495756429621875, 0.006978884326550164, 0.031595250176992995, 0.71875, 0.755\n",
      "purpose_A43, 1, -0.005121635696817672, -0.007240860799152089, -0.008908208219047706, 0.28125, 0.745\n",
      "purpose_A44, 0, -0.03682242715582562, -0.0008901412253860183, -0.09804691310584962, 0.9875, 0.645\n",
      "purpose_A44, 1, 0.0006796166757940814, 0.0006281647527840875, 0.0006396719632532686, 0.0125, 0.76\n",
      "purpose_A45, 0, -0.04919712901853779, -0.003295121549189724, 0.13419371109224998, 0.98375, 0.54\n",
      "purpose_A45, 1, 0.005014227710234964, 0.003033145076587796, 0.0031864323525387815, 0.01625, 0.755\n",
      "purpose_A46, 0, 0.00712475594497608, 0.0022492599697379176, -0.03309690619553194, 0.9575, 0.59\n",
      "purpose_A46, 1, -0.002718935858984084, -0.002511236442339853, -0.002796406956816093, 0.0425, 0.77\n",
      "purpose_A48, 0, -0.009506450538663946, -3.3979233844916976e-05, -0.249564897244674, 0.9925, 0.69\n",
      "purpose_A48, 1, -0.0007220251403635691, -0.00022799723875701507, -0.00024344563473673822, 0.0075, 0.76\n",
      "purpose_A49, 0, 0.004638502287973467, -0.0017139647563177282, -0.028562539913475377, 0.90625, 0.625\n",
      "purpose_A49, 1, 0.000937164032938087, 0.0014519882837158032, 0.0014645794009397035, 0.09375, 0.76\n",
      "gender, 0, -0.005359174831477964, -0.0032004089612008937, -0.0035899244419063635, 0.32125, 0.755\n",
      "gender, 1, 0.021481133141227682, 0.002938432488598965, 0.014418470544318215, 0.67875, 0.7\n",
      "housing_A151, 1, -0.011066767484827267, -0.004500428606839291, -0.005553979295066944, 0.19, 0.74\n",
      "housing_A151, 0, 0.015480578269188361, 0.004238452134237362, 0.0231367370647447, 0.81, 0.705\n",
      "housing_A152, 0, -0.0028719915428357945, 0.002342178374713094, 0.0065645376062469585, 0.3, 0.76\n",
      "housing_A152, 1, 0.015402144269484497, -0.002604154847315021, 0.009555348460874087, 0.7, 0.72\n",
      "housing_A153, 0, -0.03979593105762291, -0.007104583454154317, 0.06062136966644149, 0.89, 0.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "housing_A153, 1, 0.009958096706179864, 0.006842606981552387, 0.009556069514047435, 0.11, 0.76\n"
     ]
    }
   ],
   "source": [
    "print(\"Attribute, Value, Ground-truth subset, Add 1st-order inf individual, \\\n",
    "Second-order subset influence, %rowsRemoved, Accuracy\")\n",
    "clf.fit(X_train, y_train)\n",
    "continuous_cols = ['duration', 'credit_amt', 'install_rate', 'num_credits', 'residence']\n",
    "for col in X_train_orig.columns:\n",
    "    vals = X_train_orig[col].unique()\n",
    "    for val in vals:\n",
    "#         print(col, val, sep=\": \")\n",
    "        idx = X_train_orig[X_train_orig[col] == val].index \n",
    "    \n",
    "        X = np.delete(X_train, idx, 0)\n",
    "        y = y_train.drop(index=idx, inplace=False)\n",
    "        inf_gt = 0\n",
    "        if len(y.unique()) > 1:\n",
    "            # Ground truth subset influence\n",
    "            clf.fit(X, y)\n",
    "            y_pred = clf.predict_proba(X_test)\n",
    "            if metric == 0:\n",
    "                inf_gt = computeFairness(y_pred, X_test_orig, y_test, 0) - spd_0\n",
    "            elif metric == 1:\n",
    "                inf_gt = computeFairness(y_pred, X_test_orig, y_test, 1) - tpr_parity_0\n",
    "            elif metric == 2:\n",
    "                inf_gt = computeFairness(y_pred, X_test_orig, y_test, 2) - predictive_parity_0\n",
    "            accuracy = computeAccuracy(y_test, y_pred)\n",
    "\n",
    "        # First-order subset influence\n",
    "        del_f_1 = 0            \n",
    "        for i in range(len(idx)):\n",
    "            del_f_1 += infs_1[idx[i]]\n",
    "\n",
    "        # Second-order subset influence\n",
    "        size_hvp = 1\n",
    "        params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "        del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "\n",
    "        print(col, val, inf_gt, del_f_1, del_f_2, len(idx)/len(X_train), accuracy, sep=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
