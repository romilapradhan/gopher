{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital', 'occupation', 'relationship', 'race', 'gender', 'capgain', 'caploss', 'hours', 'country', 'income']\n",
    "df_train = pd.read_csv('adult.data', names=cols, sep=\",\")\n",
    "df_test = pd.read_csv('adult.test', names=cols, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    " def one_hot_encode(df):\n",
    "    df.isin(['?']).sum(axis=0)\n",
    "\n",
    "    # replace missing values (?) to nan and then drop the columns\n",
    "    df['country'] = df['country'].replace('?',np.nan)\n",
    "    df['workclass'] = df['workclass'].replace('?',np.nan)\n",
    "    df['occupation'] = df['occupation'].replace('?',np.nan)\n",
    "\n",
    "    # dropping the NaN rows now\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    df['income'] = df['income'].map({'<=50K': 0, '>50K': 1}).astype(int)\n",
    "    df = pd.concat([df, pd.get_dummies(df['gender'], prefix='gender')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['race'], prefix='race')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['marital'], prefix='marital')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['workclass'], prefix='workclass')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['relationship'], prefix='relationship')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['occupation'], prefix='occupation')],axis=1)\n",
    "\n",
    "    df = df.drop(columns=['workclass', 'gender', 'fnlwgt', 'education', 'occupation', \\\n",
    "                      'relationship', 'marital', 'race', 'country', 'capgain', \\\n",
    "                      'caploss'])\n",
    "    return df\n",
    "\n",
    "# one-hot encoding (for regression mdoels)\n",
    "df_train = one_hot_encode(df_train)\n",
    "df_test = one_hot_encode(df_test)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Protected, privileged**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protected: 'gender_Female'=1\n",
    "# privileged: 'gender_Male'=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns='income')\n",
    "y_train = df_train['income']\n",
    "\n",
    "X_test = df_test.drop(columns='income')\n",
    "y_test = df_test['income']\n",
    "\n",
    "# size=500\n",
    "# X_train = X_train[0:size]\n",
    "# y_train = y_train[0:size]\n",
    "\n",
    "X_train_orig = copy.deepcopy(X_train)\n",
    "X_test_orig = copy.deepcopy(X_test)\n",
    "\n",
    "# Scale data: regularization penalty default: ‘l2’, ‘lbfgs’ solvers support only l2 penalties. \n",
    "# Regularization makes the predictor dependent on the scale of the features.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute statistical parity difference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFairness(y_pred, X_test): \n",
    "    protected_idx = X_test[X_test['gender_Female']==1].index\n",
    "    numProtected = len(protected_idx)\n",
    "    privileged_idx = X_test[X_test['gender_Male']==1].index\n",
    "    numPrivileged = len(privileged_idx)\n",
    "    \n",
    "    p_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        p_protected += y_pred[protected_idx[i]][1]\n",
    "    p_protected /= len(protected_idx)\n",
    "    \n",
    "    p_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        p_privileged += y_pred[privileged_idx[i]][1]\n",
    "    p_privileged /= len(privileged_idx)\n",
    "    \n",
    "    spd = p_protected - p_privileged\n",
    "    return spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Influence of points computed using ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_influence(X_train, y_train, X_test, X_test_orig):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    spd_0 = computeFairness(y_pred, X_test_orig)\n",
    "\n",
    "    delta_spd = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_removed = np.delete(X_train, i, 0)\n",
    "        y_removed = y_train.drop(index=i, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        delta_spd_i = computeFairness(y_pred, X_test_orig) - spd_0\n",
    "        delta_spd.append(delta_spd_i)\n",
    "    \n",
    "    return delta_spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function** (Log loss for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if (y_pred[i][1] != 0 and y_pred[i][0] != 0):\n",
    "            loss += - y_true[i] * math.log(y_pred[i][1]) - (1 - y_true[i]) * math.log(y_pred[i][0])\n",
    "    loss /= len(y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Accuracy** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAccuracy(y_true, y_pred):\n",
    "    accuracy = 0\n",
    "    for i in range(len(y_true)):\n",
    "        idx = y_true[i]\n",
    "        accuracy += y_pred[i][idx]\n",
    "    accuracy /= len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of loss function at z with respect to model parameters**\n",
    "\n",
    "(Pre-computed for all training points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_L_del_theta_i(num_params, y_true, x, y_pred):\n",
    "#     del_L_del_theta = np.ones((num_params, 1)) * ((1 - y_true) * y_pred[1] - y_true * y_pred[0])\n",
    "    del_L_del_theta = np.ones((num_params, 1)) * (- y_true + y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_L_del_theta[j] *=  x[j-1]\n",
    "    return del_L_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessian: Second-order partial derivative of loss function with respect to model parameters**\n",
    "\n",
    "(Pre-computed for all training points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_one_point(num_params, x, y_pred):\n",
    "    H = np.ones((num_params, num_params)) * (y_pred[0] * y_pred[1])\n",
    "    for i in range(1, num_params):\n",
    "        for j in range(i + 1):\n",
    "            if j == 0:\n",
    "                H[i][j] *= x[i-1]\n",
    "            else:\n",
    "                H[i][j] *= x[i-1] * x[j-1] \n",
    "    i_lower = np.tril_indices(num_params, -1)\n",
    "    H.T[i_lower] = H[i_lower]     \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of $P(y \\mid \\textbf{x})$ with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_f_del_theta_i(num_params, x, y_pred):\n",
    "    del_f_del_theta = np.ones((num_params, 1)) * (y_pred[0] * y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_f_del_theta[j] *=  x[j-1]\n",
    "    return del_f_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($Statistical parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(SPD)/del(theta)\n",
    "def del_spd_del_theta(num_params, X_test_orig, X_test, y_pred):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    numProtected = X_test_orig['gender_Female'].sum()\n",
    "    numPrivileged = X_test_orig['gender_Male'].sum()\n",
    "    for i in range(len(X_test)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[i], y_pred[i])\n",
    "        if X_test_orig.iloc[i]['gender_Male'] == 1: #privileged\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "        elif X_test_orig.iloc[i]['gender_Female'] == 1:\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_privileged /= numPrivileged\n",
    "    del_f_protected /= numProtected\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic estimation of Hessian vector product (involving del fairness): $H_{\\theta}^{-1}v = H_{\\theta}^{-1}\\nabla_{\\theta}f(z, \\theta) = v + [I - \\nabla_{\\theta}^2L(z_{s_j}, \\theta^*)]H_{\\theta}^{-1}v$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformly sample t points from training data \n",
    "def hessian_vector_product(num_params, n, size, v, hessian_all_points):\n",
    "    if (size > n):\n",
    "        size = n\n",
    "    sample = random.sample(range(n), size)\n",
    "    hinv_v = copy.deepcopy(v)\n",
    "    for idx in range(size):\n",
    "        i = sample[idx]\n",
    "        hessian_i = hessian_all_points[i]\n",
    "        hinv_v = np.matmul(np.subtract(np.identity(num_params), hessian_i), hinv_v)\n",
    "        hinv_v = np.add(hinv_v, v)\n",
    "    return hinv_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order_influence(del_L_del_theta, hinv_v, n):\n",
    "    infs = []\n",
    "    for i in range(n):\n",
    "        inf = -np.dot(del_L_del_theta[i].transpose(), hinv_v)\n",
    "        inf *= -1/n\n",
    "        infs.append(inf[0][0].tolist())\n",
    "    return infs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics: Initial state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial fairness:  -0.20059371090978573\n",
      "Initial loss:  0.360972684923813\n",
      "Initial accuracy:  0.7683939044369612\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.0001\n",
    "clf.fit(X_train, y_train)\n",
    "num_params = len(clf.coef_.transpose()) + 1 #weights and intercept; params: clf.coef_, clf.intercept_\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "y_pred_train = clf.predict_proba(X_train)\n",
    "    \n",
    "spd_0 = computeFairness(y_pred_test, X_test_orig)\n",
    "print(\"Initial fairness: \", spd_0)\n",
    "\n",
    "loss_0 = logistic_loss(y_test, y_pred_test)\n",
    "print(\"Initial loss: \", loss_0)\n",
    "\n",
    "accuracy_0 = computeAccuracy(y_test, y_pred_test)\n",
    "print(\"Initial accuracy: \", accuracy_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-compute: (1) Hessian (2) del_L_del_theta for each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_L_del_theta = []\n",
    "for i in range(int(len(X_train))):\n",
    "    del_L_del_theta.insert(i, del_L_del_theta_i(num_params, y_train[i], X_train[i], y_pred_train[i]))\n",
    "\n",
    "hessian_all_points = []\n",
    "for i in range(len(X_train)):\n",
    "    hessian_all_points.insert(i, hessian_one_point(num_params, X_train[i], y_pred_train[i])\n",
    "                              /len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*H^{-1} computation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexact = 1\n",
    "v1 = del_spd_del_theta(num_params, X_test_orig, X_test, y_pred_test)\n",
    "# ix = 203\n",
    "# v1 = del_L_del_theta_i(num_params, y_test[ix], X_test[ix], y_pred_test[ix])\n",
    "if hexact == 1: \n",
    "    H_exact = np.zeros((num_params, num_params))\n",
    "    for i in range(len(X_train)):\n",
    "        H_exact = np.add(H_exact, hessian_all_points[i])\n",
    "    hinv_exact = np.linalg.pinv(H_exact) \n",
    "    hinv_v = np.matmul(hinv_exact, v1)\n",
    "else: #using Hessian vector product\n",
    "    size_hvp = int(len(X_train) * .01)\n",
    "    hinv_v = hessian_vector_product(num_params, len(X_train), size_hvp, v1, hessian_all_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ground truth influence of each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth influence\n",
    "# spdgt = ground_truth_influence(X_train, y_train, X_test, X_test_orig)\n",
    "# with open('delta_spd_ground_truth_v0.txt', 'w') as filehandle:\n",
    "#     for listitem in delta_spd:\n",
    "#         filehandle.write('%s\\n' % listitem)\n",
    "gt_spd = pd.read_csv('delta_spd_ground_truth_v0.txt', names=[\"Values\"], sep=\",\")\n",
    "gt_spd = gt_spd.values.tolist()\n",
    "spdgt=[]\n",
    "for i in range(len(gt_spd)):\n",
    "    spdgt.append(gt_spd[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence of each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "infs_1 = first_order_influence(del_L_del_theta, hinv_v, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second-order influence computation for a group of points in subset U**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_influence(X_train, U, size, del_L_del_theta, hessian_all_points):\n",
    "    u = len(U)\n",
    "    s = len(X_train)\n",
    "    p = u/s\n",
    "    c1 = (1 - 2*p)/(s * (1-p)**2)\n",
    "    c2 = 1/((s * (1-p))**2)\n",
    "    num_params = len(del_L_del_theta[0])\n",
    "    del_L_del_theta_hinv = np.zeros((num_params, 1))\n",
    "    del_L_del_theta_sum = np.zeros((num_params, 1))\n",
    "    hessian_U = np.zeros((num_params, num_params))\n",
    "    for i in range(u):\n",
    "        idx = U[i]\n",
    "        hessian_U = np.add(hessian_U, s * hessian_all_points[idx])\n",
    "        del_L_del_theta_sum = np.add(del_L_del_theta_sum, del_L_del_theta[idx])\n",
    "    \n",
    "    hinv_del_L_del_theta= np.matmul(hinv_exact, del_L_del_theta_sum)\n",
    "    hinv_hessian_U = np.matmul(hinv_exact, hessian_U)\n",
    "    term1 = c1 * hinv_del_L_del_theta\n",
    "    term2 = c2 * np.matmul(hinv_hessian_U, hinv_del_L_del_theta)\n",
    "    sum_term = np.add(term1, term2)\n",
    "    return sum_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification of first-order influence (implementation) on log-loss of a single point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "9\n",
      "19\n",
      "20\n",
      "47\n",
      "69\n",
      "72\n",
      "78\n",
      "88\n",
      "95\n",
      "107\n",
      "116\n",
      "123\n",
      "127\n",
      "134\n",
      "141\n",
      "146\n",
      "160\n",
      "165\n",
      "166\n",
      "175\n",
      "182\n",
      "184\n",
      "188\n",
      "190\n",
      "192\n",
      "198\n",
      "203\n",
      "206\n",
      "207\n",
      "210\n",
      "212\n",
      "213\n",
      "214\n",
      "228\n",
      "239\n",
      "244\n",
      "247\n",
      "248\n",
      "257\n",
      "272\n",
      "274\n",
      "276\n",
      "288\n",
      "290\n",
      "295\n",
      "299\n",
      "302\n",
      "304\n",
      "310\n",
      "312\n",
      "317\n",
      "318\n",
      "319\n",
      "324\n",
      "325\n",
      "335\n",
      "338\n",
      "347\n",
      "350\n",
      "352\n",
      "354\n",
      "360\n",
      "366\n",
      "367\n",
      "371\n",
      "378\n",
      "381\n",
      "385\n",
      "390\n",
      "396\n",
      "398\n",
      "400\n",
      "408\n",
      "429\n",
      "433\n",
      "443\n",
      "455\n",
      "460\n",
      "468\n",
      "475\n",
      "478\n",
      "481\n",
      "483\n",
      "484\n",
      "486\n",
      "491\n",
      "492\n",
      "494\n",
      "495\n",
      "499\n",
      "503\n",
      "506\n",
      "509\n",
      "510\n",
      "512\n",
      "515\n",
      "526\n",
      "531\n",
      "535\n",
      "536\n",
      "538\n",
      "540\n",
      "547\n",
      "553\n",
      "554\n",
      "559\n",
      "562\n",
      "600\n",
      "602\n",
      "607\n",
      "614\n",
      "634\n",
      "644\n",
      "645\n",
      "651\n",
      "656\n",
      "658\n",
      "662\n",
      "676\n",
      "677\n",
      "687\n",
      "691\n",
      "694\n",
      "698\n",
      "701\n",
      "702\n",
      "713\n",
      "724\n",
      "725\n",
      "736\n",
      "739\n",
      "740\n",
      "744\n",
      "746\n",
      "747\n",
      "750\n",
      "764\n",
      "766\n",
      "768\n",
      "769\n",
      "782\n",
      "783\n",
      "791\n",
      "801\n",
      "803\n",
      "807\n",
      "809\n",
      "817\n",
      "818\n",
      "820\n",
      "822\n",
      "823\n",
      "827\n",
      "830\n",
      "838\n",
      "841\n",
      "846\n",
      "849\n",
      "851\n",
      "853\n",
      "861\n",
      "863\n",
      "870\n",
      "872\n",
      "878\n",
      "881\n",
      "883\n",
      "886\n",
      "887\n",
      "903\n",
      "905\n",
      "906\n",
      "907\n",
      "910\n",
      "911\n",
      "913\n",
      "917\n",
      "918\n",
      "920\n",
      "921\n",
      "927\n",
      "941\n",
      "951\n",
      "952\n",
      "956\n",
      "960\n",
      "971\n",
      "982\n",
      "983\n",
      "984\n",
      "1005\n",
      "1006\n",
      "1012\n",
      "1017\n",
      "1018\n",
      "1021\n",
      "1026\n",
      "1027\n",
      "1029\n",
      "1031\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1040\n",
      "1045\n",
      "1054\n",
      "1063\n",
      "1069\n",
      "1088\n",
      "1089\n",
      "1091\n",
      "1094\n",
      "1105\n",
      "1111\n",
      "1116\n",
      "1119\n",
      "1123\n",
      "1126\n",
      "1127\n",
      "1138\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1145\n",
      "1149\n",
      "1154\n",
      "1162\n",
      "1164\n",
      "1167\n",
      "1171\n",
      "1178\n",
      "1182\n",
      "1206\n",
      "1213\n",
      "1218\n",
      "1219\n",
      "1227\n",
      "1230\n",
      "1234\n",
      "1237\n",
      "1238\n",
      "1243\n",
      "1254\n",
      "1261\n",
      "1267\n",
      "1273\n",
      "1277\n",
      "1280\n",
      "1291\n",
      "1292\n",
      "1302\n",
      "1303\n",
      "1308\n",
      "1310\n",
      "1327\n",
      "1330\n",
      "1331\n",
      "1336\n",
      "1338\n",
      "1344\n",
      "1359\n",
      "1366\n",
      "1369\n",
      "1373\n",
      "1374\n",
      "1385\n",
      "1394\n",
      "1399\n",
      "1403\n",
      "1419\n",
      "1434\n",
      "1445\n",
      "1449\n",
      "1476\n",
      "1489\n",
      "1490\n",
      "1522\n",
      "1533\n",
      "1538\n",
      "1547\n",
      "1550\n",
      "1551\n",
      "1559\n",
      "1562\n",
      "1578\n",
      "1585\n",
      "1589\n",
      "1593\n",
      "1595\n",
      "1596\n",
      "1600\n",
      "1608\n",
      "1613\n",
      "1615\n",
      "1627\n",
      "1633\n",
      "1636\n",
      "1648\n",
      "1649\n",
      "1656\n",
      "1666\n",
      "1675\n",
      "1677\n",
      "1684\n",
      "1685\n",
      "1713\n",
      "1723\n",
      "1724\n",
      "1730\n",
      "1731\n",
      "1748\n",
      "1750\n",
      "1752\n",
      "1763\n",
      "1768\n",
      "1769\n",
      "1778\n",
      "1781\n",
      "1787\n",
      "1788\n",
      "1790\n",
      "1804\n",
      "1815\n",
      "1817\n",
      "1822\n",
      "1823\n",
      "1825\n",
      "1830\n",
      "1838\n",
      "1841\n",
      "1847\n",
      "1849\n",
      "1858\n",
      "1865\n",
      "1877\n",
      "1880\n",
      "1883\n",
      "1888\n",
      "1890\n",
      "1896\n",
      "1916\n",
      "1917\n",
      "1920\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1939\n",
      "1957\n",
      "1966\n",
      "1975\n",
      "1984\n",
      "1986\n",
      "1992\n",
      "2002\n",
      "2012\n",
      "2025\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2046\n",
      "2050\n",
      "2051\n",
      "2055\n",
      "2060\n",
      "2063\n",
      "2067\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2081\n",
      "2092\n",
      "2098\n",
      "2100\n",
      "2106\n",
      "2111\n",
      "2115\n",
      "2123\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2146\n",
      "2150\n",
      "2152\n",
      "2157\n",
      "2161\n",
      "2164\n",
      "2166\n",
      "2173\n",
      "2175\n",
      "2177\n",
      "2187\n",
      "2191\n",
      "2192\n",
      "2196\n",
      "2206\n",
      "2209\n",
      "2210\n",
      "2215\n",
      "2220\n",
      "2221\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2250\n",
      "2260\n",
      "2268\n",
      "2272\n",
      "2278\n",
      "2280\n",
      "2283\n",
      "2285\n",
      "2286\n",
      "2306\n",
      "2310\n",
      "2324\n",
      "2328\n",
      "2340\n",
      "2349\n",
      "2352\n",
      "2354\n",
      "2356\n",
      "2364\n",
      "2372\n",
      "2373\n",
      "2377\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2393\n",
      "2397\n",
      "2412\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2430\n",
      "2434\n",
      "2435\n",
      "2444\n",
      "2445\n",
      "2452\n",
      "2457\n",
      "2459\n",
      "2464\n",
      "2466\n",
      "2467\n",
      "2478\n",
      "2488\n",
      "2491\n",
      "2504\n",
      "2505\n",
      "2516\n",
      "2525\n",
      "2529\n",
      "2531\n",
      "2534\n",
      "2536\n",
      "2537\n",
      "2540\n",
      "2548\n",
      "2551\n",
      "2552\n",
      "2564\n",
      "2570\n",
      "2577\n",
      "2578\n",
      "2583\n",
      "2584\n",
      "2588\n",
      "2592\n",
      "2595\n",
      "2599\n",
      "2606\n",
      "2608\n",
      "2620\n",
      "2626\n",
      "2630\n",
      "2639\n",
      "2649\n",
      "2650\n",
      "2657\n",
      "2667\n",
      "2668\n",
      "2679\n",
      "2680\n",
      "2682\n",
      "2684\n",
      "2691\n",
      "2694\n",
      "2695\n",
      "2700\n",
      "2719\n",
      "2721\n",
      "2723\n",
      "2736\n",
      "2739\n",
      "2741\n",
      "2750\n",
      "2758\n",
      "2759\n",
      "2763\n",
      "2766\n",
      "2789\n",
      "2792\n",
      "2802\n",
      "2803\n",
      "2816\n",
      "2817\n",
      "2825\n",
      "2828\n",
      "2842\n",
      "2844\n",
      "2848\n",
      "2864\n",
      "2877\n",
      "2887\n",
      "2902\n",
      "2908\n",
      "2918\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2930\n",
      "2935\n",
      "2937\n",
      "2946\n",
      "2948\n",
      "2953\n",
      "2970\n",
      "2978\n",
      "2979\n",
      "2991\n",
      "2993\n",
      "2997\n",
      "2998\n",
      "3003\n",
      "3014\n",
      "3019\n",
      "3021\n",
      "3037\n",
      "3044\n",
      "3046\n",
      "3058\n",
      "3066\n",
      "3069\n",
      "3071\n",
      "3072\n",
      "3080\n",
      "3087\n",
      "3094\n",
      "3098\n",
      "3099\n",
      "3106\n",
      "3110\n",
      "3124\n",
      "3135\n",
      "3138\n",
      "3139\n",
      "3148\n",
      "3151\n",
      "3152\n",
      "3166\n",
      "3168\n",
      "3171\n",
      "3185\n",
      "3192\n",
      "3193\n",
      "3195\n",
      "3198\n",
      "3202\n",
      "3208\n",
      "3215\n",
      "3226\n",
      "3228\n",
      "3245\n",
      "3252\n",
      "3254\n",
      "3265\n",
      "3269\n",
      "3271\n",
      "3275\n",
      "3279\n",
      "3282\n",
      "3290\n",
      "3300\n",
      "3304\n",
      "3305\n",
      "3307\n",
      "3308\n",
      "3310\n",
      "3337\n",
      "3340\n",
      "3345\n",
      "3348\n",
      "3350\n",
      "3351\n",
      "3354\n",
      "3360\n",
      "3371\n",
      "3376\n",
      "3379\n",
      "3383\n",
      "3387\n",
      "3395\n",
      "3411\n",
      "3416\n",
      "3417\n",
      "3418\n",
      "3421\n",
      "3423\n",
      "3424\n",
      "3436\n",
      "3441\n",
      "3442\n",
      "3446\n",
      "3447\n",
      "3454\n",
      "3458\n",
      "3460\n",
      "3465\n",
      "3473\n",
      "3485\n",
      "3486\n",
      "3489\n",
      "3506\n",
      "3507\n",
      "3518\n",
      "3519\n",
      "3521\n",
      "3527\n",
      "3536\n",
      "3537\n",
      "3540\n",
      "3548\n",
      "3555\n",
      "3560\n",
      "3563\n",
      "3569\n",
      "3574\n",
      "3578\n",
      "3579\n",
      "3586\n",
      "3590\n",
      "3591\n",
      "3594\n",
      "3599\n",
      "3612\n",
      "3616\n",
      "3621\n",
      "3623\n",
      "3637\n",
      "3642\n",
      "3645\n",
      "3646\n",
      "3647\n",
      "3656\n",
      "3665\n",
      "3675\n",
      "3677\n",
      "3678\n",
      "3684\n",
      "3687\n",
      "3689\n",
      "3695\n",
      "3702\n",
      "3708\n",
      "3715\n",
      "3722\n",
      "3729\n",
      "3735\n",
      "3736\n",
      "3740\n",
      "3746\n",
      "3749\n",
      "3750\n",
      "3753\n",
      "3756\n",
      "3770\n",
      "3773\n",
      "3793\n",
      "3794\n",
      "3795\n",
      "3811\n",
      "3835\n",
      "3836\n",
      "3851\n",
      "3868\n",
      "3879\n",
      "3895\n",
      "3920\n",
      "3927\n",
      "3934\n",
      "3947\n",
      "3954\n",
      "3956\n",
      "3958\n",
      "3960\n",
      "3961\n",
      "3964\n",
      "3969\n",
      "3974\n",
      "3982\n",
      "3986\n",
      "3995\n",
      "3998\n",
      "4008\n",
      "4016\n",
      "4021\n",
      "4025\n",
      "4029\n",
      "4035\n",
      "4036\n",
      "4058\n",
      "4060\n",
      "4064\n",
      "4069\n",
      "4085\n",
      "4086\n",
      "4093\n",
      "4096\n",
      "4100\n",
      "4103\n",
      "4105\n",
      "4114\n",
      "4122\n",
      "4123\n",
      "4132\n",
      "4139\n",
      "4150\n",
      "4167\n",
      "4188\n",
      "4200\n",
      "4207\n",
      "4208\n",
      "4220\n",
      "4224\n",
      "4227\n",
      "4232\n",
      "4237\n",
      "4249\n",
      "4251\n",
      "4257\n",
      "4264\n",
      "4268\n",
      "4269\n",
      "4275\n",
      "4284\n",
      "4289\n",
      "4294\n",
      "4305\n",
      "4307\n",
      "4309\n",
      "4314\n",
      "4315\n",
      "4328\n",
      "4339\n",
      "4358\n",
      "4364\n",
      "4370\n",
      "4376\n",
      "4381\n",
      "4385\n",
      "4388\n",
      "4408\n",
      "4417\n",
      "4418\n",
      "4423\n",
      "4424\n",
      "4429\n",
      "4430\n",
      "4441\n",
      "4442\n",
      "4446\n",
      "4451\n",
      "4452\n",
      "4464\n",
      "4466\n",
      "4468\n",
      "4469\n",
      "4473\n",
      "4474\n",
      "4476\n",
      "4477\n",
      "4482\n",
      "4489\n",
      "4490\n",
      "4493\n",
      "4499\n",
      "4501\n",
      "4502\n",
      "4509\n",
      "4510\n",
      "4511\n",
      "4513\n",
      "4520\n",
      "4521\n",
      "4528\n",
      "4542\n",
      "4546\n",
      "4547\n",
      "4559\n",
      "4560\n",
      "4565\n",
      "4566\n",
      "4569\n",
      "4573\n",
      "4576\n",
      "4593\n",
      "4607\n",
      "4609\n",
      "4615\n",
      "4632\n",
      "4641\n",
      "4642\n",
      "4652\n",
      "4657\n",
      "4660\n",
      "4667\n",
      "4668\n",
      "4670\n",
      "4671\n",
      "4673\n",
      "4680\n",
      "4681\n",
      "4682\n",
      "4688\n",
      "4691\n",
      "4692\n",
      "4693\n",
      "4694\n",
      "4700\n",
      "4703\n",
      "4704\n",
      "4705\n",
      "4707\n",
      "4722\n",
      "4731\n",
      "4732\n",
      "4736\n",
      "4737\n",
      "4739\n",
      "4743\n",
      "4753\n",
      "4762\n",
      "4764\n",
      "4765\n",
      "4775\n",
      "4777\n",
      "4781\n",
      "4784\n",
      "4786\n",
      "4788\n",
      "4790\n",
      "4793\n",
      "4798\n",
      "4802\n",
      "4803\n",
      "4811\n",
      "4814\n",
      "4823\n",
      "4826\n",
      "4827\n",
      "4831\n",
      "4840\n",
      "4842\n",
      "4851\n",
      "4852\n",
      "4853\n",
      "4865\n",
      "4873\n",
      "4874\n",
      "4875\n",
      "4881\n",
      "4908\n",
      "4909\n",
      "4911\n",
      "4912\n",
      "4924\n",
      "4947\n",
      "4953\n",
      "4955\n",
      "4957\n",
      "4962\n",
      "4963\n",
      "4977\n",
      "4985\n",
      "4987\n",
      "4989\n",
      "4991\n",
      "5004\n",
      "5007\n",
      "5009\n",
      "5010\n",
      "5012\n",
      "5015\n",
      "5019\n",
      "5023\n",
      "5029\n",
      "5045\n",
      "5050\n",
      "5051\n",
      "5052\n",
      "5053\n",
      "5057\n",
      "5073\n",
      "5074\n",
      "5079\n",
      "5086\n",
      "5090\n",
      "5092\n",
      "5097\n",
      "5098\n",
      "5102\n",
      "5104\n",
      "5107\n",
      "5113\n",
      "5115\n",
      "5128\n",
      "5131\n",
      "5139\n",
      "5140\n",
      "5151\n",
      "5154\n",
      "5157\n",
      "5159\n",
      "5172\n",
      "5176\n",
      "5178\n",
      "5184\n",
      "5185\n",
      "5190\n",
      "5204\n",
      "5211\n",
      "5215\n",
      "5225\n",
      "5231\n",
      "5233\n",
      "5244\n",
      "5252\n",
      "5257\n",
      "5260\n",
      "5261\n",
      "5262\n",
      "5270\n",
      "5275\n",
      "5280\n",
      "5288\n",
      "5290\n",
      "5297\n",
      "5299\n",
      "5301\n",
      "5308\n",
      "5310\n",
      "5318\n",
      "5319\n",
      "5333\n",
      "5351\n",
      "5352\n",
      "5353\n",
      "5354\n",
      "5358\n",
      "5369\n",
      "5393\n",
      "5396\n",
      "5398\n",
      "5400\n",
      "5404\n",
      "5413\n",
      "5420\n",
      "5427\n",
      "5429\n",
      "5447\n",
      "5448\n",
      "5460\n",
      "5465\n",
      "5489\n",
      "5490\n",
      "5492\n",
      "5496\n",
      "5509\n",
      "5512\n",
      "5515\n",
      "5517\n",
      "5536\n",
      "5549\n",
      "5554\n",
      "5558\n",
      "5568\n",
      "5569\n",
      "5579\n",
      "5580\n",
      "5589\n",
      "5601\n",
      "5602\n",
      "5612\n",
      "5620\n",
      "5624\n",
      "5625\n",
      "5632\n",
      "5642\n",
      "5649\n",
      "5650\n",
      "5657\n",
      "5664\n",
      "5666\n",
      "5669\n",
      "5672\n",
      "5675\n",
      "5686\n",
      "5690\n",
      "5692\n",
      "5693\n",
      "5698\n",
      "5709\n",
      "5713\n",
      "5718\n",
      "5720\n",
      "5721\n",
      "5729\n",
      "5730\n",
      "5739\n",
      "5740\n",
      "5754\n",
      "5758\n",
      "5760\n",
      "5762\n",
      "5763\n",
      "5764\n",
      "5765\n",
      "5805\n",
      "5808\n",
      "5815\n",
      "5818\n",
      "5826\n",
      "5831\n",
      "5834\n",
      "5845\n",
      "5847\n",
      "5850\n",
      "5854\n",
      "5859\n",
      "5860\n",
      "5861\n",
      "5866\n",
      "5870\n",
      "5872\n",
      "5879\n",
      "5881\n",
      "5891\n",
      "5892\n",
      "5903\n",
      "5914\n",
      "5921\n",
      "5922\n",
      "5929\n",
      "5934\n",
      "5940\n",
      "5946\n",
      "5957\n",
      "5960\n",
      "5965\n",
      "5968\n",
      "5973\n",
      "5977\n",
      "5982\n",
      "5984\n",
      "5988\n",
      "5994\n",
      "5997\n",
      "6005\n",
      "6011\n",
      "6013\n",
      "6036\n",
      "6040\n",
      "6041\n",
      "6042\n",
      "6062\n",
      "6063\n",
      "6070\n",
      "6087\n",
      "6096\n",
      "6098\n",
      "6099\n",
      "6106\n",
      "6107\n",
      "6115\n",
      "6124\n",
      "6125\n",
      "6126\n",
      "6128\n",
      "6133\n",
      "6139\n",
      "6141\n",
      "6143\n",
      "6146\n",
      "6151\n",
      "6153\n",
      "6160\n",
      "6173\n",
      "6174\n",
      "6183\n",
      "6186\n",
      "6189\n",
      "6192\n",
      "6194\n",
      "6200\n",
      "6204\n",
      "6220\n",
      "6234\n",
      "6236\n",
      "6247\n",
      "6248\n",
      "6252\n",
      "6259\n",
      "6260\n",
      "6272\n",
      "6276\n",
      "6278\n",
      "6279\n",
      "6290\n",
      "6301\n",
      "6304\n",
      "6318\n",
      "6331\n",
      "6335\n",
      "6340\n",
      "6341\n",
      "6344\n",
      "6355\n",
      "6358\n",
      "6360\n",
      "6364\n",
      "6373\n",
      "6375\n",
      "6378\n",
      "6380\n",
      "6381\n",
      "6383\n",
      "6387\n",
      "6395\n",
      "6396\n",
      "6397\n",
      "6399\n",
      "6405\n",
      "6409\n",
      "6410\n",
      "6414\n",
      "6417\n",
      "6428\n",
      "6432\n",
      "6440\n",
      "6441\n",
      "6449\n",
      "6451\n",
      "6457\n",
      "6465\n",
      "6466\n",
      "6474\n",
      "6489\n",
      "6490\n",
      "6493\n",
      "6500\n",
      "6509\n",
      "6526\n",
      "6528\n",
      "6533\n",
      "6535\n",
      "6541\n",
      "6549\n",
      "6552\n",
      "6572\n",
      "6575\n",
      "6577\n",
      "6596\n",
      "6601\n",
      "6616\n",
      "6618\n",
      "6630\n",
      "6631\n",
      "6634\n",
      "6649\n",
      "6666\n",
      "6672\n",
      "6673\n",
      "6677\n",
      "6680\n",
      "6682\n",
      "6698\n",
      "6706\n",
      "6709\n",
      "6723\n",
      "6728\n",
      "6746\n",
      "6747\n",
      "6752\n",
      "6759\n",
      "6766\n",
      "6767\n",
      "6771\n",
      "6796\n",
      "6798\n",
      "6801\n",
      "6805\n",
      "6806\n",
      "6808\n",
      "6811\n",
      "6812\n",
      "6819\n",
      "6823\n",
      "6826\n",
      "6827\n",
      "6830\n",
      "6836\n",
      "6845\n",
      "6847\n",
      "6855\n",
      "6857\n",
      "6861\n",
      "6866\n",
      "6868\n",
      "6869\n",
      "6870\n",
      "6880\n",
      "6890\n",
      "6894\n",
      "6908\n",
      "6909\n",
      "6913\n",
      "6920\n",
      "6922\n",
      "6928\n",
      "6931\n",
      "6932\n",
      "6946\n",
      "6961\n",
      "6962\n",
      "6965\n",
      "6967\n",
      "6978\n",
      "6980\n",
      "6984\n",
      "6998\n",
      "7009\n",
      "7014\n",
      "7022\n",
      "7023\n",
      "7025\n",
      "7028\n",
      "7043\n",
      "7044\n",
      "7051\n",
      "7061\n",
      "7064\n",
      "7069\n",
      "7071\n",
      "7073\n",
      "7076\n",
      "7078\n",
      "7079\n",
      "7083\n",
      "7092\n",
      "7095\n",
      "7105\n",
      "7125\n",
      "7141\n",
      "7143\n",
      "7144\n",
      "7146\n",
      "7164\n",
      "7165\n",
      "7167\n",
      "7172\n",
      "7176\n",
      "7180\n",
      "7182\n",
      "7215\n",
      "7218\n",
      "7220\n",
      "7221\n",
      "7229\n",
      "7231\n",
      "7240\n",
      "7245\n",
      "7246\n",
      "7247\n",
      "7256\n",
      "7261\n",
      "7264\n",
      "7270\n",
      "7271\n",
      "7274\n",
      "7282\n",
      "7287\n",
      "7295\n",
      "7299\n",
      "7302\n",
      "7303\n",
      "7305\n",
      "7307\n",
      "7312\n",
      "7315\n",
      "7319\n",
      "7329\n",
      "7330\n",
      "7347\n",
      "7350\n",
      "7353\n",
      "7355\n",
      "7356\n",
      "7359\n",
      "7362\n",
      "7370\n",
      "7371\n",
      "7373\n",
      "7374\n",
      "7381\n",
      "7391\n",
      "7407\n",
      "7409\n",
      "7412\n",
      "7419\n",
      "7424\n",
      "7425\n",
      "7430\n",
      "7434\n",
      "7437\n",
      "7444\n",
      "7449\n",
      "7456\n",
      "7458\n",
      "7464\n",
      "7465\n",
      "7466\n",
      "7477\n",
      "7485\n",
      "7490\n",
      "7515\n",
      "7522\n",
      "7527\n",
      "7529\n",
      "7531\n",
      "7532\n",
      "7542\n",
      "7551\n",
      "7568\n",
      "7569\n",
      "7577\n",
      "7581\n",
      "7589\n",
      "7599\n",
      "7602\n",
      "7604\n",
      "7607\n",
      "7612\n",
      "7625\n",
      "7626\n",
      "7627\n",
      "7628\n",
      "7631\n",
      "7637\n",
      "7638\n",
      "7642\n",
      "7647\n",
      "7653\n",
      "7655\n",
      "7661\n",
      "7662\n",
      "7664\n",
      "7666\n",
      "7672\n",
      "7678\n",
      "7683\n",
      "7684\n",
      "7686\n",
      "7694\n",
      "7699\n",
      "7704\n",
      "7705\n",
      "7707\n",
      "7709\n",
      "7712\n",
      "7729\n",
      "7730\n",
      "7731\n",
      "7735\n",
      "7736\n",
      "7737\n",
      "7739\n",
      "7753\n",
      "7756\n",
      "7758\n",
      "7759\n",
      "7760\n",
      "7761\n",
      "7768\n",
      "7771\n",
      "7781\n",
      "7788\n",
      "7794\n",
      "7795\n",
      "7798\n",
      "7809\n",
      "7812\n",
      "7817\n",
      "7819\n",
      "7821\n",
      "7823\n",
      "7827\n",
      "7839\n",
      "7840\n",
      "7849\n",
      "7861\n",
      "7863\n",
      "7873\n",
      "7878\n",
      "7879\n",
      "7898\n",
      "7909\n",
      "7910\n",
      "7920\n",
      "7922\n",
      "7933\n",
      "7948\n",
      "7954\n",
      "7958\n",
      "7959\n",
      "7962\n",
      "7963\n",
      "7969\n",
      "7973\n",
      "7974\n",
      "7987\n",
      "7988\n",
      "7991\n",
      "7994\n",
      "8003\n",
      "8014\n",
      "8016\n",
      "8032\n",
      "8034\n",
      "8035\n",
      "8038\n",
      "8041\n",
      "8046\n",
      "8049\n",
      "8052\n",
      "8076\n",
      "8085\n",
      "8102\n",
      "8107\n",
      "8108\n",
      "8113\n",
      "8116\n",
      "8117\n",
      "8119\n",
      "8136\n",
      "8147\n",
      "8150\n",
      "8160\n",
      "8173\n",
      "8179\n",
      "8187\n",
      "8188\n",
      "8198\n",
      "8199\n",
      "8200\n",
      "8201\n",
      "8205\n",
      "8206\n",
      "8208\n",
      "8211\n",
      "8214\n",
      "8217\n",
      "8222\n",
      "8223\n",
      "8224\n",
      "8231\n",
      "8243\n",
      "8249\n",
      "8250\n",
      "8269\n",
      "8271\n",
      "8273\n",
      "8275\n",
      "8283\n",
      "8285\n",
      "8287\n",
      "8289\n",
      "8300\n",
      "8303\n",
      "8305\n",
      "8308\n",
      "8309\n",
      "8314\n",
      "8318\n",
      "8321\n",
      "8323\n",
      "8324\n",
      "8325\n",
      "8331\n",
      "8334\n",
      "8344\n",
      "8349\n",
      "8353\n",
      "8354\n",
      "8356\n",
      "8359\n",
      "8363\n",
      "8366\n",
      "8372\n",
      "8386\n",
      "8396\n",
      "8404\n",
      "8408\n",
      "8410\n",
      "8419\n",
      "8421\n",
      "8427\n",
      "8430\n",
      "8431\n",
      "8441\n",
      "8443\n",
      "8462\n",
      "8466\n",
      "8488\n",
      "8495\n",
      "8496\n",
      "8500\n",
      "8505\n",
      "8507\n",
      "8509\n",
      "8510\n",
      "8518\n",
      "8522\n",
      "8528\n",
      "8531\n",
      "8535\n",
      "8541\n",
      "8559\n",
      "8560\n",
      "8569\n",
      "8592\n",
      "8594\n",
      "8602\n",
      "8612\n",
      "8613\n",
      "8618\n",
      "8624\n",
      "8632\n",
      "8644\n",
      "8648\n",
      "8664\n",
      "8666\n",
      "8673\n",
      "8674\n",
      "8677\n",
      "8678\n",
      "8680\n",
      "8682\n",
      "8685\n",
      "8686\n",
      "8687\n",
      "8690\n",
      "8702\n",
      "8708\n",
      "8711\n",
      "8718\n",
      "8720\n",
      "8721\n",
      "8722\n",
      "8724\n",
      "8728\n",
      "8731\n",
      "8734\n",
      "8736\n",
      "8740\n",
      "8741\n",
      "8746\n",
      "8747\n",
      "8752\n",
      "8756\n",
      "8761\n",
      "8763\n",
      "8766\n",
      "8779\n",
      "8793\n",
      "8797\n",
      "8806\n",
      "8815\n",
      "8820\n",
      "8821\n",
      "8834\n",
      "8852\n",
      "8868\n",
      "8871\n",
      "8875\n",
      "8876\n",
      "8878\n",
      "8879\n",
      "8886\n",
      "8887\n",
      "8888\n",
      "8892\n",
      "8895\n",
      "8900\n",
      "8908\n",
      "8910\n",
      "8912\n",
      "8917\n",
      "8924\n",
      "8928\n",
      "8938\n",
      "8943\n",
      "8945\n",
      "8946\n",
      "8949\n",
      "8951\n",
      "8956\n",
      "8967\n",
      "8970\n",
      "8975\n",
      "8979\n",
      "8983\n",
      "8989\n",
      "8991\n",
      "8993\n",
      "9001\n",
      "9007\n",
      "9013\n",
      "9016\n",
      "9018\n",
      "9020\n",
      "9029\n",
      "9035\n",
      "9039\n",
      "9042\n",
      "9043\n",
      "9044\n",
      "9051\n",
      "9060\n",
      "9078\n",
      "9091\n",
      "9094\n",
      "9097\n",
      "9112\n",
      "9116\n",
      "9120\n",
      "9122\n",
      "9134\n",
      "9136\n",
      "9139\n",
      "9143\n",
      "9151\n",
      "9156\n",
      "9159\n",
      "9179\n",
      "9182\n",
      "9184\n",
      "9188\n",
      "9195\n",
      "9201\n",
      "9203\n",
      "9208\n",
      "9209\n",
      "9215\n",
      "9220\n",
      "9236\n",
      "9237\n",
      "9240\n",
      "9248\n",
      "9258\n",
      "9262\n",
      "9268\n",
      "9272\n",
      "9278\n",
      "9285\n",
      "9293\n",
      "9300\n",
      "9308\n",
      "9310\n",
      "9320\n",
      "9332\n",
      "9346\n",
      "9349\n",
      "9351\n",
      "9352\n",
      "9355\n",
      "9356\n",
      "9364\n",
      "9371\n",
      "9377\n",
      "9378\n",
      "9379\n",
      "9399\n",
      "9403\n",
      "9405\n",
      "9407\n",
      "9415\n",
      "9426\n",
      "9427\n",
      "9445\n",
      "9450\n",
      "9452\n",
      "9469\n",
      "9475\n",
      "9478\n",
      "9479\n",
      "9480\n",
      "9488\n",
      "9489\n",
      "9493\n",
      "9498\n",
      "9499\n",
      "9508\n",
      "9511\n",
      "9542\n",
      "9548\n",
      "9553\n",
      "9558\n",
      "9569\n",
      "9572\n",
      "9573\n",
      "9578\n",
      "9587\n",
      "9588\n",
      "9590\n",
      "9596\n",
      "9601\n",
      "9607\n",
      "9617\n",
      "9622\n",
      "9626\n",
      "9643\n",
      "9645\n",
      "9647\n",
      "9650\n",
      "9653\n",
      "9656\n",
      "9665\n",
      "9668\n",
      "9675\n",
      "9687\n",
      "9689\n",
      "9695\n",
      "9698\n",
      "9699\n",
      "9704\n",
      "9714\n",
      "9717\n",
      "9720\n",
      "9725\n",
      "9726\n",
      "9731\n",
      "9733\n",
      "9736\n",
      "9738\n",
      "9740\n",
      "9743\n",
      "9744\n",
      "9748\n",
      "9749\n",
      "9753\n",
      "9755\n",
      "9758\n",
      "9760\n",
      "9769\n",
      "9779\n",
      "9786\n",
      "9795\n",
      "9810\n",
      "9817\n",
      "9826\n",
      "9836\n",
      "9837\n",
      "9839\n",
      "9846\n",
      "9857\n",
      "9866\n",
      "9869\n",
      "9873\n",
      "9878\n",
      "9891\n",
      "9896\n",
      "9901\n",
      "9905\n",
      "9913\n",
      "9917\n",
      "9921\n",
      "9922\n",
      "9932\n",
      "9934\n",
      "9936\n",
      "9944\n",
      "9953\n",
      "9961\n",
      "9975\n",
      "9980\n",
      "9981\n",
      "9986\n",
      "9987\n",
      "9994\n",
      "9995\n",
      "9998\n",
      "10003\n",
      "10008\n",
      "10014\n",
      "10016\n",
      "10025\n",
      "10027\n",
      "10037\n",
      "10039\n",
      "10040\n",
      "10046\n",
      "10056\n",
      "10057\n",
      "10061\n",
      "10082\n",
      "10091\n",
      "10095\n",
      "10105\n",
      "10107\n",
      "10115\n",
      "10120\n",
      "10127\n",
      "10128\n",
      "10133\n",
      "10135\n",
      "10139\n",
      "10140\n",
      "10153\n",
      "10155\n",
      "10166\n",
      "10176\n",
      "10181\n",
      "10185\n",
      "10187\n",
      "10204\n",
      "10206\n",
      "10214\n",
      "10215\n",
      "10218\n",
      "10229\n",
      "10236\n",
      "10247\n",
      "10249\n",
      "10258\n",
      "10265\n",
      "10266\n",
      "10274\n",
      "10275\n",
      "10279\n",
      "10280\n",
      "10292\n",
      "10296\n",
      "10309\n",
      "10310\n",
      "10315\n",
      "10318\n",
      "10321\n",
      "10340\n",
      "10343\n",
      "10348\n",
      "10351\n",
      "10354\n",
      "10355\n",
      "10359\n",
      "10362\n",
      "10364\n",
      "10365\n",
      "10370\n",
      "10381\n",
      "10382\n",
      "10389\n",
      "10391\n",
      "10399\n",
      "10409\n",
      "10417\n",
      "10419\n",
      "10420\n",
      "10425\n",
      "10447\n",
      "10456\n",
      "10462\n",
      "10464\n",
      "10465\n",
      "10469\n",
      "10470\n",
      "10472\n",
      "10475\n",
      "10488\n",
      "10494\n",
      "10505\n",
      "10513\n",
      "10515\n",
      "10522\n",
      "10524\n",
      "10526\n",
      "10532\n",
      "10547\n",
      "10550\n",
      "10554\n",
      "10562\n",
      "10565\n",
      "10573\n",
      "10575\n",
      "10595\n",
      "10606\n",
      "10616\n",
      "10617\n",
      "10620\n",
      "10622\n",
      "10626\n",
      "10631\n",
      "10637\n",
      "10656\n",
      "10661\n",
      "10665\n",
      "10672\n",
      "10677\n",
      "10687\n",
      "10688\n",
      "10690\n",
      "10691\n",
      "10692\n",
      "10697\n",
      "10698\n",
      "10701\n",
      "10717\n",
      "10722\n",
      "10723\n",
      "10727\n",
      "10729\n",
      "10731\n",
      "10738\n",
      "10742\n",
      "10743\n",
      "10751\n",
      "10755\n",
      "10759\n",
      "10763\n",
      "10767\n",
      "10774\n",
      "10775\n",
      "10785\n",
      "10800\n",
      "10807\n",
      "10819\n",
      "10827\n",
      "10838\n",
      "10839\n",
      "10840\n",
      "10861\n",
      "10874\n",
      "10878\n",
      "10891\n",
      "10902\n",
      "10905\n",
      "10918\n",
      "10919\n",
      "10921\n",
      "10925\n",
      "10926\n",
      "10944\n",
      "10947\n",
      "10949\n",
      "10951\n",
      "10960\n",
      "10968\n",
      "10975\n",
      "10977\n",
      "10981\n",
      "10984\n",
      "10991\n",
      "10997\n",
      "10999\n",
      "11001\n",
      "11004\n",
      "11005\n",
      "11006\n",
      "11009\n",
      "11015\n",
      "11016\n",
      "11023\n",
      "11038\n",
      "11052\n",
      "11063\n",
      "11081\n",
      "11092\n",
      "11095\n",
      "11103\n",
      "11111\n",
      "11114\n",
      "11115\n",
      "11119\n",
      "11120\n",
      "11127\n",
      "11135\n",
      "11148\n",
      "11158\n",
      "11164\n",
      "11167\n",
      "11182\n",
      "11183\n",
      "11185\n",
      "11196\n",
      "11197\n",
      "11199\n",
      "11206\n",
      "11215\n",
      "11220\n",
      "11222\n",
      "11236\n",
      "11240\n",
      "11241\n",
      "11242\n",
      "11249\n",
      "11259\n",
      "11261\n",
      "11267\n",
      "11273\n",
      "11275\n",
      "11289\n",
      "11291\n",
      "11295\n",
      "11314\n",
      "11316\n",
      "11320\n",
      "11323\n",
      "11325\n",
      "11330\n",
      "11332\n",
      "11333\n",
      "11336\n",
      "11343\n",
      "11350\n",
      "11353\n",
      "11357\n",
      "11365\n",
      "11366\n",
      "11367\n",
      "11369\n",
      "11375\n",
      "11379\n",
      "11393\n",
      "11398\n",
      "11402\n",
      "11409\n",
      "11410\n",
      "11415\n",
      "11434\n",
      "11438\n",
      "11441\n",
      "11445\n",
      "11455\n",
      "11458\n",
      "11467\n",
      "11468\n",
      "11476\n",
      "11478\n",
      "11496\n",
      "11497\n",
      "11508\n",
      "11522\n",
      "11532\n",
      "11534\n",
      "11535\n",
      "11536\n",
      "11539\n",
      "11543\n",
      "11554\n",
      "11558\n",
      "11563\n",
      "11564\n",
      "11565\n",
      "11569\n",
      "11580\n",
      "11584\n",
      "11591\n",
      "11595\n",
      "11598\n",
      "11603\n",
      "11604\n",
      "11605\n",
      "11609\n",
      "11621\n",
      "11622\n",
      "11625\n",
      "11639\n",
      "11644\n",
      "11646\n",
      "11654\n",
      "11671\n",
      "11673\n",
      "11674\n",
      "11681\n",
      "11691\n",
      "11692\n",
      "11695\n",
      "11699\n",
      "11707\n",
      "11713\n",
      "11728\n",
      "11747\n",
      "11748\n",
      "11752\n",
      "11754\n",
      "11755\n",
      "11760\n",
      "11769\n",
      "11772\n",
      "11773\n",
      "11776\n",
      "11778\n",
      "11789\n",
      "11790\n",
      "11791\n",
      "11793\n",
      "11801\n",
      "11804\n",
      "11806\n",
      "11815\n",
      "11827\n",
      "11830\n",
      "11841\n",
      "11849\n",
      "11856\n",
      "11857\n",
      "11859\n",
      "11860\n",
      "11869\n",
      "11875\n",
      "11884\n",
      "11890\n",
      "11893\n",
      "11895\n",
      "11902\n",
      "11903\n",
      "11910\n",
      "11914\n",
      "11918\n",
      "11919\n",
      "11920\n",
      "11938\n",
      "11940\n",
      "11946\n",
      "11952\n",
      "11954\n",
      "11959\n",
      "11962\n",
      "11970\n",
      "11982\n",
      "11983\n",
      "11989\n",
      "11998\n",
      "12001\n",
      "12002\n",
      "12004\n",
      "12007\n",
      "12008\n",
      "12010\n",
      "12013\n",
      "12014\n",
      "12023\n",
      "12027\n",
      "12029\n",
      "12030\n",
      "12044\n",
      "12046\n",
      "12051\n",
      "12067\n",
      "12072\n",
      "12078\n",
      "12085\n",
      "12089\n",
      "12102\n",
      "12105\n",
      "12107\n",
      "12108\n",
      "12110\n",
      "12111\n",
      "12114\n",
      "12116\n",
      "12128\n",
      "12135\n",
      "12139\n",
      "12140\n",
      "12142\n",
      "12143\n",
      "12149\n",
      "12150\n",
      "12153\n",
      "12158\n",
      "12169\n",
      "12170\n",
      "12180\n",
      "12181\n",
      "12182\n",
      "12188\n",
      "12191\n",
      "12192\n",
      "12196\n",
      "12204\n",
      "12208\n",
      "12212\n",
      "12214\n",
      "12220\n",
      "12222\n",
      "12232\n",
      "12235\n",
      "12238\n",
      "12242\n",
      "12243\n",
      "12244\n",
      "12246\n",
      "12259\n",
      "12265\n",
      "12272\n",
      "12275\n",
      "12278\n",
      "12279\n",
      "12285\n",
      "12286\n",
      "12290\n",
      "12295\n",
      "12296\n",
      "12325\n",
      "12326\n",
      "12337\n",
      "12338\n",
      "12343\n",
      "12345\n",
      "12346\n",
      "12347\n",
      "12355\n",
      "12362\n",
      "12367\n",
      "12368\n",
      "12381\n",
      "12384\n",
      "12391\n",
      "12394\n",
      "12400\n",
      "12403\n",
      "12422\n",
      "12425\n",
      "12426\n",
      "12428\n",
      "12429\n",
      "12434\n",
      "12435\n",
      "12440\n",
      "12449\n",
      "12484\n",
      "12519\n",
      "12525\n",
      "12528\n",
      "12535\n",
      "12538\n",
      "12540\n",
      "12541\n",
      "12543\n",
      "12544\n",
      "12545\n",
      "12551\n",
      "12552\n",
      "12557\n",
      "12559\n",
      "12569\n",
      "12571\n",
      "12573\n",
      "12576\n",
      "12577\n",
      "12579\n",
      "12580\n",
      "12581\n",
      "12591\n",
      "12609\n",
      "12611\n",
      "12621\n",
      "12632\n",
      "12638\n",
      "12652\n",
      "12660\n",
      "12663\n",
      "12670\n",
      "12679\n",
      "12689\n",
      "12692\n",
      "12697\n",
      "12702\n",
      "12704\n",
      "12709\n",
      "12718\n",
      "12726\n",
      "12730\n",
      "12732\n",
      "12744\n",
      "12749\n",
      "12755\n",
      "12762\n",
      "12765\n",
      "12766\n",
      "12769\n",
      "12772\n",
      "12777\n",
      "12780\n",
      "12789\n",
      "12795\n",
      "12800\n",
      "12801\n",
      "12806\n",
      "12813\n",
      "12814\n",
      "12815\n",
      "12818\n",
      "12829\n",
      "12837\n",
      "12838\n",
      "12845\n",
      "12847\n",
      "12850\n",
      "12851\n",
      "12853\n",
      "12855\n",
      "12860\n",
      "12861\n",
      "12863\n",
      "12864\n",
      "12867\n",
      "12873\n",
      "12881\n",
      "12886\n",
      "12889\n",
      "12896\n",
      "12904\n",
      "12919\n",
      "12927\n",
      "12933\n",
      "12939\n",
      "12948\n",
      "12951\n",
      "12958\n",
      "12961\n",
      "12967\n",
      "12968\n",
      "12971\n",
      "12973\n",
      "12988\n",
      "12989\n",
      "12991\n",
      "13006\n",
      "13009\n",
      "13015\n",
      "13017\n",
      "13019\n",
      "13023\n",
      "13026\n",
      "13039\n",
      "13046\n",
      "13047\n",
      "13053\n",
      "13057\n",
      "13060\n",
      "13062\n",
      "13065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13066\n",
      "13069\n",
      "13075\n",
      "13078\n",
      "13084\n",
      "13085\n",
      "13089\n",
      "13095\n",
      "13096\n",
      "13099\n",
      "13101\n",
      "13104\n",
      "13106\n",
      "13122\n",
      "13131\n",
      "13135\n",
      "13136\n",
      "13148\n",
      "13153\n",
      "13157\n",
      "13176\n",
      "13190\n",
      "13196\n",
      "13199\n",
      "13209\n",
      "13229\n",
      "13230\n",
      "13238\n",
      "13245\n",
      "13246\n",
      "13254\n",
      "13255\n",
      "13257\n",
      "13259\n",
      "13262\n",
      "13273\n",
      "13287\n",
      "13304\n",
      "13309\n",
      "13311\n",
      "13315\n",
      "13320\n",
      "13321\n",
      "13324\n",
      "13339\n",
      "13343\n",
      "13347\n",
      "13349\n",
      "13357\n",
      "13358\n",
      "13366\n",
      "13374\n",
      "13376\n",
      "13382\n",
      "13391\n",
      "13397\n",
      "13402\n",
      "13405\n",
      "13406\n",
      "13407\n",
      "13409\n",
      "13422\n",
      "13427\n",
      "13431\n",
      "13438\n",
      "13441\n",
      "13444\n",
      "13456\n",
      "13457\n",
      "13458\n",
      "13465\n",
      "13479\n",
      "13486\n",
      "13491\n",
      "13496\n",
      "13501\n",
      "13506\n",
      "13510\n",
      "13519\n",
      "13526\n",
      "13547\n",
      "13554\n",
      "13559\n",
      "13562\n",
      "13568\n",
      "13572\n",
      "13577\n",
      "13588\n",
      "13589\n",
      "13593\n",
      "13594\n",
      "13602\n",
      "13606\n",
      "13621\n",
      "13622\n",
      "13630\n",
      "13633\n",
      "13634\n",
      "13635\n",
      "13645\n",
      "13646\n",
      "13652\n",
      "13656\n",
      "13661\n",
      "13667\n",
      "13682\n",
      "13688\n",
      "13689\n",
      "13691\n",
      "13693\n",
      "13694\n",
      "13700\n",
      "13716\n",
      "13728\n",
      "13740\n",
      "13746\n",
      "13751\n",
      "13765\n",
      "13774\n",
      "13790\n",
      "13806\n",
      "13809\n",
      "13826\n",
      "13836\n",
      "13844\n",
      "13855\n",
      "13857\n",
      "13862\n",
      "13874\n",
      "13884\n",
      "13885\n",
      "13892\n",
      "13893\n",
      "13894\n",
      "13899\n",
      "13905\n",
      "13911\n",
      "13912\n",
      "13913\n",
      "13921\n",
      "13922\n",
      "13928\n",
      "13930\n",
      "13933\n",
      "13939\n",
      "13940\n",
      "13944\n",
      "13952\n",
      "13956\n",
      "13960\n",
      "13962\n",
      "13965\n",
      "13972\n",
      "13973\n",
      "13978\n",
      "13995\n",
      "13996\n",
      "14002\n",
      "14010\n",
      "14012\n",
      "14013\n",
      "14016\n",
      "14017\n",
      "14022\n",
      "14025\n",
      "14031\n",
      "14034\n",
      "14039\n",
      "14046\n",
      "14055\n",
      "14061\n",
      "14070\n",
      "14075\n",
      "14076\n",
      "14077\n",
      "14080\n",
      "14090\n",
      "14091\n",
      "14094\n",
      "14101\n",
      "14109\n",
      "14118\n",
      "14124\n",
      "14125\n",
      "14129\n",
      "14139\n",
      "14144\n",
      "14153\n",
      "14157\n",
      "14167\n",
      "14169\n",
      "14181\n",
      "14182\n",
      "14183\n",
      "14188\n",
      "14191\n",
      "14193\n",
      "14196\n",
      "14201\n",
      "14202\n",
      "14206\n",
      "14217\n",
      "14218\n",
      "14221\n",
      "14223\n",
      "14229\n",
      "14231\n",
      "14235\n",
      "14238\n",
      "14243\n",
      "14245\n",
      "14255\n",
      "14258\n",
      "14259\n",
      "14261\n",
      "14269\n",
      "14281\n",
      "14286\n",
      "14293\n",
      "14300\n",
      "14315\n",
      "14319\n",
      "14344\n",
      "14346\n",
      "14351\n",
      "14352\n",
      "14366\n",
      "14370\n",
      "14379\n",
      "14384\n",
      "14389\n",
      "14394\n",
      "14395\n",
      "14414\n",
      "14416\n",
      "14419\n",
      "14423\n",
      "14425\n",
      "14427\n",
      "14428\n",
      "14434\n",
      "14444\n",
      "14454\n",
      "14461\n",
      "14477\n",
      "14488\n",
      "14490\n",
      "14519\n",
      "14532\n",
      "14534\n",
      "14541\n",
      "14551\n",
      "14552\n",
      "14561\n",
      "14562\n",
      "14576\n",
      "14577\n",
      "14587\n",
      "14589\n",
      "14593\n",
      "14601\n",
      "14605\n",
      "14617\n",
      "14621\n",
      "14622\n",
      "14624\n",
      "14629\n",
      "14632\n",
      "14636\n",
      "14637\n",
      "14644\n",
      "14650\n",
      "14652\n",
      "14655\n",
      "14661\n",
      "14662\n",
      "14674\n",
      "14675\n",
      "14677\n",
      "14698\n",
      "14701\n",
      "14702\n",
      "14707\n",
      "14711\n",
      "14715\n",
      "14723\n",
      "14725\n",
      "14728\n",
      "14734\n",
      "14736\n",
      "14739\n",
      "14745\n",
      "14760\n",
      "14769\n",
      "14775\n",
      "14780\n",
      "14781\n",
      "14789\n",
      "14790\n",
      "14793\n",
      "14799\n",
      "14813\n",
      "14820\n",
      "14842\n",
      "14848\n",
      "14852\n",
      "14859\n",
      "14865\n",
      "14866\n",
      "14870\n",
      "14883\n",
      "14885\n",
      "14886\n",
      "14889\n",
      "14893\n",
      "14899\n",
      "14901\n",
      "14924\n",
      "14926\n",
      "14931\n",
      "14934\n",
      "14936\n",
      "14938\n",
      "14939\n",
      "14941\n",
      "14943\n",
      "14945\n",
      "14946\n",
      "14947\n",
      "14963\n",
      "14964\n",
      "14969\n",
      "14974\n",
      "14978\n",
      "14989\n",
      "15001\n",
      "15007\n",
      "15009\n",
      "15011\n",
      "15026\n",
      "15027\n",
      "15038\n",
      "15057\n"
     ]
    }
   ],
   "source": [
    "# Misclassified points\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "for i in range(len(X_test)):\n",
    "    if (y_pred[i][y_test[i]] < y_pred[i][1 - y_test[i]]):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_influence_loss(X_train, y_train, X_test, y_test, idx):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba([X_test[idx]])[0]\n",
    "    loss_0 = - y_test[idx] * math.log(y_pred[1]) - (1 - y_test[idx]) * math.log(y_pred[0])\n",
    "\n",
    "    delta_loss = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_removed = np.delete(X_train, i, 0)\n",
    "        y_removed = y_train.drop(index=i, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "        y_pred = clf.predict_proba([X_test[idx]])[0]\n",
    "        loss_i = - y_test[idx] * math.log(y_pred[1]) - (1 - y_test[idx]) * math.log(y_pred[0])\n",
    "        delta_loss_i = loss_i - loss_0\n",
    "        delta_loss.append(delta_loss_i)\n",
    "    \n",
    "    return delta_loss\n",
    "\n",
    "# ix = 203 #misprediction\n",
    "# v1 = del_L_del_theta_i(num_params, y_test[ix], X_test[ix], y_pred_test[ix])\n",
    "infs_1 = first_order_influence(del_L_del_theta, hinv_v, len(X_train))\n",
    "# delta_loss_gt = ground_truth_influence_loss(X_train, y_train, X_test, y_test, ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = (0.5,0.2,0.5)\n",
    "# fig = plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "plt.scatter(infs_1, delta_loss_gt, c=colors, alpha=0.5)\n",
    "plt.xlabel('First-order influence')\n",
    "plt.ylabel('Ground truth influence')\n",
    "ax.plot((0,1), 'r--')\n",
    "# ax.plot([0,1],[0,1], transform=ax.transAxes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation between first-order and ground truth influences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 30162 and the array at index 1 has size 500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d46aab0f573b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m print(\"Spearman rank correlation between 1st order inf and ground truth inf: \", \n\u001b[0;32m----> 2\u001b[0;31m       stats.spearmanr(spdgt, infs_1)[0])\n\u001b[0m\u001b[1;32m      3\u001b[0m print(\"Pearson correlation coefficient between 1st order inf and ground truth inf: \", \n\u001b[1;32m      4\u001b[0m       stats.pearsonr(spdgt, infs_1)[0])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36mspearmanr\u001b[0;34m(a, b, axis, nan_policy)\u001b[0m\n\u001b[1;32m   4181\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_chk_asarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxisout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4183\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4184\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4185\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcolumn_stack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mcolumn_stack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 30162 and the array at index 1 has size 500"
     ]
    }
   ],
   "source": [
    "print(\"Spearman rank correlation between 1st order inf and ground truth inf: \", \n",
    "      stats.spearmanr(spdgt, infs_1)[0])\n",
    "print(\"Pearson correlation coefficient between 1st order inf and ground truth inf: \", \n",
    "      stats.pearsonr(spdgt, infs_1)[0])\n",
    "\n",
    "colors = (0.5,0.2,0.5)\n",
    "fig = plt.figure()\n",
    "plt.scatter(infs_1, spdgt, c=colors, alpha=0.5)\n",
    "plt.xlabel('First-order influence')\n",
    "plt.ylabel('Ground truth influence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Space Partitioner for reducing bias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital', 'occupation', 'relationship', 'race', 'gender', 'capgain', 'caploss', 'hours', 'country', 'income']\n",
    "df_train = pd.read_csv('adult.data', names=cols, sep=\",\")\n",
    "df_test = pd.read_csv('adult.test', names=cols, sep=\",\")\n",
    "\n",
    "def preprocess(df):\n",
    "    df.isin(['?']).sum(axis=0)\n",
    "\n",
    "    # replace missing values (?) to nan and then drop the columns\n",
    "    df['country'] = df['country'].replace('?',np.nan)\n",
    "    df['workclass'] = df['workclass'].replace('?',np.nan)\n",
    "    df['occupation'] = df['occupation'].replace('?',np.nan)\n",
    "\n",
    "    # dropping the NaN rows now\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    df['income'] = df['income'].map({'<=50K': 0, '>50K': 1}).astype(int)\n",
    "    df = df.drop(columns=['fnlwgt', 'education.num', 'country', 'capgain', 'caploss'])\n",
    "    return df\n",
    "\n",
    "df_train = preprocess(df_train)\n",
    "df_test = preprocess(df_test)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "X_train_ = df_train.drop(columns='income')\n",
    "y_train_ = df_train['income']\n",
    "\n",
    "X_test_ = df_test.drop(columns='income')\n",
    "y_test_ = df_test['income']\n",
    "\n",
    "# size=100\n",
    "# X_train = X_train[0:size]\n",
    "# y_train = y_train[0:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column passed:  relationship\n",
      "Val:  Unmarried\n",
      "Column passed:  race\n",
      "Val:  White\n",
      "Column passed:  workclass\n",
      "Val:  Private\n",
      "Column passed:  marital\n",
      "Val:  Married-civ-spouse\n",
      "{'splitCol': 'marital', 'numRows': 30162, 'infs': [-0.2083536018505245, -0.02827855385557211, -0.1890788115264716, -0.20065933554743223, -0.19875608458881688, -0.20070750164582, -0.19730181883177864], 'vals': ['Never-married', 'Married-civ-spouse', 'Divorced', 'Married-spouse-absent', 'Separated', 'Married-AF-spouse', 'Widowed'], 'idxs': [20436, 16097, 25948, 29792, 29223, 30141, 29335]}\n",
      "Depth:  1\n",
      "Column passed:  relationship\n",
      "Val:  Husband\n",
      "Column passed:  relationship\n",
      "Val:  Unmarried\n",
      "Column passed:  race\n",
      "Val:  White\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Depth:  2\n",
      "Column passed:  relationship\n",
      "Val:  Not-in-family\n",
      "Column passed:  relationship\n",
      "Val:  Unmarried\n",
      "Column passed:  workclass\n",
      "Val:  Federal-gov\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Depth:  2\n",
      "Column passed:  relationship\n",
      "Val:  Not-in-family\n",
      "Column passed:  relationship\n",
      "Val:  Own-child\n",
      "Column passed:  relationship\n",
      "Val:  Unmarried\n",
      "Column passed:  workclass\n",
      "Val:  Private\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Depth:  2\n",
      "Column passed:  relationship\n",
      "Val:  Unmarried\n",
      "Column passed:  race\n",
      "Val:  White\n",
      "Column passed:  workclass\n",
      "Val:  Private\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Depth:  2\n",
      "Column passed:  relationship\n",
      "Val:  Unmarried\n",
      "Column passed:  race\n",
      "Val:  White\n",
      "Column passed:  workclass\n",
      "Val:  Private\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Depth:  2\n",
      "Column passed:  relationship\n",
      "Val:  Unmarried\n",
      "Column passed:  race\n",
      "Val:  White\n",
      "Column passed:  workclass\n",
      "Val:  Private\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Depth:  2\n",
      "Column passed:  relationship\n",
      "Val:  Own-child\n",
      "Column passed:  relationship\n",
      "Val:  Unmarried\n",
      "Column passed:  race\n",
      "Val:  White\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Depth:  2\n"
     ]
    }
   ],
   "source": [
    "def computeFairness(y_pred, X_test): \n",
    "    protected_idx = X_test[X_test['gender']=='Female'].index\n",
    "    numProtected = len(protected_idx)\n",
    "    privileged_idx = X_test[X_test['gender']=='Male'].index\n",
    "    numPrivileged = len(privileged_idx)\n",
    "    \n",
    "    p_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        p_protected += y_pred[protected_idx[i]][1]\n",
    "    p_protected /= len(protected_idx)\n",
    "    \n",
    "    p_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        p_privileged += y_pred[privileged_idx[i]][1]\n",
    "    p_privileged /= len(privileged_idx)\n",
    "    \n",
    "    spd = p_protected - p_privileged\n",
    "    return spd\n",
    "\n",
    "def getInfluenceOfSet(indices, f, X_train, y_train, X_test, X_test_, method): \n",
    "    del_f = 0\n",
    "    if (method == 1):\n",
    "        X = X_train.drop(index=indices, inplace=False)\n",
    "        y = y_train.drop(index=indices, inplace=False)\n",
    "        if len(y.unique()) < 2:\n",
    "            return 0\n",
    "        clf.fit(X, y)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        del_f = computeFairness(y_pred, X_test_)\n",
    "    elif (method == 2):\n",
    "        for i in range(len(indices)):\n",
    "            del_f += infs_1[indices[i]]\n",
    "    elif (method == 3):\n",
    "#         second_order_influence(X_train, U, size, del_L_del_theta, hessian_all_points)\n",
    "        size_hvp = 1\n",
    "        params_f_2 = second_order_influence(X_train, indices, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "        del_f = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "    return  del_f\n",
    "\n",
    "def getSplitVal(infs):\n",
    "    return (np.argmax(np.asarray(infs)))\n",
    "\n",
    "def getSplitAttribute(cols, cols_continuous, X_train, y_train, X_test, X_train_, X_test_, method):\n",
    "    splitCol, numRows, score = None, len(X_train_), abs(spd_0)\n",
    "    infs = []\n",
    "    vals = []\n",
    "    idxs = []\n",
    "    for i in range(len(cols)):\n",
    "        col = cols[i]\n",
    "        infs_i = []\n",
    "        vals_i = []\n",
    "        idxs_i = []\n",
    "        if col not in cols_continuous:\n",
    "            colVals = X_train_[col].unique()\n",
    "            for val in colVals:\n",
    "                idx = X_train_[X_train_[col] == val].index\n",
    "                idxs_i.append(len(X_train)-len(idx))\n",
    "                infs_i.append(getInfluenceOfSet(idx, spd_0, X_train, y_train, X_test, X_test_, method))\n",
    "                vals_i.append(val)\n",
    "                ix = getSplitVal(infs_i)\n",
    "                if (abs(infs_i[ix]) < score):\n",
    "                    print(\"Column passed: \", col)\n",
    "                    print(\"Val: \", val)\n",
    "                    splitCol = col\n",
    "                    splitIdx = i\n",
    "                    score = abs(infs_i[ix])\n",
    "        infs.append(infs_i)\n",
    "        vals.append(vals_i)\n",
    "        idxs.append(idxs_i)\n",
    "    return {'splitCol':splitCol, 'numRows':numRows, \n",
    "            'infs': infs[splitIdx], 'vals': vals[splitIdx], 'idxs': idxs[splitIdx]}\n",
    "\n",
    "def partition(node, maxDepth, minSize, depth, cols, cols_continuous, \n",
    "              X_train_, y_train_, X_train, X_test_, X_test, method):\n",
    "    print(\"Depth: \", depth)\n",
    "    if depth >= maxDepth or node['numRows'] < minSize:\n",
    "        node['children'] = None\n",
    "        return\n",
    "    col = node['splitCol']\n",
    "    if col not in cols_continuous:\n",
    "        vals = X_train_[col].unique()\n",
    "        child = [None] * len(vals)\n",
    "        for i in range(len(vals)):\n",
    "            idx = X_train_[X_train_[col] == vals[i]].index \n",
    "            X = X_train.drop(index=idx, inplace=False)\n",
    "            y = y_train.drop(index=idx, inplace=False)\n",
    "            X_ = X_train_.drop(index=idx, inplace=False)\n",
    "            if len(X) < minSize:\n",
    "                node['children'] = None\n",
    "            else:\n",
    "                cols_ = copy.deepcopy(cols)\n",
    "                cols_.remove(col)\n",
    "                child[i] = getSplitAttribute(cols_, cols_continuous,\n",
    "                                             X, y, X_test, X_, X_test_, method)\n",
    "                child[i]['col'] = col\n",
    "                child[i]['val'] = vals[i]\n",
    "                partition(child[i], maxDepth, minSize, depth + 1, cols_, cols_continuous, \n",
    "                  X_, y, X, X_test_, X_test, method)\n",
    "    node['children'] = child\n",
    "\n",
    "def buildTree(X_train_, X_train, maxDepth, minSize, method):\n",
    "    cols = copy.deepcopy(X_train_.columns).tolist()\n",
    "    cols_continuous = ['age', 'hours']\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X_train_orig.columns)\n",
    "    cols = list(set(cols) - set(cols_continuous))\n",
    "    root = getSplitAttribute(cols, cols_continuous,\n",
    "                             X_train, y_train, X_test, X_train_, X_test_, method)\n",
    "    print(root)\n",
    "    partition(root, maxDepth, minSize, 1, cols, cols_continuous,\n",
    "              X_train_, y_train_, X_train, X_test_, X_test, method)\n",
    "    return root\n",
    "\n",
    "method = 1\n",
    "dtree = buildTree(X_train_, X_train, 2, 100, method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'splitCol': 'marital',\n",
       " 'numRows': 30162,\n",
       " 'infs': [-0.2083536018505245,\n",
       "  -0.02827855385557211,\n",
       "  -0.1890788115264716,\n",
       "  -0.20065933554743223,\n",
       "  -0.19875608458881688,\n",
       "  -0.20070750164582,\n",
       "  -0.19730181883177864],\n",
       " 'vals': ['Never-married',\n",
       "  'Married-civ-spouse',\n",
       "  'Divorced',\n",
       "  'Married-spouse-absent',\n",
       "  'Separated',\n",
       "  'Married-AF-spouse',\n",
       "  'Widowed'],\n",
       " 'idxs': [20436, 16097, 25948, 29792, 29223, 30141, 29335],\n",
       " 'children': [{'splitCol': 'gender',\n",
       "   'numRows': 20436,\n",
       "   'infs': [-0.05522396149784553, -0.09339174663761593],\n",
       "   'vals': ['Male', 'Female'],\n",
       "   'idxs': [5470, 14966],\n",
       "   'col': 'marital',\n",
       "   'val': 'Never-married',\n",
       "   'children': None},\n",
       "  {'splitCol': 'occupation',\n",
       "   'numRows': 16097,\n",
       "   'infs': [-0.0089975462644349,\n",
       "    -0.02940095693213822,\n",
       "    -0.04129455247150572,\n",
       "    -0.01219578883237249,\n",
       "    -0.020033867671982286,\n",
       "    -0.013187659058612106,\n",
       "    -0.029191105506546675,\n",
       "    -0.024927399301536438,\n",
       "    -0.026194908337570252,\n",
       "    -0.02611891490945213,\n",
       "    -0.04062035030949793,\n",
       "    -0.04155946504785883,\n",
       "    -0.026674015162344744,\n",
       "    -0.028097924725885404],\n",
       "   'vals': ['Adm-clerical',\n",
       "    'Handlers-cleaners',\n",
       "    'Other-service',\n",
       "    'Prof-specialty',\n",
       "    'Sales',\n",
       "    'Farming-fishing',\n",
       "    'Machine-op-inspct',\n",
       "    'Exec-managerial',\n",
       "    'Tech-support',\n",
       "    'Craft-repair',\n",
       "    'Protective-serv',\n",
       "    'Transport-moving',\n",
       "    'Armed-Forces',\n",
       "    'Priv-house-serv'],\n",
       "   'idxs': [13347,\n",
       "    15204,\n",
       "    13584,\n",
       "    14125,\n",
       "    14144,\n",
       "    15683,\n",
       "    15100,\n",
       "    14500,\n",
       "    15584,\n",
       "    14591,\n",
       "    15832,\n",
       "    15507,\n",
       "    16091,\n",
       "    15969],\n",
       "   'col': 'marital',\n",
       "   'val': 'Married-civ-spouse',\n",
       "   'children': None},\n",
       "  {'splitCol': 'gender',\n",
       "   'numRows': 25948,\n",
       "   'infs': [-0.08535082648751632, -0.09247278611058918],\n",
       "   'vals': ['Male', 'Female'],\n",
       "   'idxs': [7253, 18695],\n",
       "   'col': 'marital',\n",
       "   'val': 'Divorced',\n",
       "   'children': None},\n",
       "  {'splitCol': 'gender',\n",
       "   'numRows': 29792,\n",
       "   'infs': [-0.0994233437389613, -0.12670982665702807],\n",
       "   'vals': ['Male', 'Female'],\n",
       "   'idxs': [9593, 20199],\n",
       "   'col': 'marital',\n",
       "   'val': 'Married-spouse-absent',\n",
       "   'children': None},\n",
       "  {'splitCol': 'gender',\n",
       "   'numRows': 29223,\n",
       "   'infs': [-0.10569201023538384, -0.11201309623842412],\n",
       "   'vals': ['Male', 'Female'],\n",
       "   'idxs': [9208, 20015],\n",
       "   'col': 'marital',\n",
       "   'val': 'Separated',\n",
       "   'children': None},\n",
       "  {'splitCol': 'gender',\n",
       "   'numRows': 30141,\n",
       "   'infs': [-0.12635036811093395, -0.12809545595031435],\n",
       "   'vals': ['Male', 'Female'],\n",
       "   'idxs': [9770, 20371],\n",
       "   'col': 'marital',\n",
       "   'val': 'Married-AF-spouse',\n",
       "   'children': None},\n",
       "  {'splitCol': 'gender',\n",
       "   'numRows': 29335,\n",
       "   'infs': [-0.1158902922570635, -0.11684756683883646],\n",
       "   'vals': ['Male', 'Female'],\n",
       "   'idxs': [9096, 20239],\n",
       "   'col': 'marital',\n",
       "   'val': 'Widowed',\n",
       "   'children': None}]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking ground truth, first-order and second-order influences for a set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'education.num', 'hours', 'gender_Female', 'gender_Male',\n",
       "       'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black',\n",
       "       'race_Other', 'race_White', 'marital_Divorced',\n",
       "       'marital_Married-AF-spouse', 'marital_Married-civ-spouse',\n",
       "       'marital_Married-spouse-absent', 'marital_Never-married',\n",
       "       'marital_Separated', 'marital_Widowed', 'workclass_Federal-gov',\n",
       "       'workclass_Local-gov', 'workclass_Private', 'workclass_Self-emp-inc',\n",
       "       'workclass_Self-emp-not-inc', 'workclass_State-gov',\n",
       "       'workclass_Without-pay', 'relationship_Husband',\n",
       "       'relationship_Not-in-family', 'relationship_Other-relative',\n",
       "       'relationship_Own-child', 'relationship_Unmarried', 'relationship_Wife',\n",
       "       'occupation_Adm-clerical', 'occupation_Armed-Forces',\n",
       "       'occupation_Craft-repair', 'occupation_Exec-managerial',\n",
       "       'occupation_Farming-fishing', 'occupation_Handlers-cleaners',\n",
       "       'occupation_Machine-op-inspct', 'occupation_Other-service',\n",
       "       'occupation_Priv-house-serv', 'occupation_Prof-specialty',\n",
       "       'occupation_Protective-serv', 'occupation_Sales',\n",
       "       'occupation_Tech-support', 'occupation_Transport-moving'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Rows removed:  4312\n",
      "#Rows left:  25850\n",
      "Ground truth fairness of subset:  -0.20625215414793924\n",
      "Ground truth influence of subset:  -0.00565844323815351\n",
      "First-order influence:  -0.0022943232829864472\n",
      "Second-order influence:  -0.004167267551007168\n"
     ]
    }
   ],
   "source": [
    "predicates = ['marital_Never-married', 'gender_Female']\n",
    "idx = X_train_orig[(X_train_orig[predicates[0]] == 1)\n",
    "                   & (X_train_orig[predicates[1]] == 1) \n",
    "                  ].index \n",
    "print(\"#Rows removed: \", len(idx))\n",
    "print(\"#Rows left: \", len(X_train) - len(idx))\n",
    "X = np.delete(X_train, idx, 0)\n",
    "y = y_train.drop(index=idx, inplace=False)\n",
    "clf.fit(X, y)\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "print(\"Ground truth fairness of subset: \", computeFairness(y_pred_test, X_test_orig))\n",
    "print(\"Ground truth influence of subset: \", computeFairness(y_pred_test, X_test_orig) - spd_0)\n",
    "\n",
    "del_f_1 = 0\n",
    "for i in range(len(idx)):\n",
    "    del_f_1 += infs_1[idx[i]]\n",
    "print(\"First-order influence: \", del_f_1)\n",
    "\n",
    "size_hvp = 1\n",
    "params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "print(\"Second-order influence: \", del_f_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-truth subset, Add 1st-order inf individual, Add ground-truth inf individual, Second-order subset influence\n",
      "0.0009629809259190658, 0.0007608505954095434, 0, 0.0009548758946811795\n",
      "-0.0024558415138332956, -0.0019834973549170565, 0, -0.0024435206041296758\n",
      "0.0007159796115257799, 0.0005817511282417933, 0, 0.0007062044545100855\n",
      "0.0016977733073159085, 0.0012353292323012293, 0, 0.0016106707249222418\n",
      "-0.0008390032699661376, -0.000794650035863751, 0, -0.0009214936138884197\n",
      "0.0018752947121944419, 0.0015120618885418689, 0, 0.0018605853483757773\n",
      "0.0047508936878143115, 0.003793923999265885, 0, 0.004636187042642229\n",
      "-0.0008180093770101537, -0.000686133938715342, 0, -0.0009453824554443728\n",
      "-0.00097319105268176, -0.0007660839715716599, 0, -0.0009792674820370327\n",
      "0.0016469861163182498, 0.001342382859108319, 0, 0.001636853984906911\n",
      "0.001218346289418748, 0.001030334564643214, 0, 0.0012184676544054083\n",
      "0.001000833245967725, 0.0008488569873065311, 0, 0.0010078183911659024\n",
      "-0.0004866763661370843, -0.0003610104910981604, 0, -0.0004696710812161898\n",
      "0.001897270729228101, 0.0016216638341432067, 0, 0.0018785835633862566\n",
      "0.0005126457310338783, 0.0004148293082061451, 0, 0.000498007473479655\n",
      "-0.0033166825604644956, -0.002760899935410087, 0, -0.0033718175976245267\n",
      "0.0015996123182563382, 0.0011676365840611763, 0, 0.001515948707055887\n",
      "0.0017862504540413493, 0.0013116510956466345, 0, 0.0016891358379195083\n",
      "-0.000788163619832527, -0.0005897112813646712, 0, -0.0008268387150507074\n",
      "0.0007814030675673544, 0.0006028884311215365, 0, 0.0007511530685115982\n",
      "-0.0029330770898801928, -0.0023617218220201323, 0, -0.0029424724407913975\n",
      "0.00018397668377040377, 0.0001508727687988297, 0, 0.00017343792221683504\n",
      "-0.0006119343924154141, -0.00047845619425279174, 0, -0.0006155946044609082\n",
      "-0.0014446061656331333, -0.0011560722188584417, 0, -0.0014677557358784357\n",
      "0.002581664698908831, 0.0020623365473811882, 0, 0.002579302432104428\n",
      "-0.0017758382790217797, -0.0014027229510277816, 0, -0.0017817514190464233\n",
      "0.0002823116836558337, 0.00023959811736561906, 0, 0.00027694305069504374\n",
      "0.0004140306048781017, 0.0003597864998594199, 0, 0.0004264580329534955\n",
      "0.0007853702364644943, 0.0006347760302722921, 0, 0.000765184958458794\n",
      "-0.0025744148122749666, -0.0020906455596325646, 0, -0.0025811495459200154\n",
      "0.0013765749237405478, 0.0011297618574626698, 0, 0.0013784656755750063\n",
      "-0.0026536447281900744, -0.002108167632683453, 0, -0.0026623838851322582\n",
      "-0.002384489315734817, -0.0019639555804021017, 0, -0.0024855504626290622\n",
      "0.0011706860470001024, 0.0009977991219201473, 0, 0.0011885498994256638\n",
      "0.0017465390698883088, 0.0013990661334447143, 0, 0.001723660996562753\n",
      "-0.000663880864949562, -0.0005512432402986463, 0, -0.000676702145445332\n",
      "-0.001979001753355969, -0.0015646815590200454, 0, -0.0019642446366532314\n",
      "-0.0003303485157434838, -0.000252432564812675, 0, -0.00032319325671608637\n",
      "-0.0001339909354330926, -4.9123881547759165e-05, 0, -0.0001242568468023787\n",
      "-0.0016916847746257235, -0.001377720686715722, 0, -0.0017004150055899999\n",
      "-0.0041432359749130065, -0.0032739260832998574, 0, -0.004101889257349411\n",
      "0.00031426674893736406, 0.00018532552053032475, 0, 0.0002872615689118645\n",
      "-0.0016456620412285683, -0.0013932065144303919, 0, -0.0016325648962430299\n",
      "-0.0002128359586353601, -0.000142774271627157, 0, -0.00019358541126560813\n",
      "0.003026383956078127, 0.002460885279225245, 0, 0.0030233284866769588\n",
      "0.0014111152879023037, 0.0011224400382774404, 0, 0.0013988413295908336\n",
      "-0.00030194693419416385, -0.0002816327866695278, 0, -0.0002991950592661133\n",
      "0.0008134415571311171, 0.0006131270543370073, 0, 0.000798341393579644\n",
      "0.003571513521543085, 0.0028383531433927144, 0, 0.0035384688502375556\n",
      "-0.0013815400497401154, -0.001097174582016734, 0, -0.0013956002061057782\n",
      "0.0005183001741060622, 0.00039392895511939524, 0, 0.000495941337923552\n",
      "-0.0013884391060253187, -0.001156743418387385, 0, -0.0013766424837659069\n",
      "-0.002063856838974698, -0.0017367387187512378, 0, -0.0021893394848039\n",
      "-0.002010938031109133, -0.0015862804332853681, 0, -0.0020084751669779545\n",
      "-0.002097578424681612, -0.001680479677621414, 0, -0.0020947984659661044\n",
      "0.0008220032165865976, 0.0006713033972622203, 0, 0.0008089611022243778\n",
      "0.0011637112290045493, 0.0009563645012852659, 0, 0.0011609046590201578\n",
      "0.00042757335062776347, 0.0003116084799377974, 0, 0.0004189694093757642\n",
      "-0.0003546114704571224, -0.0003544117897699137, 0, -0.00043242924571638145\n",
      "0.00017810045251359652, 6.269251007299923e-05, 0, 5.896504075385078e-05\n",
      "-0.000718286476464769, -0.0005348799891589659, 0, -0.000765563906554442\n",
      "0.0017400315061928961, 0.0014206008020637157, 0, 0.001731165579627519\n",
      "-0.004449256931221723, -0.003509861827745763, 0, -0.004455398313043082\n",
      "0.0006706335906658534, 0.0004331177947721401, 0, 0.0005862210557979619\n",
      "-0.00020920858841899648, -0.00026132562901767244, 0, -0.00032862233063969584\n",
      "7.065766731031986e-05, 2.202907128354846e-05, 0, -1.9991669664090062e-05\n",
      "0.001893670071651865, 0.0015255818751264377, 0, 0.0018641552423634706\n",
      "0.002036602380625696, 0.0014876960083108097, 0, 0.001947274093341566\n",
      "0.0013765803381068364, 0.0009887540178627322, 0, 0.0012573375026923403\n",
      "0.002812161142812769, 0.0021513285141796417, 0, 0.0027114519304790655\n",
      "-0.0020927018107756157, -0.0016470928723574352, 0, -0.0020954184541991266\n",
      "0.0009004915257797008, 0.0006484736101091581, 0, 0.0007865068519400571\n",
      "-0.00021515038418729038, -0.0001835686888069504, 0, -0.00021820095875251362\n",
      "0.002695507321912316, 0.002173051601476332, 0, 0.002701944627038639\n",
      "0.002144956370262019, 0.0016171062933850128, 0, 0.0019974105912119802\n",
      "-0.0029614439404129222, -0.0023599866788661184, 0, -0.0029531277628888604\n",
      "-0.0032458939311514334, -0.0025837621053616967, 0, -0.0032540477003471283\n",
      "-0.00163451507162915, -0.001306823334942063, 0, -0.0016326703733844257\n",
      "0.0010473135697174896, 0.0008207939303889977, 0, 0.0010463874974744058\n",
      "0.0016012137848195884, 0.0012102573368037424, 0, 0.0015042094462955232\n",
      "0.002310067692904205, 0.0018544180104695224, 0, 0.00231141537640994\n",
      "0.001479551867514417, 0.0010367199882054523, 0, 0.0013885988001444464\n",
      "-0.003251750196952924, -0.0026704278349045296, 0, -0.003254878593442571\n",
      "0.0004479611064798772, 0.0003600377405737655, 0, 0.00045875951992107656\n",
      "-0.0015635853623232332, -0.001305991873088007, 0, -0.0015263438743588745\n",
      "0.000984688100185288, 0.0007831265089833258, 0, 0.0009745384911420683\n",
      "0.0022637955648899533, 0.0017905296683794984, 0, 0.002259809881996047\n",
      "0.002974657157510663, 0.002456753187953795, 0, 0.0029891335805461642\n",
      "0.002053425254916502, 0.0016340558217290326, 0, 0.0020378743278608416\n",
      "-0.002016433949104196, -0.0016234421293230884, 0, -0.002019137813411129\n",
      "0.0010537044880756796, 0.0007155907595263872, 0, 0.0009757116481371524\n",
      "-0.0005869470309735614, -0.0005302487977965715, 0, -0.0006466987664477555\n",
      "0.002324668807249436, 0.0018995129015431129, 0, 0.002312525745099786\n",
      "-0.0038221595534868036, -0.0030407536853617016, 0, -0.003778758989024411\n",
      "0.0029844355074848117, 0.0023736336352308627, 0, 0.0030143890136493973\n",
      "0.0008730092598309702, 0.0006946920571357802, 0, 0.0009005138926727081\n",
      "0.005917311972390205, 0.0047612329907528875, 0, 0.005975419248769056\n",
      "-0.0004181759433323218, -0.0004567982198824131, 0, -0.0005086172734426182\n",
      "-0.0016412586526728667, -0.0012864861003085704, 0, -0.0017063390663525028\n",
      "-0.0013195722374447771, -0.0010831978343757694, 0, -0.0013475757159260038\n"
     ]
    }
   ],
   "source": [
    "# ground truth predicates\n",
    "# predicates = ['marital_Married-civ-spouse']\n",
    "predicates = ['marital_Never-married']\n",
    "idx = X_train_orig[(X_train_orig[predicates[0]] == 1)\n",
    "#                    & (X_train_orig[predicates[1]] == 1) \n",
    "                  ].index \n",
    "# print(predicates[0])\n",
    "# print(\"#Rows removed: \", len(idx))\n",
    "\n",
    "print(\"Ground-truth subset, Add 1st-order inf individual, Add ground-truth inf individual, Second-order subset influence\")\n",
    "for i in range(100):\n",
    "    idx = random.sample(range(1, len(X_train)), 6032)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict_proba([X_test[ix]])[0]\n",
    "#     loss_ix = - y_test[ix] * math.log(y_pred[1]) - (1 - y_test[ix]) * math.log(y_pred[0])\n",
    "\n",
    "    X = np.delete(X_train, idx, 0)\n",
    "    y = y_train.drop(index=idx, inplace=False)\n",
    "    clf.fit(X, y)\n",
    "    y_pred_test = clf.predict_proba(X_test)\n",
    "    # print(\"Ground truth influence of subset: \", computeFairness(y_pred_test, X_test_orig)-spd_0)\n",
    "    inf_gt = computeFairness(y_pred_test, X_test_orig)-spd_0\n",
    "#     y_pred = clf.predict_proba([X_test[ix]])[0]\n",
    "#     inf_gt = - y_test[ix] * math.log(y_pred[1]) - (1 - y_test[ix]) * math.log(y_pred[0]) - loss_ix\n",
    "    \n",
    "    del_f_gt = 0\n",
    "    del_f_1 = 0\n",
    "    diff = []\n",
    "    for i in range(len(idx)):\n",
    "        del_f_1 += infs_1[idx[i]]\n",
    "#         del_f_gt += spdgt[idx[i]] #delta_loss_gt[idx[i]]\n",
    "    \n",
    "    size_hvp = 1\n",
    "    params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "    del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "    \n",
    "    print(inf_gt, del_f_1, del_f_gt, del_f_2, sep=\", \")\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.intercept_ += del_f_2[0]\n",
    "# for i in range(1, num_params):\n",
    "#     clf.coef_.transpose()[i-1][0] += del_f_2[i]\n",
    "# # Mostly highly correlated, a few closer to zero have flipped signs for the actual\n",
    "# # and the computed numbers \n",
    "# gt_idx = []\n",
    "# infs_idx = []\n",
    "# tol = 0.00001\n",
    "# for i in range(len(idx)):\n",
    "#     gt_idx.append(spdgt[idx[i]])\n",
    "#     infs_idx.append(infs_1[idx[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Effect of coherent subset removal on fairness metric** \n",
    "\n",
    "(by coherent, we mean group of data points that share some properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-truth subset, Add 1st-order inf individual, Add ground-truth inf individual, Second-order subset influence\n",
      "0.00032155796236454925, 0.00029454082652533996, 0, 0.00030328036997329463, 786\n",
      "0.0011907157004082403, 0.0011543171079813003, 0, 0.001180300970831471, 575\n",
      "2.7218782747251424e-05, 1.2118612942596616e-05, 0, -3.8425141131604313e-07, 791\n",
      "0.0005263499141654471, 0.0005046670273034681, 0, 0.0005148722481624437, 448\n",
      "-0.0008794362696055713, -0.0008384900270714601, 0, -0.0008687648463629457, 808\n",
      "0.0012078108891511452, 0.0011554814772832803, 0, 0.0011815711584213942, 828\n",
      "0.0005324645782582016, 0.0005128287129365049, 0, 0.0005208576370774723, 555\n",
      "0.00024342051822168198, 0.000224524501465064, 0, 0.00023805720152062236, 455\n",
      "-0.00038275424889328, -0.0003772696302561302, 0, -0.00038636324775190507, 851\n",
      "-0.0001571038718774509, -0.00015012975792855852, 0, -0.00016694723767588833, 741\n",
      "8.599345233262223e-05, 7.441341870919887e-05, 0, 8.297223274831478e-05, 813\n",
      "-1.2616298432133188e-05, -1.591757515660045e-05, 0, -1.707605669751515e-05, 824\n",
      "-0.0006265034795917834, -0.0006151169097003863, 0, -0.0006319949692579965, 789\n",
      "0.00046234236732373146, 0.0004383412097503919, 0, 0.0004603031794720993, 836\n",
      "-0.0007597972553145538, -0.0007346542669646196, 0, -0.0007615789130028658, 799\n",
      "-0.0015465853435260857, -0.0014968090315044655, 0, -0.0015495630632829053, 743\n",
      "0.0005896947571366606, 0.0005695171157946065, 0, 0.0005938897198946146, 765\n",
      "0.0004917612016862583, 0.00046657675983447395, 0, 0.0004839552920240453, 394\n",
      "-0.0021425074806072086, -0.0020963695161144956, 0, -0.002138132170522166, 828\n",
      "-0.00021953637352301048, -0.00021796139099687528, 0, -0.0002301776000538358, 332\n",
      "-2.317740572249649e-05, -1.9983169266170817e-05, 0, -2.9089368712692823e-05, 343\n",
      "0.0001774701109040766, 0.0001646000997876807, 0, 0.00017444557443448616, 594\n",
      "-8.012899631282266e-05, -8.599135068836013e-05, 0, -8.615981582030725e-05, 629\n",
      "0.0014382713597071994, 0.0013970745917161432, 0, 0.0014405393277919815, 706\n",
      "-0.00014660269948796323, -0.00014083051059260953, 0, -0.000141650638430337, 674\n",
      "4.40856114009569e-05, 3.993879621529371e-05, 0, 4.315137453986485e-05, 523\n",
      "-0.00033545045939159523, -0.0003415772807185304, 0, -0.0003455037165637193, 621\n",
      "-0.00015141840132365525, -0.00013970512578298776, 0, -0.0001522280668768918, 752\n",
      "0.0003181590992049621, 0.00031687599176761713, 0, 0.00031201075935539664, 337\n",
      "0.0015973207332586337, 0.0015554609542273396, 0, 0.0016004777041835336, 704\n",
      "0.0006081288314586375, 0.0005886232134927224, 0, 0.0006103481182529139, 769\n",
      "-0.0012964234035247046, -0.0012772144082867757, 0, -0.00130842936843885, 774\n",
      "0.0011720522135507117, 0.0011365409849692712, 0, 0.0011677084671562836, 683\n",
      "0.0018944595741498538, 0.0017216950233988708, 0, 0.0017608978833720877, 711\n",
      "0.0010858882257603897, 0.0010456919366174525, 0, 0.0010681897571655466, 852\n",
      "4.7058113933851065e-05, 4.163960919210817e-05, 0, 4.282968577097949e-05, 15\n",
      "-0.0006861288814067057, -0.0006650220951516758, 0, -0.0006947300800384205, 789\n",
      "7.480808504434222e-05, 7.175861905047699e-05, 0, 7.116304425357949e-05, 447\n",
      "-0.0009742231239623089, -0.0009451907268020769, 0, -0.0009626049380202173, 837\n",
      "-7.6957685814949e-05, -8.516420789837932e-05, 0, -8.468575220138666e-05, 29\n",
      "0.0006967378510597255, 0.0006867947604158622, 0, 0.0006943113012041833, 386\n",
      "0.00014519101186666328, 0.0001413302673710197, 0, 0.0001405534475269104, 259\n",
      "-0.0002531496972831837, -0.00025499083325459637, 0, -0.0002594852933192598, 64\n",
      "-0.00023673748305869102, -0.00023720404371523594, 0, -0.0002361376630203082, 173\n",
      "1.9419660994557386e-05, 1.2071889797536357e-05, 0, 1.438579174240417e-05, 54\n",
      "-0.000548201864029374, -0.0005559921595551079, 0, -0.000547665778309979, 110\n",
      "0.0011922138443911379, 0.0011560368146824318, 0, 0.0011869946956356518, 571\n",
      "-0.0007112810414471871, -0.0007024889252264669, 0, -0.0007160369643988918, 344\n",
      "-0.0002700857861620276, -0.00025263984073174933, 0, -0.0002746167426322264, 745\n",
      "8.766926378744877e-07, -6.674661923133518e-07, 0, -9.57918664509615e-07, 328\n",
      "-0.0004030440116353229, -0.000397720553467731, 0, -0.000412023217297811, 276\n",
      "-0.00036355756162814723, -0.00036157362676391574, 0, -0.0003683423534513741, 35\n",
      "-0.0001395588946399018, -0.0001447444269880168, 0, -0.00014260331622470147, 34\n",
      "7.559297527914843e-05, 7.937120166854719e-05, 0, 7.070790996485981e-05, 136\n",
      "-9.710684742342268e-05, -0.00010219790709985764, 0, -9.665216779358282e-05, 20\n",
      "-0.00030539383010913523, -0.00030768943300380625, 0, -0.0003178107756066167, 213\n",
      "-0.00040763527388218423, -0.0004091393664442601, 0, -0.00041157103152627715, 186\n",
      "-0.00048769773708209896, -0.0004859525610761041, 0, -0.0004828388965200132, 111\n",
      "-0.0001537585762270144, -0.00015700228192283675, 0, -0.00015506590964219186, 38\n",
      "-0.00024834669802278886, -0.00025245181052975437, 0, -0.00025360934027386764, 40\n",
      "-0.00017483215903330018, -0.0001740550308040164, 0, -0.00018274294999409677, 80\n",
      "-0.00021376145799739676, -0.00021466750015697748, 0, -0.00021553247674697, 90\n",
      "-0.00015224771575078622, -0.00015351469163729718, 0, -0.0001524580489534062, 49\n",
      "6.547556466435234e-05, 6.058438573660387e-05, 0, 6.111122289457035e-05, 13\n",
      "2.8785131359509997e-05, 2.421166200443489e-05, 0, 2.4075602544748185e-05, 14\n",
      "-7.267367471280872e-05, -7.660122675451024e-05, 0, -7.562865625605524e-05, 3\n",
      "-9.089182909663429e-05, -9.440238623210916e-05, 0, -9.426458145462839e-05, 16\n",
      "-1.1611799495603314e-05, -1.4129975152910941e-05, 0, -1.454595954573108e-05, 8\n",
      "-2.070648807916964e-06, -2.4880313685253235e-06, 0, -4.274451738255856e-06, 5\n",
      "-3.280705808855755e-05, -3.467980552678684e-05, 0, -3.35258091265322e-05, 3\n",
      "-3.8113042764825744e-05, -4.1539658225779434e-05, 0, -4.039797424106678e-05, 7\n",
      "3.1329366654009316e-05, 2.936597152759617e-05, 0, 2.8882827970779387e-05, 1\n",
      "-0.0020538478739760557, -0.0017378041023753726, 0, -0.0020627866171322937, 5044\n",
      "0.002845400670084014, 0.0018766454838229567, 0, 0.0026992009366760845, 9840\n",
      "-0.00047417276146533016, -0.00047045525368188506, 0, -0.0004873259911626047, 1048\n",
      "-0.0013562982206719032, -0.0013575532738796048, 0, -0.001467365886558944, 1627\n",
      "-0.00016907773977045681, -0.00017788901076406434, 0, -0.00017401312466765184, 455\n",
      "0.0037041211392657702, 0.003005712653531026, 0, 0.0036910899864853242, 6678\n",
      "-0.0009547485681621892, -0.0009296190140186864, 0, -0.0009340091138769158, 1008\n",
      "0.00039852318559280286, 0.0003796054716978147, 0, 0.00039397653026614575, 557\n",
      "-0.001521953855123992, -0.0014774144270182823, 0, -0.0015145491823177074, 375\n",
      "-0.0010044519439844357, -0.0009467545515410577, 0, -0.0009957978618249015, 1307\n",
      "0.0009948118512034254, 0.0007728930936374381, 0, 0.0009591311494910542, 542\n",
      "7.079176090052375e-06, -3.4463507590479225e-07, 0, 3.604118810332576e-06, 288\n",
      "0.000742688273737524, 0.0007118978806397987, 0, 0.0007307666795749135, 820\n",
      "-1.0162208394115524e-05, -1.420992709100299e-05, 0, -1.4451729557953283e-05, 45\n",
      "0.0002057855240914508, 0.0002028305899119667, 0, 0.0002003078429584108, 377\n",
      "0.00017364988097190448, 0.00016752204365825927, 0, 0.0001662888032767722, 151\n",
      "0.01065409325610206, 0.005297491198054383, 0, 0.010214954667668476, 14251\n",
      "-6.78177705797367e-05, -6.75653887507256e-05, 0, -6.794762399732062e-05, 18\n",
      "-0.0008817891196927219, -0.0008869490481554328, 0, -0.0008887415437779618, 180\n",
      "0.002241591874339477, 0.002023803197822623, 0, 0.002236857063529733, 1753\n",
      "0.0017704542948810409, 0.0011434500266886006, 0, 0.0016742577009078489, 2718\n",
      "-0.0001961207953383448, -0.00018783475501532742, 0, -0.00020496003206717467, 120\n",
      "-0.0017132896503604944, -0.0016726994049941696, 0, -0.0017058210336561535, 989\n",
      "-0.0008004428646438133, -0.0007553948290822878, 0, -0.0008046107877040569, 1184\n",
      "-0.0018770120313908178, -0.0016119260950003995, 0, -0.0017907681466957774, 1405\n",
      "-0.0020163326112761504, -0.001971666814898436, 0, -0.0020228507806097887, 1054\n",
      "0.00011462499527825631, 0.00010921637054904566, 0, 0.00010734445431704914, 135\n",
      "0.0007363824879399739, 0.0007297166703033698, 0, 0.0007321343031305255, 209\n",
      "-6.432043574366997e-05, -7.935510521870361e-05, 0, -7.705922628828578e-05, 346\n",
      "-7.747725802115246e-05, -0.00010161664129687344, 0, -8.370453670873581e-05, 574\n",
      "0.00031928089660196446, 0.0003108825128747869, 0, 0.0003098882818841855, 146\n",
      "0.000553905937876964, 0.0005428773158334234, 0, 0.0005583626588832011, 456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00020518398030416884, 0.0001806813285455724, 0, 0.0002022562587816609, 672\n",
      "0.0005404065906345901, 0.0005208677799630199, 0, 0.000537101361893004, 494\n",
      "6.3630520586333e-05, 5.801025060591234e-05, 0, 5.724292962436897e-05, 27\n",
      "0.0001080208432066021, 0.00010304875849298792, 0, 0.00010336583355210776, 238\n",
      "-0.0007161247343761001, -0.0006588411761220491, 0, -0.0007146510528770105, 276\n",
      "-1.8217209975857074e-05, -2.2631751387643898e-05, 0, -2.2673887443399418e-05, 39\n",
      "0.00012402612676287728, 0.00011659293601365963, 0, 0.00012048327562802879, 90\n",
      "-0.00017297699364049457, -0.00017995746567534715, 0, -0.00017935087489683404, 34\n",
      "-0.0001794915076853143, -0.00018073967282725714, 0, -0.00018065231137772054, 72\n",
      "0.00015008072553598928, 0.00014764916561869242, 0, 0.00014634816451191012, 202\n",
      "-0.0005155451534887001, -0.0005198843372466437, 0, -0.0005158652373725129, 217\n",
      "-3.6919400352664056e-05, -4.22611711721743e-05, 0, -4.149630324203788e-05, 80\n",
      "1.5124748653089215e-05, 9.827344460343383e-06, 0, 1.1228763763021637e-05, 15\n",
      "0.0004392275610160534, 0.00043689420123216744, 0, 0.00042733134423942985, 213\n",
      "-0.00022235963590910846, -0.00022534512004215683, 0, -0.0002250885982129835, 137\n",
      "-0.000457593642616938, -0.0004601793441765081, 0, -0.00046059777114319285, 241\n",
      "1.5016064755679626e-05, 8.792971121546881e-06, 0, 8.792891671462337e-06, 7\n",
      "-6.784276694665747e-05, -6.795044975147766e-05, 0, -6.702345922844733e-05, 27\n",
      "9.365841637573591e-05, 9.599461307214287e-05, 0, 9.223126436361643e-05, 63\n",
      "-0.0001082645859229181, -0.0001051258269480244, 0, -0.00010733712740548313, 11\n",
      "-5.1515176331035084e-05, -5.108964510776078e-05, 0, -5.201637567923549e-05, 37\n",
      "-5.9618383493220684e-05, -5.9990856892345905e-05, 0, -6.194978120778604e-05, 38\n",
      "-0.0003308144746269892, -0.00032948517205502547, 0, -0.0003290156461097139, 222\n",
      "-2.1069760036684926e-05, -3.9195752973116556e-05, 0, -3.903831308315409e-05, 40\n",
      "0.00016701924592923967, 0.0001612753988712978, 0, 0.00016259995216692265, 14\n",
      "2.6976410470919143e-05, 2.5631142085755527e-05, 0, 2.5531365378952673e-05, 14\n",
      "0.00017884505547435814, 0.00017306270637091082, 0, 0.0001721046789429751, 63\n",
      "-0.0003264598355792736, -0.00032690610010775956, 0, -0.00032818054839647793, 62\n",
      "7.129497166796783e-05, 6.428036925450553e-05, 0, 6.429695521432314e-05, 102\n",
      "5.115362340990526e-06, 7.522610657660884e-06, 0, 6.696789660124142e-06, 17\n",
      "8.988718082081348e-07, -2.2303007377355813e-06, 0, -4.271605674855583e-06, 49\n",
      "-0.00010927948906319651, -0.00011301942776540813, 0, -0.00010897878881464172, 145\n",
      "-0.0001268996870708028, -0.00013634573149325522, 0, -0.0001357159382519729, 23\n",
      "7.466819008933312e-05, 6.870394970426168e-05, 0, 6.894631459107909e-05, 28\n",
      "-5.713396346168631e-06, -1.4032385747574439e-05, 0, -1.3609495416089184e-05, 29\n",
      "0.00013060228223524262, 0.00012716914614244854, 0, 0.00012625758090594098, 38\n",
      "-8.544112596836628e-05, -8.937848979281711e-05, 0, -8.897841983335326e-05, 19\n",
      "-0.0004145215652223144, -0.0003931095829046222, 0, -0.00042422118531730145, 78\n",
      "6.48691098639187e-05, 4.687345398961722e-05, 0, 4.621216529027584e-05, 22\n",
      "-5.026646703637505e-05, -5.350905284351322e-05, 0, -5.190538069259202e-05, 37\n",
      "-0.00011958329888259178, -0.00012658932326280287, 0, -0.00012321092957380262, 18\n",
      "7.37911034298655e-06, 6.088822704863544e-06, 0, 6.1176355237890365e-06, 4\n",
      "-1.9396616080052942e-05, -2.272024381969304e-05, 0, -2.1818368076768805e-05, 17\n",
      "-1.3197681405785477e-05, -1.5708039336534358e-05, 0, -1.5028732159146135e-05, 8\n",
      "-9.843330217831903e-05, -9.416860626819091e-05, 0, -9.606427829788496e-05, 28\n",
      "-0.0002384190787657614, -0.00024090646034658498, 0, -0.000241772691743667, 16\n",
      "-3.2620332697075316e-05, -3.273947435715937e-05, 0, -3.2817294920473826e-05, 9\n",
      "5.7750911668930094e-05, 5.4511322403116574e-05, 0, 5.133689820062005e-05, 26\n",
      "-3.1448969785019543e-06, -5.91240858270914e-06, 0, -8.044947965588524e-06, 40\n",
      "-0.00011195892787146944, -0.00011890360338242194, 0, -0.00011837865764001982, 27\n",
      "-5.372202966999473e-05, -5.969100791235477e-05, 0, -6.005571110633479e-05, 12\n",
      "-0.00010922537177202041, -0.00010927218680562368, 0, -0.00010559991785509693, 24\n",
      "3.556883132482813e-05, 3.3160588506216484e-05, 0, 3.305203240128945e-05, 28\n",
      "1.9594866968619673e-05, 1.672188920064893e-05, 0, 1.7169079398801808e-05, 13\n",
      "1.2448436613965441e-05, 8.796962623092432e-06, 0, 8.921544804743833e-06, 5\n",
      "-6.689833153514235e-05, -7.065559524439783e-05, 0, -6.660135931992472e-05, 13\n",
      "-0.00012262275764834962, -0.00012323323151440917, 0, -0.00012413434926723119, 6\n",
      "3.056234794737711e-05, 2.7178793071057556e-05, 0, 2.6474505661718526e-05, 9\n",
      "3.9525228964193415e-05, 3.406047599412978e-05, 0, 3.435749384980823e-05, 20\n",
      "-0.00012774050091740596, -0.0001324934875985956, 0, -0.00013098710440230313, 27\n",
      "-3.5628098088436033e-06, -7.442337120995933e-06, 0, -7.581263700123328e-06, 1\n",
      "-4.8148300093531216e-05, -5.123865792523426e-05, 0, -5.0850345387146634e-05, 2\n",
      "6.170667816926545e-05, 5.842170215716131e-05, 0, 5.751824017671636e-05, 2\n",
      "-2.1080873788825727e-05, -2.5103791135935805e-05, 0, -2.446963202881064e-05, 2\n",
      "-1.0285089081463195e-05, -1.3457380965655098e-05, 0, -1.3776243892660587e-05, 2\n",
      "-1.4710571312470666e-05, -1.4640473168830473e-05, 0, -1.434235125894472e-05, 1\n",
      "2.1805248865802707e-05, 1.8537994533815147e-05, 0, 1.8201779113755764e-05, 5\n",
      "-8.380717581457642e-05, -8.652040368561996e-05, 0, -8.588202790181275e-05, 5\n",
      "0.00014163773650727185, 0.0001365492419615199, 0, 0.00013502103947238067, 4\n",
      "9.047231390946209e-06, 8.61510472970922e-06, 0, 8.562370128331444e-06, 1\n",
      "-1.9145948269572166e-05, -2.221980571319764e-05, 0, -2.2245094561430217e-05, 2\n",
      "-8.373598579461405e-05, -8.650348076931236e-05, 0, -8.650626304706609e-05, 3\n",
      "-3.763453903993241e-05, -4.001466451372344e-05, 0, -4.122822014267247e-05, 3\n",
      "1.989474209876385e-05, 1.785349007800166e-05, 0, 1.9360287894092007e-05, 3\n",
      "-2.8135578749588497e-05, -3.386158408266271e-05, 0, -3.3117296683072276e-05, 1\n",
      "3.871318058826234e-05, 3.2392774102095634e-05, 0, 3.186217597284489e-05, 2\n",
      "-1.874990384315156e-05, -2.4430382731071843e-05, 0, -2.4079132223191364e-05, 1\n",
      "8.110701064878434e-06, 6.501967485511333e-06, 0, 6.5728728948501605e-06, 2\n",
      "0.0843803045900777, -0.00039916929512216976, 0, -0.0006549249162457642, 9782\n",
      "0.0843803045900777, -0.00039916929512216976, 0, -0.0006549249162457642, 9782\n",
      "0.08002045761292803, 0.00040423231657556706, 0, 0.0005016156249863892, 20380\n",
      "0.08002045761292803, 0.00040423231657556706, 0, 0.0005016156249863892, 20380\n",
      "-0.0005114668816826873, -0.0003430795279765841, 0, -0.00034035901048368915, 286\n",
      "-0.0005114668816826873, -0.0003430795279765841, 0, -0.00034035901048368915, 286\n",
      "-0.001052484592632713, -0.0006503307842282993, 0, -0.0006593236944492516, 895\n",
      "-0.001052484592632713, -0.0006503307842282993, 0, -0.0006593236944492516, 895\n",
      "0.001994781236516918, 0.0018802808869682359, 0, 0.0020160943094911896, 2817\n",
      "0.001994781236516918, 0.0018802808869682359, 0, 0.0020160943094911896, 2817\n",
      "-0.0011475500891248158, -0.00022649545083275568, 0, -0.00023116284922238705, 231\n",
      "-0.0011475500891248158, -0.00022649545083275568, 0, -0.00023116284922238705, 231\n",
      "0.005329785983341245, -0.0006553121024771924, 0, -0.004776205358891391, 25933\n",
      "0.005329785983341245, -0.0006553121024771924, 0, -0.004776205358891391, 25933\n",
      "0.011514899383314126, 0.0015972973206123762, 0, 0.002437647295191667, 4214\n",
      "0.011514899383314126, 0.0015972973206123762, 0, 0.002437647295191667, 4214\n",
      "-0.00011379073603426382, -0.00014314821494868554, 0, -0.00014369485369681174, 21\n",
      "-0.00011379073603426382, -0.00014314821494868554, 0, -0.00014369485369681174, 21\n",
      "0.17231515705421363, -0.0005217175937695872, 0, 0.0009893210727226164, 14065\n",
      "0.17231515705421363, -0.0005217175937695872, 0, 0.0009893210727226164, 14065\n",
      "-6.562463764650528e-05, -0.0001367226303094415, 0, -0.00014035575019295034, 370\n",
      "-6.562463764650528e-05, -0.0001367226303094415, 0, -0.00014035575019295034, 370\n",
      "-0.007759890940738784, -0.003432154395958667, 0, -0.006162757549937273, 9726\n",
      "-0.007759890940738784, -0.003432154395958667, 0, -0.006162757549937273, 9726\n",
      "0.0018376263209688437, 0.0009595191492121792, 0, 0.0010349909247034613, 939\n",
      "0.0018376263209688437, 0.0009595191492121792, 0, 0.0010349909247034613, 939\n",
      "0.0032918920780070915, 0.0016819893866151978, 0, 0.0018052754652791083, 827\n",
      "0.0032918920780070915, 0.0016819893866151978, 0, 0.0018052754652791083, 827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0024088247367832216, 0.0006643311202665592, 0, 0.0007277911134415611, 943\n",
      "0.0024088247367832216, 0.0006643311202665592, 0, 0.0007277911134415611, 943\n",
      "0.0013416282098084875, 0.0001408496774750524, 0, 0.0009035685539643527, 2067\n",
      "0.0013416282098084875, 0.0001408496774750524, 0, 0.0009035685539643527, 2067\n",
      "0.016080198794457323, 0.002171886303439524, 0, 0.016496224471700162, 22286\n",
      "0.016080198794457323, 0.002171886303439524, 0, 0.016496224471700162, 22286\n",
      "0.002050567117793084, 0.0001404393948815077, 0, 0.00012375246478622148, 1074\n",
      "0.002050567117793084, 0.0001404393948815077, 0, 0.00012375246478622148, 1074\n",
      "-0.007413520413749708, -0.002113515182600834, 0, -0.002228453244490122, 2499\n",
      "-0.007413520413749708, -0.002113515182600834, 0, -0.002228453244490122, 2499\n",
      "-0.0017816691742466029, -0.0009986865136153975, 0, -0.0011110382977382476, 1279\n",
      "-0.0017816691742466029, -0.0009986865136153975, 0, -0.0011110382977382476, 1279\n",
      "-0.0002075749550743411, -2.417783930347705e-07, 0, -4.811153277243314e-07, 14\n",
      "-0.0002075749550743411, -2.417783930347705e-07, 0, -4.811153277243314e-07, 14\n",
      "-0.011446196580878448, 0.00020016262055667553, 0, 0.00039956840956267695, 12463\n",
      "-0.011446196580878448, 0.00020016262055667553, 0, 0.00039956840956267695, 12463\n",
      "-0.006172196252192258, -0.0021402477705084583, 0, -0.004471050518641856, 7726\n",
      "-0.006172196252192258, -0.0021402477705084583, 0, -0.004471050518641856, 7726\n",
      "0.00016214904688605092, -0.0002699674198300297, 0, -0.0002642634884385902, 889\n",
      "0.00016214904688605092, -0.0002699674198300297, 0, -0.0002642634884385902, 889\n",
      "-0.0017590046578270235, -0.0009761525574180404, 0, -0.0010686206317899463, 4466\n",
      "-0.0017590046578270235, -0.0009761525574180404, 0, -0.0010686206317899463, 4466\n",
      "0.002548424148924533, 0.0031632358498914466, 0, 0.003982372528117456, 3212\n",
      "0.002548424148924533, 0.0031632358498914466, 0, 0.003982372528117456, 3212\n",
      "-0.034951093353242146, 2.8032298761815304e-05, 0, 1.692290914048758e-05, 1406\n",
      "-0.034951093353242146, 2.8032298761815304e-05, 0, 1.692290914048758e-05, 1406\n",
      "0.0013694625777754266, 0.0007230312681765405, 0, 0.0007472073998790252, 3721\n",
      "0.0013694625777754266, 0.0007230312681765405, 0, 0.0007472073998790252, 3721\n",
      "-0.0001606351330849798, 1.553872094967001e-06, 0, 1.3689799156691226e-06, 9\n",
      "-0.0001606351330849798, 1.553872094967001e-06, 0, 1.3689799156691226e-06, 9\n",
      "-0.00036035964267222553, -0.00025156656770553677, 0, -0.0002827576427789984, 4030\n",
      "-0.00036035964267222553, -0.00025156656770553677, 0, -0.0002827576427789984, 4030\n",
      "0.011607201733277311, -0.0006866949422149813, 0, -0.0011440647842274818, 3992\n",
      "0.011607201733277311, -0.0006866949422149813, 0, -0.0011440647842274818, 3992\n",
      "-0.005765441508572833, 7.303649210554202e-05, 0, 7.265947031271336e-05, 989\n",
      "-0.005765441508572833, 7.303649210554202e-05, 0, 7.265947031271336e-05, 989\n",
      "-0.0030429891187805924, 6.527606192693635e-05, 0, 5.2077657988812914e-05, 1350\n",
      "-0.0030429891187805924, 6.527606192693635e-05, 0, 5.2077657988812914e-05, 1350\n",
      "-0.0009462273701826285, 0.001118537894665689, 0, 0.0011608922250651336, 1966\n",
      "-0.0009462273701826285, 0.001118537894665689, 0, 0.0011608922250651336, 1966\n",
      "-0.0006243881911789928, -0.0009459811654522248, 0, -0.0010158337249513703, 3212\n",
      "-0.0006243881911789928, -0.0009459811654522248, 0, -0.0010158337249513703, 3212\n",
      "0.0035155109378589566, -5.032232798143915e-05, 0, -4.900863018172106e-05, 143\n",
      "0.0035155109378589566, -5.032232798143915e-05, 0, -4.900863018172106e-05, 143\n",
      "0.0015910986911271685, -0.002391180242057327, 0, -0.0026734073446202447, 4038\n",
      "0.0015910986911271685, -0.002391180242057327, 0, -0.0026734073446202447, 4038\n",
      "0.0016340498266165793, -0.0004810624208413037, 0, -0.0004777297277467085, 644\n",
      "0.0016340498266165793, -0.0004810624208413037, 0, -0.0004777297277467085, 644\n",
      "0.006589790823944591, 0.00257756028540891, 0, 0.0029769996690220462, 3584\n",
      "0.006589790823944591, 0.00257756028540891, 0, 0.0029769996690220462, 3584\n",
      "0.0014802543322554873, 0.0004191250478815906, 0, 0.00043821112399851294, 912\n",
      "0.0014802543322554873, 0.0004191250478815906, 0, 0.00043821112399851294, 912\n",
      "-0.0015573142141178664, -0.00016625023455393812, 0, -0.0001668240154025134, 1572\n",
      "-0.0015573142141178664, -0.00016625023455393812, 0, -0.0001668240154025134, 1572\n"
     ]
    }
   ],
   "source": [
    "print(\"Ground-truth subset, Add 1st-order inf individual, Add ground-truth inf individual, Second-order subset influence\")\n",
    "clf.fit(X_train, y_train)\n",
    "continuous_cols = ['age', 'education.num', 'hours',]\n",
    "# y_pred = clf.predict_proba([X_test[ix]])[0]\n",
    "# loss_ix = - y_test[ix] * math.log(y_pred[1]) - (1 - y_test[ix]) * math.log(y_pred[0])\n",
    "for col in X_train_orig.columns:\n",
    "    vals = X_train_orig[col].unique()\n",
    "    for val in vals:\n",
    "#         print(col, val, sep=\": \")\n",
    "        if col in continuous_cols:\n",
    "            idx = X_train_orig[X_train_orig[col] == val].index \n",
    "        else:\n",
    "            idx = X_train_orig[X_train_orig[col] == 1].index \n",
    "        X = np.delete(X_train, idx, 0)\n",
    "        y = y_train.drop(index=idx, inplace=False)\n",
    "        if len(y.unique())>1:\n",
    "            clf.fit(X, y)\n",
    "            y_pred = clf.predict_proba(X_test)\n",
    "#             y_pred = clf.predict_proba([X_test[ix]])[0]\n",
    "#             inf_gt = - y_test[ix] * math.log(y_pred[1]) - (1 - y_test[ix]) * math.log(y_pred[0]) - loss_ix\n",
    "            inf_gt = computeFairness(y_pred, X_test_orig)-spd_0\n",
    "            del_f_gt = 0\n",
    "            del_f_1 = 0\n",
    "            diff = []\n",
    "            for i in range(len(idx)):\n",
    "                del_f_1 += infs_1[idx[i]]\n",
    "#                 del_f_gt += delta_loss_gt[idx[i]] #spdgt[idx[i]]\n",
    "\n",
    "            size_hvp = 1\n",
    "            params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "            del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "\n",
    "            print(inf_gt, del_f_1, del_f_gt, del_f_2, len(idx), sep=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
