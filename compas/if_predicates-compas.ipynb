{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('compas-scores-two-years.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description of columns\n",
    "# sex\tSex of inmate (male/female)\n",
    "# age\tAge of inmate\n",
    "# age_cat\tAge of inmate as a categorical variable(Less than 25/25-45/Greater than 45)\n",
    "# race\tRace of inmate (Caucasian/African American/Asian/Hispanic/Native American/Other)\n",
    "# priors_count\tThe count of prior adult convictions\n",
    "# two_year_recid\tDoes the inmate recidivicate with 2 years after out of jail? (1/0)\n",
    "# r_charge_desc\tDescription of the last crime of the inmate\n",
    "# c_charge_desc\tType of the crime\n",
    "# c_charge_degree\tCharge of the crime: Felony (F)/ Misdemeanor(M)\n",
    "# r_charge_degree\tDegree of the charge of the crime: (F1/F2/F3/F5/F6/F7/M1/M2/MO3/CO3)\n",
    "# juv_other_count\tTimes of crimes commited as juvenile\n",
    "# length_of_stay\tDays stayed in jail\n",
    "# Recidivicated\tWhether the inmate recidivicated or not (Recidivate or notRecidivate)\n",
    "# is_violent_recid\tWhether the recidivicated crimes violent or not (1 or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[['sex', 'juv_fel_count', 'priors_count', 'race', 'age_cat', 'juv_misd_count', 'c_charge_degree', 'juv_other_count', 'is_recid']]\n",
    "# these attributes were used in \"Exacerbating Algorithmic Bias through Fairness Attacks\" Mehrabi et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['event', 'is_violent_recid',\n",
    "       'is_recid', 'priors_count', 'juv_other_count',\n",
    "       'juv_misd_count', 'juv_fel_count', 'race', 'age_cat', 'sex','score_text']]\n",
    "# attributes used in Lewis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing** (categorical to numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " def preprocess(df):\n",
    "    df['age_cat'] = df['age_cat'].map({'Less than 25': 0, '25 - 45': 1, 'Greater than 45': 2}).astype(int)    \n",
    "    df['score_text'] = df['score_text'].map({'Low': 0, 'Medium': 1, 'High': 2}).astype(int)    \n",
    "    df['race'] = df['race'].map({'Other': 0, 'African-American': 0, 'Hispanic': 0, 'Native American': 0, 'Asian': 0, 'Caucasian': 1}).astype(int)    \n",
    "    df['sex'] = df['sex'].map({'Male': 1, 'Female': 0}).astype(int)    \n",
    "    \n",
    "    df.loc[(df['priors_count'] <= 5), 'priors_count'] = 0\n",
    "    df.loc[(df['priors_count'] > 5) & (df['priors_count'] <= 15), 'priors_count'] = 1\n",
    "    df.loc[(df['priors_count'] > 15), 'priors_count'] = 2\n",
    "    \n",
    "    df.loc[(df['juv_fel_count'] == 0), 'juv_fel_count'] = 0\n",
    "    df.loc[(df['juv_fel_count'] == 1), 'juv_fel_count'] = 1\n",
    "    df.loc[(df['juv_fel_count'] > 1), 'juv_fel_count'] = 2\n",
    "    \n",
    "    df.loc[(df['juv_misd_count'] == 0), 'juv_misd_count'] = 0\n",
    "    df.loc[(df['juv_misd_count'] == 1), 'juv_misd_count'] = 1\n",
    "    df.loc[(df['juv_misd_count'] > 1), 'juv_misd_count'] = 2\n",
    "    \n",
    "    df.loc[(df['juv_other_count'] == 0), 'juv_other_count'] = 0\n",
    "    df.loc[(df['juv_other_count'] == 1), 'juv_other_count'] = 1\n",
    "    df.loc[(df['juv_other_count'] > 1), 'juv_other_count'] = 2\n",
    "    return df\n",
    "\n",
    "df = preprocess(df)\n",
    "\n",
    "y = df['is_recid']\n",
    "# y = df['is_violent_recid']\n",
    "df = df.drop(columns=['is_recid', 'is_violent_recid'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=1)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Protected, privileged**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protected: 'gender'=0\n",
    "# privileged: 'gender'=1\n",
    "\n",
    "# protected: 'race'=0\n",
    "# privileged: 'race'=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size=500\n",
    "# X_train = X_train[0:size]\n",
    "# y_train = y_train[0:size]\n",
    "\n",
    "X_train_orig = copy.deepcopy(X_train)\n",
    "X_test_orig = copy.deepcopy(X_test)\n",
    "\n",
    "# Scale data: regularization penalty default: ‘l2’, ‘lbfgs’ solvers support only l2 penalties. \n",
    "# Regularization makes the predictor dependent on the scale of the features.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute fairness metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFairness(y_pred, X_test, y_test, metric): \n",
    "    fairnessMetric = 0\n",
    "    protected_idx = X_test[X_test['race']==0].index\n",
    "    numProtected = len(protected_idx)\n",
    "    privileged_idx = X_test[X_test['race']==1].index\n",
    "    numPrivileged = len(privileged_idx)\n",
    "        \n",
    "    p_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        p_protected += y_pred[protected_idx[i]][1]\n",
    "    p_protected /= len(protected_idx)\n",
    "    \n",
    "    p_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        p_privileged += y_pred[privileged_idx[i]][1]\n",
    "    p_privileged /= len(privileged_idx)\n",
    "    \n",
    "    # statistical parity difference\n",
    "    statistical_parity = p_protected - p_privileged\n",
    "    \n",
    "    # equality of opportunity, or \n",
    "    # true positive rate parity\n",
    "    # P(outcome=1 | Y=1, G=0)- P(outcome=1 | Y=1, G=1)\n",
    "    true_positive_protected = 0\n",
    "    actual_positive_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            actual_positive_protected += 1\n",
    "#             if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "            true_positive_protected += y_pred[protected_idx[i]][1]\n",
    "    tpr_protected = true_positive_protected/actual_positive_protected\n",
    "    \n",
    "    true_positive_privileged = 0\n",
    "    actual_positive_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            actual_positive_privileged += 1\n",
    "#             if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "            true_positive_privileged += y_pred[privileged_idx[i]][1]\n",
    "    tpr_privileged = true_positive_privileged/actual_positive_privileged\n",
    "    \n",
    "    tpr_parity = tpr_protected - tpr_privileged\n",
    "    \n",
    "    # equalized odds or TPR parity + FPR parity\n",
    "    # false positive rate parity\n",
    "    \n",
    "    # predictive parity\n",
    "    p_o1_y1_s1 = 0\n",
    "    p_o1_s1 = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "#         if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "        p_o1_s1 += y_pred[protected_idx[i]][1]\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            p_o1_y1_s1 += y_pred[protected_idx[i]][1]\n",
    "    ppv_protected = p_o1_y1_s1/p_o1_s1\n",
    "    \n",
    "    p_o1_y1_s0 = 0\n",
    "    p_o1_s0 = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "#         if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "        p_o1_s0 += y_pred[privileged_idx[i]][1]\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            p_o1_y1_s0 += y_pred[privileged_idx[i]][1]\n",
    "    ppv_privileged = p_o1_y1_s0/p_o1_s0\n",
    "    \n",
    "    predictive_parity = ppv_protected - ppv_privileged\n",
    "    \n",
    "    if (metric == 0):\n",
    "        fairnessMetric = statistical_parity\n",
    "    elif (metric == 1):\n",
    "        fairnessMetric = tpr_parity\n",
    "    elif (metric == 2):\n",
    "        fairnessMetric = predictive_parity\n",
    "        \n",
    "    return fairnessMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Influence of points computed using ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_influence(X_train, y_train, X_test, X_test_orig, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    spd_0 = computeFairness(y_pred, X_test_orig, y_test, 0)\n",
    "\n",
    "    delta_spd = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_removed = np.delete(X_train, i, 0)\n",
    "        y_removed = y_train.drop(index=i, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        delta_spd_i = computeFairness(y_pred, X_test_orig, y_test, 0) - spd_0\n",
    "        delta_spd.append(delta_spd_i)\n",
    "    \n",
    "    return delta_spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function** (Log loss for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if (y_pred[i][1] != 0 and y_pred[i][0] != 0):\n",
    "            loss += - y_true[i] * math.log(y_pred[i][1]) - (1 - y_true[i]) * math.log(y_pred[i][0])\n",
    "    loss /= len(y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Accuracy** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def computeAccuracy(y_true, y_pred):\n",
    "    accuracy = 0\n",
    "    for i in range(len(y_true)):\n",
    "        idx = y_true[i]\n",
    "        if (y_pred[i][idx] > y_pred[i][1 - idx]):\n",
    "            accuracy += 1\n",
    "#         accuracy += y_pred[i][idx]\n",
    "    accuracy /= len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of loss function at z with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_L_del_theta_i(num_params, y_true, x, y_pred):\n",
    "#     del_L_del_theta = np.ones((num_params, 1)) * ((1 - y_true) * y_pred[1] - y_true * y_pred[0])\n",
    "    del_L_del_theta = np.ones((num_params, 1)) * (- y_true + y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_L_del_theta[j] *=  x[j-1]\n",
    "    return del_L_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessian: Second-order partial derivative of loss function with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_one_point(num_params, x, y_pred):\n",
    "    H = np.ones((num_params, num_params)) * (y_pred[0] * y_pred[1])\n",
    "    for i in range(1, num_params):\n",
    "        for j in range(i + 1):\n",
    "            if j == 0:\n",
    "                H[i][j] *= x[i-1]\n",
    "            else:\n",
    "                H[i][j] *= x[i-1] * x[j-1] \n",
    "    i_lower = np.tril_indices(num_params, -1)\n",
    "    H.T[i_lower] = H[i_lower]     \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of $P(y \\mid \\textbf{x})$ with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_f_del_theta_i(num_params, x, y_pred):\n",
    "    del_f_del_theta = np.ones((num_params, 1)) * (y_pred[0] * y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_f_del_theta[j] *=  x[j-1]\n",
    "    return del_f_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($Statistical parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(SPD)/del(theta)\n",
    "def del_spd_del_theta(num_params, X_test_orig, X_test, y_pred):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    numPrivileged = X_test_orig['race'].sum()\n",
    "    numProtected = len(X_test_orig) - numPrivileged\n",
    "    for i in range(len(X_test)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[i], y_pred[i])\n",
    "        if X_test_orig.iloc[i]['race'] == 1: #privileged\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "        elif X_test_orig.iloc[i]['race'] == 0:\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_privileged /= numPrivileged\n",
    "    del_f_protected /= numProtected\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($TPR parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(TPR_parity)/del(theta)\n",
    "def del_tpr_parity_del_theta(num_params, X_test_orig, X_test, y_pred, y_test):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    \n",
    "    protected_idx = X_test_orig[X_test_orig['race']==0].index\n",
    "    privileged_idx = X_test_orig[X_test_orig['race']==1].index\n",
    "\n",
    "    actual_positive_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            actual_positive_privileged += 1\n",
    "#             if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "            del_f_i = del_f_del_theta_i(num_params, X_test[privileged_idx[i]], y_pred[privileged_idx[i]])\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "    del_f_privileged /= actual_positive_privileged\n",
    "    \n",
    "    actual_positive_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            actual_positive_protected += 1\n",
    "#             if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "            del_f_i = del_f_del_theta_i(num_params, X_test[protected_idx[i]], y_pred[protected_idx[i]])\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_protected /= actual_positive_protected\n",
    "\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($Predictive parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(Predictive_parity)/del(theta)\n",
    "def del_predictive_parity_del_theta(num_params, X_test_orig, X_test, y_pred, y_test):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    \n",
    "    protected_idx = X_test_orig[X_test_orig['race']==0].index\n",
    "    privileged_idx = X_test_orig[X_test_orig['race']==1].index\n",
    "\n",
    "    u_dash_protected = np.zeros((num_params, 1))\n",
    "    v_protected = 0\n",
    "    v_dash_protected = np.zeros((num_params, 1))\n",
    "    u_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[protected_idx[i]], y_pred[protected_idx[i]])\n",
    "#         if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "        v_protected += y_pred[protected_idx[i]][1]\n",
    "        v_dash_protected = np.add(v_dash_protected, del_f_i)\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            u_dash_protected = np.add(u_dash_protected, del_f_i)\n",
    "            u_protected += y_pred[protected_idx[i]][1]\n",
    "    del_f_protected = (u_dash_protected * v_protected - u_protected * v_dash_protected)/(v_protected * v_protected)\n",
    "    \n",
    "    u_dash_privileged = np.zeros((num_params, 1))\n",
    "    v_privileged = 0\n",
    "    v_dash_privileged = np.zeros((num_params, 1))\n",
    "    u_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[privileged_idx[i]], y_pred[privileged_idx[i]])\n",
    "#         if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "        v_privileged += y_pred[privileged_idx[i]][1]\n",
    "        v_dash_privileged = np.add(v_dash_privileged, del_f_i)\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            u_dash_privileged = np.add(u_dash_privileged, del_f_i)\n",
    "            u_privileged += y_pred[privileged_idx[i]][1]\n",
    "    del_f_privileged = (u_dash_privileged * v_privileged - u_privileged * v_dash_privileged)/(v_privileged * v_privileged)\n",
    "    \n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic estimation of Hessian vector product (involving del fairness): $H_{\\theta}^{-1}v = H_{\\theta}^{-1}\\nabla_{\\theta}f(z, \\theta) = v + [I - \\nabla_{\\theta}^2L(z_{s_j}, \\theta^*)]H_{\\theta}^{-1}v$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformly sample t points from training data \n",
    "def hessian_vector_product(num_params, n, size, v, hessian_all_points):\n",
    "    if (size > n):\n",
    "        size = n\n",
    "    sample = random.sample(range(n), size)\n",
    "    hinv_v = copy.deepcopy(v)\n",
    "    for idx in range(size):\n",
    "        i = sample[idx]\n",
    "        hessian_i = hessian_all_points[i]\n",
    "        hinv_v = np.matmul(np.subtract(np.identity(num_params), hessian_i), hinv_v)\n",
    "        hinv_v = np.add(hinv_v, v)\n",
    "    return hinv_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order_influence(del_L_del_theta, hinv_v, n):\n",
    "    infs = []\n",
    "    for i in range(n):\n",
    "        inf = -np.dot(del_L_del_theta[i].transpose(), hinv_v)\n",
    "        inf *= -1/n\n",
    "        infs.append(inf[0][0].tolist())\n",
    "    return infs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second-order influence computation for a group of points in subset U**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_influence(X_train, U, size, del_L_del_theta, hessian_all_points):\n",
    "    u = len(U)\n",
    "    s = len(X_train)\n",
    "    p = u/s\n",
    "    c1 = (1 - 2*p)/(s * (1-p)**2)\n",
    "    c2 = 1/((s * (1-p))**2)\n",
    "    num_params = len(del_L_del_theta[0])\n",
    "    del_L_del_theta_hinv = np.zeros((num_params, 1))\n",
    "    del_L_del_theta_sum = np.zeros((num_params, 1))\n",
    "    hessian_U = np.zeros((num_params, num_params))\n",
    "    for i in range(u):\n",
    "        idx = U[i]\n",
    "        hessian_U = np.add(hessian_U, s * hessian_all_points[idx])\n",
    "        del_L_del_theta_sum = np.add(del_L_del_theta_sum, del_L_del_theta[idx])\n",
    "    \n",
    "    hinv_del_L_del_theta= np.matmul(hinv_exact, del_L_del_theta_sum)\n",
    "    hinv_hessian_U = np.matmul(hinv_exact, hessian_U)\n",
    "    term1 = c1 * hinv_del_L_del_theta\n",
    "    term2 = c2 * np.matmul(hinv_hessian_U, hinv_del_L_del_theta)\n",
    "    sum_term = np.add(term1, term2)\n",
    "    return sum_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics: Initial state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial statistical parity:  0.1141183433861288\n",
      "Initial TPR parity:  0.027076868274455768\n",
      "Initial predictive parity:  0.026473352225881874\n",
      "Initial loss:  0.22119003355216119\n",
      "Initial accuracy:  0.9085239085239085\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "num_params = len(clf.coef_.transpose()) + 1 #weights and intercept; params: clf.coef_, clf.intercept_\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "y_pred_train = clf.predict_proba(X_train)\n",
    "\n",
    "spd_0 = computeFairness(y_pred_test, X_test_orig, y_test, 0)\n",
    "print(\"Initial statistical parity: \", spd_0)\n",
    "\n",
    "tpr_parity_0 = computeFairness(y_pred_test, X_test_orig, y_test, 1)\n",
    "print(\"Initial TPR parity: \", tpr_parity_0)\n",
    "\n",
    "predictive_parity_0 = computeFairness(y_pred_test, X_test_orig, y_test, 2)\n",
    "print(\"Initial predictive parity: \", predictive_parity_0)\n",
    "\n",
    "loss_0 = logistic_loss(y_test, y_pred_test)\n",
    "print(\"Initial loss: \", loss_0)\n",
    "\n",
    "accuracy_0 = computeAccuracy(y_test, y_pred_test)\n",
    "print(\"Initial accuracy: \", accuracy_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-compute: (1) Hessian (2) del_L_del_theta for each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_L_del_theta = []\n",
    "for i in range(int(len(X_train))):\n",
    "    del_L_del_theta.insert(i, del_L_del_theta_i(num_params, y_train[i], X_train[i], y_pred_train[i]))\n",
    "\n",
    "hessian_all_points = []\n",
    "for i in range(len(X_train)):\n",
    "    hessian_all_points.insert(i, hessian_one_point(num_params, X_train[i], y_pred_train[i])\n",
    "                              /len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Select delta fairness function depending on selected metric*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 2\n",
    "if metric == 0:\n",
    "    v1 = del_spd_del_theta(num_params, X_test_orig, X_test, y_pred_test)\n",
    "elif metric == 1:\n",
    "    v1 = del_tpr_parity_del_theta(num_params, X_test_orig, X_test, y_pred_test, y_test)\n",
    "elif metric == 2:\n",
    "    v1 = del_predictive_parity_del_theta(num_params, X_test_orig, X_test, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*H^{-1} computation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexact = 1\n",
    "if hexact == 1: \n",
    "    H_exact = np.zeros((num_params, num_params))\n",
    "    for i in range(len(X_train)):\n",
    "        H_exact = np.add(H_exact, hessian_all_points[i])\n",
    "    hinv_exact = np.linalg.pinv(H_exact) \n",
    "    hinv_v = np.matmul(hinv_exact, v1)\n",
    "else: #using Hessian vector product\n",
    "    size_hvp = int(len(X_train) * .01)\n",
    "    hinv_v = hessian_vector_product(num_params, len(X_train), size_hvp, v1, hessian_all_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence of each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "infs_1 = first_order_influence(del_L_del_theta, hinv_v, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking ground truth, first-order and second-order influences for a set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical parity \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'age'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'age'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-3dc7cbe3bd79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m        \u001b[0midx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m        \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'age'"
     ]
    }
   ],
   "source": [
    "if metric == 0:\n",
    "    print(\"Statistical parity \")\n",
    "elif metric == 1:\n",
    "    print(\"True positive rate parity \")\n",
    "elif metric == 2:\n",
    "    print(\"Predictive parity\")\n",
    "    \n",
    "active = 1\n",
    "if active:\n",
    "    predicates = [ 'age']\n",
    "    idx=X_train_orig.index \n",
    "    for pred in predicates:\n",
    "       idx0 = X_train_orig[(X_train_orig[pred] == 1)].index \n",
    "       idx=idx.intersection(idx0)\n",
    "      \n",
    "    print(\"#Rows removed: \", len(idx))\n",
    "    print(\"#Rows left: \", len(X_train) - len(idx))\n",
    "    X = np.delete(X_train, idx, 0)\n",
    "    y = y_train.drop(index=idx, inplace=False)\n",
    "    clf.fit(X, y)\n",
    "    y_pred_test = clf.predict_proba(X_test)\n",
    "    print(\"Ground truth influence of subset (on statistical parity): \", computeFairness(y_pred_test, X_test_orig, y_test, 0) - spd_0)\n",
    "    print(\"Ground truth influence of subset (on tpr parity): \", computeFairness(y_pred_test, X_test_orig, y_test, 1) - tpr_parity_0)\n",
    "    print(\"Ground truth influence of subset (on predictive parity): \", computeFairness(y_pred_test, X_test_orig, y_test, 2) - predictive_parity_0)\n",
    "\n",
    "    del_f_1 = 0\n",
    "    for i in range(len(idx)):\n",
    "        del_f_1 += infs_1[idx[i]]\n",
    "    print(\"First-order influence: \", del_f_1)\n",
    "\n",
    "    size_hvp = 1\n",
    "    params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "    del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "    print(\"Second-order influence: \", del_f_2)\n",
    "    \n",
    "    spd_1 = computeFairness(y_pred_test, X_test_orig, y_test, 0)\n",
    "    print(\"Ground truth statistical parity after removing subset: \", spd_1)\n",
    "    \n",
    "    tpr_parity_1 = computeFairness(y_pred_test, X_test_orig, y_test, 1)\n",
    "    print(\"Ground truth tpr parity after removing subset: \", tpr_parity_1)\n",
    "\n",
    "    predictive_parity_1 = computeFairness(y_pred_test, X_test_orig, y_test, 1)\n",
    "    print(\"Ground truth predictive parity after removing subset: \", predictive_parity_1)\n",
    "\n",
    "    loss_1 = logistic_loss(y_test, y_pred_test)\n",
    "    print(\"Loss after removing subset: \", loss_1)\n",
    "\n",
    "    accuracy_1 = computeAccuracy(y_test, y_pred_test)\n",
    "    print(\"Accuracy after removing subset: \", accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.20044233691822827 -0.17393358242162493 0.3597551530603338 0.8310092961487384\n",
      "-0.1225835505238218 -0.01100904749147602 0.3789946670357515 0.8205179282868525\n"
     ]
    }
   ],
   "source": [
    "print(spd_0, tpr_parity_0, preditive_parity_0, loss_0, accuracy_0)\n",
    "print(spd_1, tpr_parity_1, predictive_parity_1, loss_1, accuracy_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fairness: Ground-truth subset influence vs. computed subset influences: Random subset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-truth subset, Add 1st-order inf individual, Second-order subset influence\n"
     ]
    }
   ],
   "source": [
    "print(\"Ground-truth subset, Add 1st-order inf individual, Second-order subset influence\")\n",
    "sampleSize = int(.2 * len(X_train))\n",
    "for i in range(100):\n",
    "    idx = random.sample(range(1, len(X_train)), sampleSize) \n",
    "    \n",
    "    # Ground truth subset influence\n",
    "    X = np.delete(X_train, idx, 0)\n",
    "    y = y_train.drop(index=idx, inplace=False)\n",
    "    clf.fit(X, y)\n",
    "    y_pred_test = clf.predict_proba(X_test)\n",
    "    inf_gt = computeFairness(y_pred_test, X_test_orig, y_test, 0) - spd_0\n",
    "    \n",
    "    # First-order subset influence\n",
    "    del_f_1 = 0\n",
    "    for i in range(len(idx)):\n",
    "        del_f_1 += infs_1[idx[i]]\n",
    "    \n",
    "    # Second-order subset influence\n",
    "    size_hvp = 1\n",
    "    params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "    del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "    \n",
    "#     print(inf_gt, del_f_1, del_f_2, sep=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fairness: Ground-truth subset influence vs. computed subset influences: Coherent subset** \n",
    "\n",
    "(by coherent, we mean group of data points that share some properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['juv_other_count'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute, Value, Ground-truth subset, Add 1st-order inf individual, Second-order subset influence, %rowsRemoved, Accuracy\n",
      "event, 1, 0.07874570756472943, 5.318593107702603e-05, 0.000171996709142683, 0.3800034656038815, 0.5474705474705475\n",
      "priors_count, 1, 0.007570836047698926, 0.006762594089373776, 0.007807401935665577, 0.173626754461965, 0.9099099099099099\n",
      "priors_count, 0, -0.029068590899494984, -0.00562136747911325, -0.038856443362916084, 0.7879050424536476, 0.9064449064449065\n",
      "priors_count, 2, -0.0009690017224928127, -0.000887180522362561, -0.0009419546051544658, 0.038468203084387455, 0.9085239085239085\n",
      "juv_other_count, 2, 0.005206844505824093, 0.003638675794325552, 0.004281349733050933, 0.02235314503552244, 0.9085239085239085\n",
      "juv_other_count, 0, -0.00596499682559426, -0.003494489804704684, -0.0014243651276616663, 0.9272223184889967, 0.8683298683298684\n",
      "juv_other_count, 1, -6.409598136680739e-05, 0.00010986009827703359, -6.575310060001583e-05, 0.050424536475480855, 0.9071379071379071\n",
      "juv_misd_count, 2, 0.0012316901381848044, 0.0010927081279115969, 0.001160656409346618, 0.01715473921330792, 0.9071379071379071\n",
      "juv_misd_count, 0, -0.0028807613880538208, 0.0013673740421463322, -0.02225608632365413, 0.9442037775082308, 0.9029799029799029\n",
      "juv_misd_count, 1, -0.002572139333342882, -0.0022060360821600567, -0.002488789193323692, 0.03864148327846127, 0.9092169092169092\n",
      "juv_fel_count, 0, 0.05681948591048658, 0.0014450309859426005, 0.04580557243433098, 0.9627447582741293, 0.9022869022869023\n",
      "juv_fel_count, 2, -0.0008867536190696468, -0.00043503105476304664, -0.000578714859541766, 0.01247617397331485, 0.9092169092169092\n",
      "juv_fel_count, 1, -0.0006945596814230681, -0.0007559538432816841, -0.0007103318830498706, 0.024779067752555884, 0.9078309078309078\n",
      "race, 0, -0.0007700759856217321, 0.001743077850055596, 0.004593698849987066, 0.6605440998093918, 0.9112959112959113\n",
      "race, 1, -0.0107892441900308, -0.0014890317621577382, -0.0023966278022808417, 0.33945590019060823, 0.9099099099099099\n",
      "age_cat, 1, -0.011887746902776497, -0.004133894022793038, -0.011939492919563463, 0.573037601802114, 0.9112959112959113\n",
      "age_cat, 2, -0.009891301437102151, -0.006741859765031314, -0.009066869314258449, 0.21538728123375497, 0.9078309078309078\n",
      "age_cat, 0, 0.01499570844622855, 0.01112979987572231, 0.014742966709387925, 0.211575116964131, 0.9064449064449065\n",
      "sex, 1, 0.013181374584366412, 0.0040775917177847424, 0.023016006336398317, 0.8092185063247271, 0.9057519057519058\n",
      "sex, 0, -0.0008020130119459346, -0.0038235456298868755, -0.0046284741863925095, 0.19078149367527292, 0.9092169092169092\n",
      "score_text, 2, -0.002041005022291875, -0.0007880654391316786, -0.0013177755234299888, 0.19805926182637323, 0.9112959112959113\n",
      "score_text, 0, 0.012789424270732153, 0.004897988018728769, 0.010187574141659592, 0.5359556402703171, 0.9071379071379071\n",
      "score_text, 1, -0.005558973769286379, -0.0038558764916991235, -0.0055331770466423656, 0.26598509790330965, 0.9078309078309078\n"
     ]
    }
   ],
   "source": [
    "print(\"Attribute, Value, Ground-truth subset, Add 1st-order inf individual, \\\n",
    "Second-order subset influence, %rowsRemoved, Accuracy\")\n",
    "clf.fit(X_train, y_train)\n",
    "for col in X_train_orig.columns:\n",
    "    vals = X_train_orig[col].unique()\n",
    "    for val in vals:\n",
    "#         print(col, val, sep=\": \")\n",
    "        idx = X_train_orig[X_train_orig[col] == val].index \n",
    "    \n",
    "        X = np.delete(X_train, idx, 0)\n",
    "        y = y_train.drop(index=idx, inplace=False)\n",
    "        inf_gt = 0\n",
    "        if len(y.unique()) > 1:\n",
    "            # Ground truth subset influence\n",
    "            clf.fit(X, y)\n",
    "            y_pred = clf.predict_proba(X_test)\n",
    "            if metric == 0:\n",
    "                inf_gt = computeFairness(y_pred, X_test_orig, y_test, 0) - spd_0\n",
    "            elif metric == 1:\n",
    "                inf_gt = computeFairness(y_pred, X_test_orig, y_test, 1) - tpr_parity_0\n",
    "            elif metric == 2:\n",
    "                inf_gt = computeFairness(y_pred, X_test_orig, y_test, 2) - predictive_parity_0\n",
    "            accuracy = computeAccuracy(y_test, y_pred)\n",
    "\n",
    "            # First-order subset influence\n",
    "            del_f_1 = 0            \n",
    "            for i in range(len(idx)):\n",
    "                del_f_1 += infs_1[idx[i]]\n",
    "\n",
    "            # Second-order subset influence\n",
    "            size_hvp = 1\n",
    "            params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "            del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "\n",
    "            print(col, val, inf_gt, del_f_1, del_f_2, len(idx)/len(X_train), accuracy, sep=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
