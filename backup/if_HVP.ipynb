{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital', 'occupation', 'relationship', 'race', 'gender', 'capgain', 'caploss', 'hours', 'country', 'income']\n",
    "df_train = pd.read_csv('adult.data', names=cols, sep=\",\")\n",
    "df_test = pd.read_csv('adult.test', names=cols, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def one_hot_encode(df):\n",
    "    df.isin(['?']).sum(axis=0)\n",
    "\n",
    "    # replace missing values (?) to nan and then drop the columns\n",
    "    df['country'] = df['country'].replace('?',np.nan)\n",
    "    df['workclass'] = df['workclass'].replace('?',np.nan)\n",
    "    df['occupation'] = df['occupation'].replace('?',np.nan)\n",
    "\n",
    "    # dropping the NaN rows now\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    df['income'] = df['income'].map({'<=50K': 0, '>50K': 1}).astype(int)\n",
    "    df = pd.concat([df, pd.get_dummies(df['gender'], prefix='gender')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['race'], prefix='race')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['marital'], prefix='marital')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['workclass'], prefix='workclass')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['relationship'], prefix='relationship')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['occupation'], prefix='occupation')],axis=1)\n",
    "\n",
    "    df = df.drop(columns=['workclass', 'gender', 'fnlwgt', 'education', 'occupation', \\\n",
    "                      'relationship', 'marital', 'race', 'country', 'capgain', \\\n",
    "                      'caploss'])\n",
    "    return df\n",
    "\n",
    "# one-hot encoding (for regression mdoels)\n",
    "df_train = one_hot_encode(df_train)\n",
    "df_test = one_hot_encode(df_test)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Protected, privileged**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protected: 'gender_Female'=1\n",
    "# privileged: 'gender_Male'=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns='income')\n",
    "y_train = df_train['income']\n",
    "\n",
    "X_test = df_test.drop(columns='income')\n",
    "y_test = df_test['income']\n",
    "\n",
    "# size=10000\n",
    "# X_train = X_train[0:size]\n",
    "# y_train = y_train[0:size]\n",
    "\n",
    "# X_test = X_test[0:size]\n",
    "# y_test = y_test[0:size]\n",
    "X_train_orig = copy.deepcopy(X_train)\n",
    "X_test_orig = copy.deepcopy(X_test)\n",
    "\n",
    "# Scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "num_params = len(clf.coef_.transpose()) + 1 #weights and intercept\n",
    "# clf.classes_, clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute statistical parity difference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFairness(y_pred, X_test): \n",
    "    protected_idx = X_test[X_test['gender_Female']==1].index\n",
    "    numProtected = len(protected_idx)\n",
    "    privileged_idx = X_test[X_test['gender_Male']==1].index\n",
    "    numPrivileged = len(privileged_idx)\n",
    "    \n",
    "    p_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        p_protected += y_pred[protected_idx[i]][1]\n",
    "    p_protected /= len(protected_idx)\n",
    "    \n",
    "    p_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        p_privileged += y_pred[privileged_idx[i]][1]\n",
    "    p_privileged /= len(privileged_idx)\n",
    "    \n",
    "    spd = p_protected - p_privileged\n",
    "    return spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Influence of points computed using ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ground_truth_influence(X_train, y_train, X_test, X_test_orig):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    spd_0 = computeFairness(y_pred, X_test_orig)\n",
    "\n",
    "    delta_spd = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_removed = np.delete(X_train, i, 0)\n",
    "        y_removed = y_train.drop(index=i, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        delta_spd_i = computeFairness(y_pred, X_test_orig) - spd_0\n",
    "        delta_spd.append(delta_spd_i)\n",
    "    \n",
    "    return delta_spd\n",
    "\n",
    "# delta_spd = ground_truth_influence(X_train, y_train, X_test, X_test_orig)\n",
    "\n",
    "# with open('delta_spd_ground_truth_v0.txt', 'w') as filehandle:\n",
    "#     for listitem in delta_spd:\n",
    "#         filehandle.write('%s\\n' % listitem)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function** (Log loss for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.360972684923813\n"
     ]
    }
   ],
   "source": [
    "def logistic_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(y_true)):\n",
    "        loss += - y_true[i] * math.log(y_pred[i][1]) - (1 - y_true[i]) * math.log(y_pred[i][0])\n",
    "    loss /= len(y_true)\n",
    "    return loss\n",
    "\n",
    "print(logistic_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of loss function at z with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_L_del_theta_i(num_params, y_true, x, y_pred):\n",
    "    del_L_del_theta = np.ones((num_params, 1)) * ((1 - y_true) * y_pred[1] - y_true * y_pred[0])\n",
    "    for j in range(1, num_params):\n",
    "            del_L_del_theta[j] *=  x[j-1]\n",
    "    return del_L_del_theta\n",
    "# value to be multiplied by a factor of 1/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second-order partial derivative of loss function with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_one_point(num_params, x, y_pred):\n",
    "    H = np.ones((num_params, num_params)) * (y_pred[0] * y_pred[1])\n",
    "    for i in range(1, num_params):\n",
    "        for j in range(i+1):\n",
    "            if j == 0:\n",
    "                H[i][j] *= x[i-1]\n",
    "            else:\n",
    "                H[i][j] *= x[i-1] * x[j-1] \n",
    "    i_lower = np.tril_indices(num_params, -1)\n",
    "    H.T[i_lower] = H[i_lower]     \n",
    "    return H\n",
    "# value to be multiplied by a factor of 1/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of $P(y \\mid \\textbf{x})$ with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_f_del_theta_i(num_params, x, y_pred):\n",
    "    del_f_del_theta = np.ones((num_params, 1)) * (y_pred[0] * y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_f_del_theta[j] *=  x[j-1]\n",
    "    return del_f_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($Statistical parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07592556 -0.02845483 -0.00188227 -0.048868    0.18897018 -0.18897018\n",
      "   0.00432292  0.00389831  0.02087455  0.00332219 -0.02144056  0.05088862\n",
      "   0.01123877 -0.10314517  0.00858797  0.05175203  0.01726284  0.03296542\n",
      "  -0.00357517  0.01094144  0.01323908 -0.0188173  -0.02027118  0.00537222\n",
      "   0.00162495 -0.16729359  0.05124135  0.01690447  0.03085331  0.05147186\n",
      "   0.14380155  0.04574062 -0.00243153 -0.04495888 -0.00261241 -0.00736998\n",
      "   0.00239309 -0.00383049  0.02655014  0.00646609  0.0154396  -0.01892167\n",
      "  -0.00983248  0.00367317 -0.02545955]]\n"
     ]
    }
   ],
   "source": [
    "# Return v = del(SPD)/del(theta)\n",
    "def del_spd_del_theta(num_params, X_test_orig, X_test):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    numProtected = X_test_orig['gender_Female'].sum()\n",
    "    numPrivileged = X_test_orig['gender_Male'].sum()\n",
    "    for i in range(len(X_test)):\n",
    "        y_pred = clf.predict_proba(np.reshape(X_test[i], (1, num_params-1)))\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[i], y_pred[0])\n",
    "        if X_test_orig.iloc[i]['gender_Male'] == 1: #privileged\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "        elif X_test_orig.iloc[i]['gender_Female'] == 1:\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_privileged /= numPrivileged\n",
    "    del_f_protected /= numProtected\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v\n",
    "\n",
    "v = del_spd_del_theta(num_params, X_test_orig, X_test)\n",
    "print(v.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic estimation of Hessian vector product: $H_{\\theta}^{-1}v = H_{\\theta}^{-1}\\nabla_{\\theta}f(z, \\theta) = v + [I - \\nabla_{\\theta}^2L(z_{s_j}, \\theta^*)]H_{\\theta}^{-1}v$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1672.9111597   -580.29551121   -34.57929932 -1111.86475458\n",
      "   4720.8993446  -4720.8993446     95.04910186    79.32070279\n",
      "    443.19416718    65.2445744   -453.11135939  1142.13366335\n",
      "    260.67203796 -2231.0055117    172.90148265  1062.77409049\n",
      "    374.89408321   791.35171249  -118.76142207   269.7143674\n",
      "    263.86542359  -410.05720537  -417.42980648   133.53820109\n",
      "     35.69504493 -3825.71335916  1045.49530535   394.81820952\n",
      "    664.39427663  1202.87330613  3575.00767673  1056.95962479\n",
      "    -69.24628059  -994.59089849   -20.54905754  -157.76678817\n",
      "     15.73158025  -113.48342033   589.85550467   149.44073924\n",
      "    322.35913863  -439.07760244  -214.15738769    90.2713275\n",
      "   -597.73291738]]\n"
     ]
    }
   ],
   "source": [
    "# Uniformly sample t points from training data \n",
    "def hessian_vector_product(num_params, X_train, v):\n",
    "    size = len(X_train)\n",
    "    sample = random.sample(range(len(X_train)), size)\n",
    "\n",
    "    hinv_v = copy.deepcopy(v)\n",
    "    for idx in range(size):\n",
    "        i = sample[idx]\n",
    "        y_pred = clf.predict_proba(np.reshape(X_train[i], (1, num_params-1)))\n",
    "        hessian_i = hessian_one_point(num_params, X_train[i], y_pred[0])/len(X_train)\n",
    "        hinv_v = np.matmul(np.subtract(np.identity(num_params), hessian_i), hinv_v)\n",
    "        hinv_v = np.add(hinv_v, v)\n",
    "        \n",
    "    return hinv_v\n",
    "\n",
    "hinv_v = hessian_vector_product(num_params, X_train, v)\n",
    "print(hinv_v.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Influence of points computed using Hessian vector product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_vector_product_influence(num_params, X_train, y_train, hinv_v):\n",
    "    infs = []\n",
    "    clf.fit(X_train, y_train)\n",
    "    for i in range(len(X_train)):\n",
    "        y_pred = clf.predict_proba(np.reshape(X_train[i], (1, num_params-1)))\n",
    "        del_L_del_theta = del_L_del_theta_i(num_params, y_train[i], X_train[i], y_pred[0])/len(X_train)\n",
    "        inf = -np.dot(del_L_del_theta.transpose(), hinv_v)\n",
    "        inf *= -1/len(X_train)\n",
    "        infs.append(inf[0][0].tolist())\n",
    "    \n",
    "    return infs\n",
    "\n",
    "infs = hessian_vector_product_influence(num_params, X_train, y_train, hinv_v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.9387353625440371, pvalue=0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_spd = pd.read_csv('delta_spd_ground_truth_v0.txt', names=[\"Values\"], sep=\",\")\n",
    "stats.spearmanr(delta_spd, infs)\n",
    "# stats.pearsonr(delta_spd, infs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Effect of removing top-k influential data points on SPD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t-0.20059371090978573\n",
      "1\t-0.20019896110340057\n",
      "2\t-0.20079805860725908\n",
      "3\t-0.20089537129947332\n",
      "4\t-0.2003245351770151\n",
      "5\t-0.199197101881629\n",
      "6\t-0.2003945898067733\n",
      "7\t-0.19987096655872746\n",
      "8\t-0.19886892180907423\n",
      "9\t-0.19955410806643825\n",
      "10\t-0.2026601953926205\n",
      "11\t-0.20069799065008298\n",
      "12\t-0.19925243413043084\n",
      "13\t-0.20010917105113452\n",
      "14\t-0.1995912969401628\n",
      "15\t-0.20105193972898766\n",
      "16\t-0.1978304536330429\n",
      "17\t-0.19869418148789794\n",
      "18\t-0.1969436525721363\n",
      "19\t-0.20266227216431026\n",
      "20\t-0.20247976513226312\n",
      "21\t-0.20089556078731932\n",
      "22\t-0.20097722417441716\n",
      "23\t-0.2015690302725581\n",
      "24\t-0.19765885674580252\n",
      "25\t-0.2020900570727276\n",
      "26\t-0.2017976129622087\n",
      "27\t-0.20097957033763628\n",
      "28\t-0.19920461175158355\n",
      "29\t-0.20338241133326548\n",
      "30\t-0.20228737565482172\n",
      "31\t-0.19979759149690451\n",
      "32\t-0.2008541448640143\n",
      "33\t-0.1970410515122821\n",
      "34\t-0.1988089368563656\n",
      "35\t-0.19762442035380248\n",
      "36\t-0.19524952107177795\n",
      "37\t-0.20064507728904932\n",
      "38\t-0.19981446527215851\n",
      "39\t-0.2005872506420061\n",
      "40\t-0.20102436114224154\n",
      "41\t-0.1984441160670296\n",
      "42\t-0.1977382954570795\n",
      "43\t-0.20088241245109895\n",
      "44\t-0.20502625478135222\n",
      "45\t-0.2041354953704872\n",
      "46\t-0.20553103355874883\n",
      "47\t-0.2042752258165501\n",
      "48\t-0.20264285692452016\n",
      "49\t-0.19901352420054066\n",
      "50\t-0.19751165803179688\n",
      "51\t-0.20192153631206092\n",
      "52\t-0.20932533612923848\n",
      "53\t-0.20435129667772395\n",
      "54\t-0.20969457384260726\n",
      "55\t-0.1976165283946592\n",
      "56\t-0.19552363444069834\n",
      "57\t-0.19567755033035622\n",
      "58\t-0.20757364144070334\n",
      "59\t-0.1990776232893694\n",
      "60\t-0.19029792255307748\n",
      "61\t-0.20571744971750217\n",
      "62\t-0.20353488593486913\n",
      "63\t-0.20252974915629657\n",
      "64\t-0.20196983560403328\n",
      "65\t-0.19243687219667216\n",
      "66\t-0.20208923511305976\n",
      "67\t-0.19345385858398068\n",
      "68\t-0.19832808403805885\n",
      "69\t-0.19488478499681927\n",
      "70\t-0.20483411647287392\n",
      "71\t-0.20601833632505206\n",
      "72\t-0.19858360369815345\n",
      "73\t-0.1998228245541554\n",
      "74\t-0.20622884311364692\n",
      "75\t-0.19445457100220565\n",
      "76\t-0.19053861470254313\n",
      "77\t-0.19461034386936027\n",
      "78\t-0.19471540757194572\n",
      "79\t-0.19670706351199535\n",
      "80\t-0.19526320362373892\n",
      "81\t-0.20937099373415327\n",
      "82\t-0.20356844523219786\n",
      "83\t-0.19497643543532744\n",
      "84\t-0.21013464067167442\n",
      "85\t-0.17557272578697775\n",
      "86\t-0.19239385349030086\n",
      "87\t-0.1959040492234262\n",
      "88\t-0.19528403971753222\n",
      "89\t-0.19866581757960788\n",
      "90\t-0.1957991195636558\n",
      "91\t-0.20663167491601328\n",
      "92\t-0.18865900415470443\n",
      "93\t-0.20038929008622675\n",
      "94\t-0.2048185573353009\n",
      "95\t-0.2075225565198976\n",
      "96\t-0.21628545002943153\n",
      "97\t-0.20504432253980764\n",
      "98\t-0.19163668083434315\n",
      "99\t-0.16924495565396808\n"
     ]
    }
   ],
   "source": [
    "def top_k_influence(X_train, y_train, X_test, X_test_orig, size, infs):\n",
    "    if (size==0):\n",
    "        clf.fit(X_train, y_train)\n",
    "    else:\n",
    "#         idx = sorted(range(len(infs)), key = lambda x:infs[x])[-size:] \n",
    "        idx = random.sample(range(len(X_train)), size) #without replacement\n",
    "        X_removed = np.delete(X_train, idx, 0)\n",
    "        y_removed = y_train.drop(index=idx, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    spd = computeFairness(y_pred, X_test_orig)\n",
    "    return spd\n",
    "\n",
    "for i in range(100):\n",
    "    size = int(len(X_train) * i/100)\n",
    "#     delta_spd = (top_k_influence(X_train, y_train, X_test, X_test_orig, i, infs) - spd_0)/spd_0\n",
    "    spd = (top_k_influence(X_train, y_train, X_test, X_test_orig, size, infs))\n",
    "    print('{}\\t{}'.format(i, spd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision-tree predicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPD:  -0.027324387352106308\n",
      "%Rows removed:  46.63152310854718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "predicates = ['marital_Married-civ-spouse', 'relationship_Husband', 'gender_Male']\n",
    "\n",
    "idx = X_train_orig[(X_train_orig[predicates[0]] == 1)\n",
    "#                    & (X_train_orig[predicates[1]] == 1) \n",
    "#                    & (X_train_orig[predicates[2]] == 1)\n",
    "                  ].index \n",
    "X_removed = np.delete(X_train, idx, 0)\n",
    "y_removed = y_train.drop(index=idx, inplace=False)\n",
    "clf.fit(X_removed, y_removed)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "spd = computeFairness(y_pred, X_test_orig)\n",
    "perc_removed = len(idx)*100/len(X_train)\n",
    "print(\"SPD: \",spd)\n",
    "print(\"%Rows removed: \", perc_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ground truth influence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "spd_gt = pd.read_csv('delta_spd_ground_truth_v0.txt', names=[\"Values\"], sep=\",\")\n",
    "spd_gt = spd_gt.values.tolist()\n",
    "spdgt=[]\n",
    "for i in range(len(spd_gt)):\n",
    "    spdgt.append(spd_gt[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30162,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t-0.20059371090978573\n",
      "1\t-0.1782636035442026\n",
      "2\t-0.16138098146193416\n",
      "3\t-0.1453020086840519\n",
      "4\t-0.1299314006209027\n",
      "5\t-0.11455255231500353\n",
      "6\t-0.09906797269479006\n",
      "7\t-0.08415625898807713\n",
      "8\t-0.06912851506354964\n",
      "9\t-0.053762661676293094\n",
      "10\t-0.03845447539716873\n",
      "11\t-0.022883696770197803\n",
      "12\t-0.0069519674255935915\n",
      "13\t0.009635389654990473\n",
      "14\t0.026110232107541292\n",
      "15\t0.04229223409307015\n",
      "16\t0.05864091259709972\n",
      "17\t0.07567459879616573\n",
      "18\t0.09263938994558184\n",
      "19\t0.10874852459709408\n",
      "20\t0.12592647412326624\n",
      "21\t0.14355613718638893\n",
      "22\t0.16086556626243464\n",
      "23\t0.1787040715503414\n",
      "24\t0.19698420982794979\n",
      "25\t0.2145056165142775\n",
      "26\t0.23323263524350527\n",
      "27\t0.2520912837686001\n",
      "28\t0.2717961499960052\n",
      "29\t0.2933610044954307\n",
      "30\t0.3142703931798674\n",
      "31\t0.33570938135492495\n",
      "32\t0.3557685304274989\n",
      "33\t0.36640027436200606\n",
      "34\t0.3752785781891367\n",
      "35\t0.38446843837995637\n",
      "36\t0.39685597654962557\n",
      "37\t0.41072394649926025\n",
      "38\t0.4236195895482251\n",
      "39\t0.43800710991599523\n",
      "40\t0.45350484840607586\n",
      "41\t0.4668075931906793\n",
      "42\t0.480221969976156\n",
      "43\t0.4923393060815152\n",
      "44\t0.5026350364295842\n",
      "45\t0.5125247705143442\n",
      "46\t0.5260088425157454\n",
      "47\t0.5443806249184759\n",
      "48\t0.5628029517433241\n",
      "49\t0.5768797552391559\n",
      "50\t0.5909366260746367\n",
      "51\t0.6032442775190153\n",
      "52\t0.6237070027182525\n",
      "53\t0.6330527290845283\n",
      "54\t0.644089401049663\n",
      "55\t0.6571902074882948\n",
      "56\t0.6700750706638781\n",
      "57\t0.6798470500064414\n",
      "58\t0.6916981580891738\n",
      "59\t0.6952351515947826\n",
      "60\t0.6990145928869033\n",
      "61\t0.7101521963461359\n",
      "62\t0.7143736691230754\n",
      "63\t0.717036041667864\n",
      "64\t0.7318576243043247\n",
      "65\t0.7433164460410336\n",
      "66\t0.7517418336992385\n",
      "67\t0.8007895889813763\n",
      "68\t0.8448848508576425\n",
      "69\t0.9961030480697407\n",
      "70\t0.99633388625425\n",
      "71\t0.9965165726682232\n",
      "72\t0.9966782726734079\n",
      "73\t0.996803833271093\n",
      "74\t0.9969065242530429\n",
      "75\t0.9970136949911331\n",
      "76\t0.9971049523259253\n",
      "77\t0.9971660011173958\n",
      "78\t0.9972030093645291\n",
      "79\t0.9972140063658758\n",
      "80\t0.9972081373180512\n",
      "81\t0.9971765792956676\n",
      "82\t0.9971065119497765\n",
      "83\t0.9970290040478287\n",
      "84\t0.9968583791523645\n",
      "85\t0.9966078841649284\n",
      "86\t0.9962644726266201\n",
      "87\t0.9956792025933904\n",
      "88\t0.9945419918108356\n",
      "89\t0.9922451051252059\n",
      "90\t0.9900055794071776\n",
      "91\t0.9850344188381807\n",
      "92\t0.9753366182919347\n",
      "93\t0.9624189043455026\n",
      "94\t0.9363607706118465\n",
      "95\t0.8520250581333025\n",
      "96\t0.7827965391494923\n",
      "97\t0.6702775842535347\n",
      "98\t0.01035698375247951\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-e67778b42cf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mspd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_influence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspd_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}\\t{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-e67778b42cf8>\u001b[0m in \u001b[0;36mgt_influence\u001b[0;34m(X_train, y_train, X_test, X_test_orig, size, spd_gt)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mX_removed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my_removed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_removed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_removed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mspd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeFairness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[0m\u001b[1;32m   1373\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m                              \" class: %r\" % classes_[0])\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "def gt_influence(X_train, y_train, X_test, X_test_orig, size, spd_gt):\n",
    "    if (size==0):\n",
    "        clf.fit(X_train, y_train)\n",
    "    else:\n",
    "        idx = sorted(range(len(spd_gt)), key = lambda x:spd_gt[x])[-size:] \n",
    "        X_removed = np.delete(X_train, idx, 0)\n",
    "        y_removed = y_train.drop(index=idx, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    spd = computeFairness(y_pred, X_test_orig)\n",
    "    return spd\n",
    "\n",
    "for i in range(100):\n",
    "    size = int(len(X_train) * i/100)\n",
    "    spd = gt_influence(X_train, y_train, X_test, X_test_orig, size, spd_gt)\n",
    "    print('{}\\t{}'.format(i, spd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
