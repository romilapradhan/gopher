{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:04.025326Z",
     "start_time": "2021-11-08T19:15:03.054746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x172e6e830>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import tqdm\n",
    "import time\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from load_dataset import load, generate_random_dataset\n",
    "from classifier import NeuralNetwork, LogisticRegression, SVM\n",
    "from utils import *\n",
    "from metrics import *  # include fairness and corresponding derivatives\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "from operator import itemgetter\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:04.028624Z",
     "start_time": "2021-11-08T19:15:04.026673Z"
    }
   },
   "outputs": [],
   "source": [
    "# ignore all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:04.047940Z",
     "start_time": "2021-11-08T19:15:04.029501Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "dataset = 'german'\n",
    "X_train, X_test, y_train, y_test = load(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:04.053043Z",
     "start_time": "2021-11-08T19:15:04.049488Z"
    }
   },
   "outputs": [],
   "source": [
    "duplicates = 1\n",
    "make_duplicates = lambda x, d: pd.concat([x]*d, axis=0).reset_index(drop=True)\n",
    "X_train = make_duplicates(X_train, duplicates)\n",
    "X_test = make_duplicates(X_test, duplicates)\n",
    "y_train = make_duplicates(y_train, duplicates)\n",
    "y_test = make_duplicates(y_test, duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:04.060434Z",
     "start_time": "2021-11-08T19:15:04.053807Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "X_train_orig = copy.deepcopy(X_train)\n",
    "X_test_orig = copy.deepcopy(X_test)\n",
    "\n",
    "# Scale data: regularization penalty default: ‘l2’, ‘lbfgs’ solvers support only l2 penalties. \n",
    "# Regularization makes the predictor dependent on the scale of the features.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function** (Log loss for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:04.108076Z",
     "start_time": "2021-11-08T19:15:04.103960Z"
    }
   },
   "outputs": [],
   "source": [
    "# clf = NeuralNetwork(input_size=X_train.shape[-1])\n",
    "clf = LogisticRegression(input_size=X_train.shape[-1])\n",
    "# clf = SVM(input_size=X_train.shape[-1])\n",
    "num_params = len(convert_grad_to_ndarray(list(clf.parameters())))\n",
    "if isinstance(clf, LogisticRegression) or isinstance(clf, NeuralNetwork):\n",
    "    loss_func = logistic_loss_torch\n",
    "elif isinstance(clf, SVM):\n",
    "    loss_func = svm_loss_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Influence of points computed using ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:04.281321Z",
     "start_time": "2021-11-08T19:15:04.270545Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def ground_truth_influence(X_train, y_train, X_test, X_test_orig, y_test):\n",
    "    clf.fit(X_train, y_train, verbose=True)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    spd_0 = computeFairness(y_pred, X_test_orig, y_test, 0)\n",
    "\n",
    "    delta_spd = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_removed = np.delete(X_train, i, 0)\n",
    "        y_removed = y_train.drop(index=i, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        delta_spd_i = computeFairness(y_pred, X_test_orig, y_test, 0) - spd_0\n",
    "        delta_spd.append(delta_spd_i)\n",
    "\n",
    "    return delta_spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Accuracy** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:04.633303Z",
     "start_time": "2021-11-08T19:15:04.628924Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def computeAccuracy(y_true, y_pred):\n",
    "    return np.sum((y_pred>0.5) == y_true)/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of loss function at z with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:05.168586Z",
     "start_time": "2021-11-08T19:15:05.161913Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_L_del_theta_i(model, x, y_true, retain_graph=False):\n",
    "    loss = loss_func(model, x, y_true)\n",
    "    w = [ p for p in model.parameters() if p.requires_grad ]\n",
    "    return grad(loss, w, create_graph=True, retain_graph=retain_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of $P(y \\mid \\textbf{x})$ with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:05.742094Z",
     "start_time": "2021-11-08T19:15:05.734287Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_f_del_theta_i(model, x, retain_graph=False):\n",
    "    w = [ p for p in model.parameters() if p.requires_grad ]\n",
    "    return grad(model(torch.FloatTensor(x)), w, retain_graph=retain_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic estimation of Hessian vector product (involving del fairness): $H_{\\theta}^{-1}v = H_{\\theta}^{-1}\\nabla_{\\theta}f(z, \\theta) = v + [I - \\nabla_{\\theta}^2L(z_{s_j}, \\theta^*)]H_{\\theta}^{-1}v$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:06.214957Z",
     "start_time": "2021-11-08T19:15:06.203177Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def hvp(y, w, v):\n",
    "    ''' Multiply the Hessians of y and w by v.'''\n",
    "    # First backprop\n",
    "    first_grads = grad(y, w, retain_graph=True, create_graph=True)\n",
    "\n",
    "    # Elementwise products\n",
    "    elemwise_products = 0\n",
    "    for grad_elem, v_elem in zip(convert_grad_to_tensor(first_grads), v):\n",
    "        elemwise_products += torch.sum(grad_elem * v_elem)\n",
    "\n",
    "    # Second backprop\n",
    "    return_grads = grad(elemwise_products, w, create_graph=True)\n",
    "\n",
    "    return return_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:06.729322Z",
     "start_time": "2021-11-08T19:15:06.716183Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def hessian_one_point(model, x, y):\n",
    "    x, y = torch.FloatTensor(x), torch.FloatTensor([y])\n",
    "    loss = loss_func(model, x, y)\n",
    "    params = [ p for p in model.parameters() if p.requires_grad ]\n",
    "    first_grads = convert_grad_to_tensor(grad(loss, params, retain_graph=True, create_graph=True))\n",
    "    hv = np.zeros((len(first_grads), len(first_grads)))\n",
    "    for i in range(len(first_grads)):\n",
    "        hv[i, :] = convert_grad_to_ndarray(grad(first_grads[i], params, create_graph=True)).ravel()\n",
    "    return hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:06.809945Z",
     "start_time": "2021-11-08T19:15:06.790830Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Compute multiplication of inverse hessian matrix and vector v\n",
    "def s_test(model, xs, ys, v, hinv=None, damp=0.01, scale=25.0, r=-1, batch_size=-1, recursive=False, verbose=False):\n",
    "    ''' Arguments:\n",
    "        xs: list of data points\n",
    "        ys: list of true labels corresponding to data points in xs\n",
    "        damp: dampening factor\n",
    "        scale: scaling factor\n",
    "        r: number of iterations aka recursion depth\n",
    "            should be enough so that the value stabilises.\n",
    "        batch_size: number of instances in each batch in recursive approximation\n",
    "        recursive: determine whether to recursively approximate hinv_v'''\n",
    "    xs, ys = torch.FloatTensor(xs.copy()), torch.FloatTensor(ys.copy())\n",
    "    n = len(xs)\n",
    "    if recursive:\n",
    "        hinv_v = copy.deepcopy(v)\n",
    "        if verbose:\n",
    "            print('Computing s_test...')\n",
    "            tbar = tqdm.tqdm(total=r)\n",
    "        if (batch_size == -1):  # default\n",
    "            batch_size = 10\n",
    "        if (r == -1):\n",
    "            r = n // batch_size + 1\n",
    "        sample = np.random.choice(range(n), r*batch_size, replace=True)\n",
    "        for i in range(r):\n",
    "            sample_idx = sample[i*batch_size:(i+1)*batch_size]\n",
    "            x, y = xs[sample_idx], ys[sample_idx]\n",
    "            loss = loss_func(model, x, y)\n",
    "            params = [ p for p in model.parameters() if p.requires_grad ]\n",
    "            hv = convert_grad_to_ndarray(hvp(loss, params, torch.FloatTensor(hinv_v)))\n",
    "            # Recursively caclulate h_estimate\n",
    "            hinv_v = v + (1 - damp) * hinv_v - hv / scale\n",
    "            if verbose:\n",
    "                tbar.update(1)\n",
    "    else:\n",
    "        if hinv is None:\n",
    "            hinv = np.linalg.pinv(np.sum(hessian_all_points, axis=0))\n",
    "        scale = 1.0\n",
    "        hinv_v = np.matmul(hinv, v)\n",
    "\n",
    "    return hinv_v / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics: Initial state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:07.029095Z",
     "start_time": "2021-11-08T19:15:06.952078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial statistical parity:  -0.09508493496105075\n",
      "Initial TPR parity:  -0.07770497195525183\n",
      "Initial predictive parity:  -0.10142464519673511\n",
      "Initial loss:  0.5078865702279716\n",
      "Initial accuracy:  0.755\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(input_size=X_train.shape[-1])\n",
    "# clf = NeuralNetwork(input_size=X_train.shape[-1])\n",
    "# clf = SVM(input_size=X_train.shape[-1])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "y_pred_train = clf.predict_proba(X_train)\n",
    "\n",
    "spd_0 = computeFairness(y_pred_test, X_test_orig, y_test, 0, dataset)\n",
    "print(\"Initial statistical parity: \", spd_0)\n",
    "\n",
    "tpr_parity_0 = computeFairness(y_pred_test, X_test_orig, y_test, 1, dataset)\n",
    "print(\"Initial TPR parity: \", tpr_parity_0)\n",
    "\n",
    "predictive_parity_0 = computeFairness(y_pred_test, X_test_orig, y_test, 2, dataset)\n",
    "print(\"Initial predictive parity: \", predictive_parity_0)\n",
    "\n",
    "loss_0 = logistic_loss(y_test, y_pred_test)\n",
    "print(\"Initial loss: \", loss_0)\n",
    "\n",
    "accuracy_0 = computeAccuracy(y_test, y_pred_test)\n",
    "print(\"Initial accuracy: \", accuracy_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:14.405030Z",
     "start_time": "2021-11-08T19:15:07.734553Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 795/800 [00:06<00:00, 121.45it/s]"
     ]
    }
   ],
   "source": [
    "hessian_all_points = []\n",
    "tbar = tqdm.tqdm(total=len(X_train))\n",
    "total_time = 0\n",
    "for i in range(len(X_train)):\n",
    "    t0 = time.time()\n",
    "    hessian_all_points.append(hessian_one_point(clf, X_train[i], y_train[i])/len(X_train))\n",
    "    total_time += time.time()-t0\n",
    "    tbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:14.790415Z",
     "start_time": "2021-11-08T19:15:14.775583Z"
    }
   },
   "outputs": [],
   "source": [
    "hessian_all_points = np.array(hessian_all_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:15.121398Z",
     "start_time": "2021-11-08T19:15:15.112958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.600985288619995"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-compute: (1) Hessian (2) del_L_del_theta for each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:16.013259Z",
     "start_time": "2021-11-08T19:15:15.794125Z"
    }
   },
   "outputs": [],
   "source": [
    "del_L_del_theta = []\n",
    "for i in range(int(len(X_train))):\n",
    "    gradient = convert_grad_to_ndarray(del_L_del_theta_i(clf, X_train[i], int(y_train[i])))\n",
    "    while np.sum(np.isnan(gradient))>0:\n",
    "        gradient = convert_grad_to_ndarray(del_L_del_theta_i(clf, X_train[i], int(y_train[i])))\n",
    "    del_L_del_theta.append(gradient)\n",
    "del_L_del_theta = np.array(del_L_del_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Select delta fairness function depending on selected metric*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:16.937285Z",
     "start_time": "2021-11-08T19:15:16.850165Z"
    }
   },
   "outputs": [],
   "source": [
    "metric = 0\n",
    "if metric == 0:\n",
    "    v1 = del_spd_del_theta(clf, X_test_orig, X_test, dataset)\n",
    "elif metric == 1:\n",
    "    v1 = del_tpr_parity_del_theta(clf, X_test_orig, X_test, y_test, dataset)\n",
    "elif metric == 2:\n",
    "    v1 = del_predictive_parity_del_theta(clf, X_test_orig, X_test, y_test, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:17.175493Z",
     "start_time": "2021-11-08T19:15:17.161121Z"
    }
   },
   "outputs": [],
   "source": [
    "hinv = np.linalg.pinv(np.sum(hessian_all_points, axis=0))\n",
    "hinv_v = s_test(clf, X_train, y_train, v1, hinv=hinv, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:17.773614Z",
     "start_time": "2021-11-08T19:15:17.765194Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def first_order_influence(del_L_del_theta, hinv_v, n):\n",
    "    infs = []\n",
    "    for i in range(n):\n",
    "        inf = -np.dot(del_L_del_theta[i].transpose(), hinv_v)\n",
    "        inf *= -1/n\n",
    "        infs.append(inf)\n",
    "    return infs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:18.118117Z",
     "start_time": "2021-11-08T19:15:18.108465Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def first_order_group_influence(U, del_L_del_theta):\n",
    "    infs = []\n",
    "    u = len(U)\n",
    "    n = len(X_train)\n",
    "    for i in range(u):\n",
    "        idx = U[i]\n",
    "        inf = -np.dot(del_L_del_theta[idx].transpose(), hinv)\n",
    "        inf *= -1/n\n",
    "        infs.append(inf)\n",
    "    return np.sum(infs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second-order influence computation for a group of points in subset U**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:18.685460Z",
     "start_time": "2021-11-08T19:15:18.667403Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def second_order_influence(model, X_train, y_train, U, del_L_del_theta, r=-1, verbose=False):\n",
    "    u = len(U)\n",
    "    s = len(X_train)\n",
    "    p = u/s\n",
    "    c1 = (1 - 2*p)/(s * (1-p)**2)\n",
    "    c2 = 1/((s * (1-p))**2)\n",
    "    num_params = len(del_L_del_theta[0])\n",
    "    del_L_del_theta_sum = np.sum([del_L_del_theta[i] for i in U], axis=0)\n",
    "    hinv_del_L_del_theta= s_test(model, X_train, y_train, del_L_del_theta_sum, hinv=hinv)\n",
    "    hessian_U_hinv_del_L_del_theta = np.zeros((num_params,))\n",
    "    for i in range(u):\n",
    "        idx = U[i]\n",
    "        x, y = torch.FloatTensor(X_train[idx]), torch.FloatTensor([y_train[idx]])\n",
    "        loss = loss_func(model, x, y)\n",
    "        params = [ p for p in model.parameters() if p.requires_grad ]\n",
    "        hessian_U_hinv_del_L_del_theta += convert_grad_to_ndarray(hvp(loss, params, torch.FloatTensor(hinv_del_L_del_theta)))\n",
    "\n",
    "    term1 = c1 * hinv_del_L_del_theta\n",
    "    term2 = c2 * s_test(model, X_train, y_train, hessian_U_hinv_del_L_del_theta, hinv=hinv)\n",
    "    sum_term = term1 + term2\n",
    "    return sum_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:19.042650Z",
     "start_time": "2021-11-08T19:15:19.027732Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def second_order_group_influence(U, del_L_del_theta):\n",
    "    u = len(U)\n",
    "    s = len(X_train)\n",
    "    p = u/s\n",
    "    c1 = (1 - 2*p)/(s * (1-p)**2)\n",
    "    c2 = 1/((s * (1-p))**2)\n",
    "    num_params = len(del_L_del_theta[0])\n",
    "    del_L_del_theta_sum = np.sum(del_L_del_theta[U, :], axis=0)\n",
    "    hinv_del_L_del_theta= np.matmul(hinv, del_L_del_theta_sum)\n",
    "    hessian_U_hinv_del_L_del_theta = np.sum(np.matmul(hessian_all_points[U, :], hinv_del_L_del_theta), axis=0)\n",
    "    term1 = c1 * hinv_del_L_del_theta\n",
    "    term2 = c2 * np.matmul(hinv, hessian_U_hinv_del_L_del_theta)\n",
    "    sum_term = (term1 + term2*len(X_train))\n",
    "    return sum_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence of each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:22.413312Z",
     "start_time": "2021-11-08T19:15:22.401825Z"
    }
   },
   "outputs": [],
   "source": [
    "infs_1 = first_order_influence(del_L_del_theta, hinv_v, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fairness: Ground-truth subset influence vs. computed subset influences: Coherent subset** \n",
    "\n",
    "(by coherent, we mean group of data points that share some properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE:*** The retraining of the clf would cause the change in model parameters and thus lead to the change of gradients, so in this part, we first acquire all the first- and second-order influence functions together based on the original model. After all the influence functions are calculated, we retrain the model corresponding to different removed coherent subset of data and get the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:23.574401Z",
     "start_time": "2021-11-08T19:15:23.567677Z"
    }
   },
   "outputs": [],
   "source": [
    "time_gt = []\n",
    "time_first = []\n",
    "time_second = []\n",
    "rep = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:24.357045Z",
     "start_time": "2021-11-08T19:15:24.344848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_f_lower: 0.0009508493496105075\n",
      "alpha_f_upper: 0.09508493496105075\n",
      "del_f_threshold: -0.009508493496105075\n",
      "support_small: 0.3\n",
      "del_f_threshold_small: 0.009508493496105075\n"
     ]
    }
   ],
   "source": [
    "alpha_f_lower = (-0.01) * (spd_0)\n",
    "alpha_f_upper = -spd_0\n",
    "del_f_threshold = (0.1) * spd_0\n",
    "support = 0.05 # Do not consider extremely small patterns\n",
    "support_small = 0.3 # For small patterns, 2nd-order estimation is quite accurate\n",
    "del_f_threshold_small = (-0.1) * (spd_0)\n",
    "print(\"alpha_f_lower:\", alpha_f_lower)\n",
    "print(\"alpha_f_upper:\", alpha_f_upper)\n",
    "print(\"del_f_threshold:\", del_f_threshold)\n",
    "print(\"support_small:\", support_small)\n",
    "print(\"del_f_threshold_small:\", del_f_threshold_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:27.241266Z",
     "start_time": "2021-11-08T19:15:27.102311Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:19<00:00, 121.45it/s]"
     ]
    }
   ],
   "source": [
    "for _ in range(rep):\n",
    "#     Get the original model\n",
    "#     clf = NeuralNetwork(input_size=X_train.shape[-1])\n",
    "    clf = LogisticRegression(input_size=X_train.shape[-1])\n",
    "#     clf = SVM()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    attributes = []\n",
    "    attributeValues = []\n",
    "    first_order_influences = []\n",
    "    second_order_influences = []\n",
    "    fractionRows = []\n",
    "\n",
    "    v1_orig = v1\n",
    "    for col in X_train_orig.columns:\n",
    "        if dataset == 'german':\n",
    "            if \"purpose\" in col or \"housing\" in col: #dummy variables purpose=0 doesn't make sense\n",
    "                vals = [1]\n",
    "            else:\n",
    "                vals = X_train_orig[col].unique()\n",
    "        elif dataset == 'adult':\n",
    "            vals = X_train_orig[col].unique()\n",
    "        elif dataset == 'compas':\n",
    "            vals = X_train_orig[col].unique()\n",
    "        elif dataset == 'sqf':\n",
    "            vals = X_train_orig[col].unique()\n",
    "        elif dataset == 'random':\n",
    "            vals = X_train_orig[col].unique()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        for val in vals:\n",
    "#             print(col, val, sep=\": \")\n",
    "            idx = X_train_orig[X_train_orig[col] == val].index  \n",
    "            if (len(idx)/len(X_train) > support):  \n",
    "                X = np.delete(X_train, idx, 0)\n",
    "                y = y_train.drop(index=idx, inplace=False)\n",
    "                if len(y.unique()) > 1:\n",
    "                    idx = X_train_orig[X_train_orig[col] == val].index \n",
    "\n",
    "                    # First-order subset influence\n",
    "                    t0 = time.time()\n",
    "                    del_f_1 = 0            \n",
    "\n",
    "                    # Second-order subset influence\n",
    "                    t0 = time.time()\n",
    "                    params_f_2 = second_order_group_influence(idx, del_L_del_theta)\n",
    "                    del_f_2 = np.dot(v1.transpose(), params_f_2)\n",
    "                    time_second.append(time.time()-t0)\n",
    "\n",
    "                    attributes.append(col)\n",
    "                    attributeValues.append(val)\n",
    "                    first_order_influences.append(del_f_1)\n",
    "                    second_order_influences.append(del_f_2)\n",
    "                    fractionRows.append(len(idx)/len(X_train)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:29.522914Z",
     "start_time": "2021-11-08T19:15:29.506086Z"
    }
   },
   "outputs": [],
   "source": [
    "expl = [attributes, attributeValues, first_order_influences, second_order_influences, fractionRows]\n",
    "expl = (np.array(expl).T).tolist()\n",
    "\n",
    "explanations = pd.DataFrame(expl, columns=[\"attributes\", \"attributeValues\", \"first_order_influences\", \"second_order_influences\", \"fractionRows\"])\n",
    "explanations['second_order_influences'] = explanations['second_order_influences'].astype(float)\n",
    "explanations['first_order_influences'] = explanations['first_order_influences'].astype(float)\n",
    "explanations['fractionRows'] = explanations['fractionRows'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:30.509146Z",
     "start_time": "2021-11-08T19:15:30.501282Z"
    }
   },
   "outputs": [],
   "source": [
    "candidates = copy.deepcopy(explanations)\n",
    "candidates.loc[:, 'score'] = candidates.loc[:, 'second_order_influences']*100/candidates.loc[:, 'fractionRows']\n",
    "# display(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:30.965618Z",
     "start_time": "2021-11-08T19:15:30.956342Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:15:31.593181Z",
     "start_time": "2021-11-08T19:15:31.585765Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_subset(explanation):\n",
    "    subset = X_train_orig.copy()\n",
    "    for predicate in explanation:\n",
    "        attr = predicate.split(\"=\")[0].strip(' ')\n",
    "        val = int(predicate.split(\"=\")[1].strip(' '))\n",
    "        subset = subset[subset[attr]==val]\n",
    "    return subset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:16:24.283816Z",
     "start_time": "2021-11-08T19:16:07.049716Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated:  62  1-candidates\n",
      "Generated:  341  2-candidates\n",
      "Generated:  2\n",
      "Generated: 2418   3 -candidates\n",
      "Generated:  3\n",
      "Generated: 10809   4 -candidates\n",
      "CPU times: user 17.1 s, sys: 107 ms, total: 17.2 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "candidates_all = []\n",
    "total_rows = len(X_train_orig)\n",
    "\n",
    "# Generating 1-candidates\n",
    "candidates_1 = []\n",
    "for i in range(len(candidates)):\n",
    "    candidate = []\n",
    "    candidate_i = candidates.iloc[i]\n",
    "    if ((candidate_i[\"fractionRows\"] >= support_small) or\n",
    "       ((candidate_i[\"fractionRows\"] >= support) & (candidate_i[\"second_order_influences\"] > del_f_threshold))\n",
    "       ):\n",
    "        attr_i = candidate_i[\"attributes\"]\n",
    "        val_i = int(float(candidate_i[\"attributeValues\"]))\n",
    "        idx = X_train_orig[X_train_orig[attr_i] == val_i].index\n",
    "        predicates = [attr_i + '=' + str(val_i)]\n",
    "        candidate = [predicates, candidate_i[\"fractionRows\"],\n",
    "                     candidate_i[\"score\"], candidate_i[\"second_order_influences\"], idx]\n",
    "        candidates_1.append(candidate)\n",
    "\n",
    "print(\"Generated: \", len(candidates_1), \" 1-candidates\")\n",
    "candidates_1.sort()\n",
    "# display(candidates_1)\n",
    "\n",
    "for i in range(len(candidates_1)):\n",
    "    if (float(candidates_1[i][2]) >= support): # if score > top-k, keep in candidates, not otherwise\n",
    "        candidates_all.insert(len(candidates_all), candidates_1[i])\n",
    "\n",
    "# Generating 2-candidates\n",
    "candidates_2 = []\n",
    "for i in range(len(candidates_1)):\n",
    "    predicate_i = candidates_1[i][0][0]\n",
    "    attr_i = predicate_i.split(\"=\")[0]\n",
    "    val_i = int(float(predicate_i.split(\"=\")[1]))\n",
    "    sup_i = candidates_1[i][1]\n",
    "    idx_i = candidates_1[i][-1]\n",
    "    for j in range(i):\n",
    "        predicate_j = candidates_1[j][0][0]\n",
    "        attr_j = predicate_j.split(\"=\")[0]\n",
    "        val_j = int(float(predicate_j.split(\"=\")[1]))\n",
    "        sup_j = candidates_1[j][1]\n",
    "        idx_j = candidates_1[j][-1]\n",
    "        if (attr_i != attr_j):\n",
    "            idx = idx_i.intersection(idx_j)\n",
    "            fractionRows = len(idx)/total_rows * 100\n",
    "            isCompact = True\n",
    "            if (fractionRows == min(sup_i, sup_j)): # pattern is not compact if intersection equals one of its parents\n",
    "                isCompact = False\n",
    "            if (fractionRows/100 >= support):\n",
    "                params_f_2 = second_order_group_influence(idx, del_L_del_theta)\n",
    "                del_f_2 = np.dot(v1.transpose(), params_f_2)\n",
    "                score = del_f_2 * 100/fractionRows\n",
    "                if ((fractionRows/100 >= support_small) or\n",
    "                    ((score > candidates_1[i][2]) & (score > candidates_1[j][2]))):\n",
    "                        predicates = [attr_i + '=' + str(val_i), attr_j + '=' + str(val_j)]\n",
    "                        candidate = [sorted(predicates, key=itemgetter(0)), len(idx)*100/total_rows,\n",
    "                                    score, del_f_2, idx]\n",
    "                        candidates_2.append(candidate)\n",
    "#                         print(candidate)\n",
    "                        if (isCompact):\n",
    "                            candidates_all.append(candidate)\n",
    "print(\"Generated: \", len(candidates_2), \" 2-candidates\")\n",
    "\n",
    "# Recursively generating the rest\n",
    "candidates_L_1 = copy.deepcopy(candidates_2)\n",
    "set_L_1 = set()\n",
    "iteration = 2\n",
    "while((len(candidates_L_1) > 0) & (iteration < 4)):\n",
    "    print(\"Generated: \", iteration)    \n",
    "    candidates_L = []\n",
    "    for i in range(len(candidates_L_1)):\n",
    "        candidate_i = set(candidates_L_1[i][0])\n",
    "        sup_i = candidates_L_1[i][1]\n",
    "        idx_i = candidates_L_1[i][-1]\n",
    "        for j in range(i):\n",
    "            candidate_j = set(candidates_L_1[j][0])\n",
    "            sup_j = candidates_L_1[j][1]\n",
    "            idx_j = candidates_L_1[j][-1]\n",
    "            merged_candidate = sorted(candidate_i.union(candidate_j), key=itemgetter(0))\n",
    "            if json.dumps(merged_candidate) in set_L_1:\n",
    "                continue\n",
    "            if (len(merged_candidate) == iteration + 1):\n",
    "                intersect_candidates = candidate_i.intersection(candidate_j)\n",
    "                setminus_i = list(candidate_i - intersect_candidates)[0].split(\"=\")\n",
    "                setminus_j = list(candidate_j - intersect_candidates)[0].split(\"=\")\n",
    "                attr_i = setminus_i[0]\n",
    "                val_i = int(setminus_i[1])\n",
    "                attr_j = setminus_j[0]\n",
    "                val_j = int(setminus_j[1])\n",
    "                if (attr_i != attr_j):\n",
    "                    # merge to get L list\n",
    "                    idx = idx_i.intersection(idx_j)\n",
    "                    fractionRows = len(idx)/len(X_train) * 100\n",
    "                    isCompact = True\n",
    "                    if (fractionRows == min(sup_i, sup_j)): # pattern is not compact if intersection equals one of its parents\n",
    "                        isCompact = False\n",
    "                    if (fractionRows/100 >= support):\n",
    "                        X = np.delete(X_train, idx, 0)\n",
    "                        y = y_train.drop(index=idx, inplace=False)\n",
    "\n",
    "                        params_f_2 = second_order_group_influence(idx, del_L_del_theta)\n",
    "                        del_f_2 = np.dot(v1.transpose(), params_f_2)\n",
    "\n",
    "                        score = del_f_2 * 100/fractionRows\n",
    "                        if (((score > candidates_L_1[i][2]) & (score > candidates_L_1[j][2])) or \n",
    "                           (fractionRows >= support_small)):\n",
    "                            candidate = [merged_candidate, fractionRows,\n",
    "                                         del_f_2*len(X_train)/len(idx), del_f_2, idx]\n",
    "                            candidates_L.append(candidate)\n",
    "                            set_L_1.add(json.dumps(merged_candidate))\n",
    "                            if (isCompact):\n",
    "                                candidates_all.insert(len(candidates_all), candidate)\n",
    "    set_L_1 = set()\n",
    "    print(\"Generated:\", len(candidates_L), \" \", str(iteration+1), \"-candidates\")\n",
    "    candidates_L_1 = copy.deepcopy(candidates_L)\n",
    "    candidates_L_1.sort()\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:16:27.355479Z",
     "start_time": "2021-11-08T19:16:27.081316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13409\n",
      "13409\n"
     ]
    }
   ],
   "source": [
    "candidates_support_3_compact = copy.deepcopy(candidates_all)\n",
    "print(len(candidates_support_3_compact))\n",
    "candidates_df_3_compact = pd.DataFrame(candidates_support_3_compact, columns=[\"predicates\",\"support\",\"score\",\"2nd-inf\",'idx'])\n",
    "candidates_df_3_compact = candidates_df_3_compact.sort_values(by=['score'], ascending=False)\n",
    "print(len(candidates_df_3_compact))\n",
    "# display(candidates_df_3_compact)\n",
    "# display(candidates_df_3_compact[candidates_df_3_compact[\"support\"] < 20].sort_values(by=['2nd-inf'], ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Containment-based filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:16:33.442596Z",
     "start_time": "2021-11-08T19:16:33.417506Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Topk:\n",
    "    \"\"\"\n",
    "        top explanations: explanation -> (minhash, set_index, score)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='containment', threshold=0.75, k=5):\n",
    "        self.method = method\n",
    "        if method == 'lshensemble':\n",
    "            raise NotImplementedError\n",
    "        elif method == 'lsh':\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.top_explanations = dict()\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "        self.min_score = -100\n",
    "        self.min_score_explanation = None\n",
    "        self.containment_hist = []\n",
    "\n",
    "    def _update_min(self, new_explanation, new_score):\n",
    "        if len(self.top_explanations) > 0:\n",
    "            for explanation, t in self.top_explanations.items():\n",
    "                if t[1] < new_score:\n",
    "                    new_score = t[1]\n",
    "                    new_explanation = explanation\n",
    "        self.min_score = new_score\n",
    "        self.min_score_explanation = new_explanation\n",
    "\n",
    "    def _containment(self, x, q):\n",
    "        c = len(x & q) / len(q)\n",
    "        self.containment_hist.append(c)\n",
    "        return c\n",
    "\n",
    "    def update(self, explanation, score):\n",
    "        if (len(self.top_explanations) < self.k) or (score > self.min_score):\n",
    "            s = get_subset(explanation)\n",
    "            explanation = json.dumps(explanation)\n",
    "\n",
    "            if self.method == 'lshensemble':\n",
    "                raise NotImplementedError\n",
    "            elif self.method == 'lsh':\n",
    "                raise NotImplementedError\n",
    "            elif self.method == 'containment':\n",
    "                q_result = set()\n",
    "                for k, v in self.top_explanations.items():\n",
    "                    if self._containment(v[0], s) > self.threshold:\n",
    "                        q_result.add(k)\n",
    "\n",
    "            if len(q_result) == 0:\n",
    "                if len(self.top_explanations) <= self.k - 1:\n",
    "                    self._update_min(explanation, score)\n",
    "                    self.top_explanations[explanation] = (s, score)\n",
    "                    return 0\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:16:38.088605Z",
     "start_time": "2021-11-08T19:16:37.908148Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "support_threshold = 0.2\n",
    "containment_df = candidates_df_3_compact[candidates_df_3_compact['support'] < support_threshold*100].sort_values(by=['score'],\n",
    "                                                                                              ascending=False).copy()\n",
    "\n",
    "topk = Topk(method='containment', threshold=0.2, k=5)\n",
    "for row_idx in range(len(containment_df)):\n",
    "    row = containment_df.iloc[row_idx]\n",
    "    explanation, score = row[0], row[2]\n",
    "    topk.update(explanation, score)\n",
    "    if len(topk.top_explanations) == topk.k:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:16:41.336188Z",
     "start_time": "2021-11-08T19:16:40.980393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explanations</th>\n",
       "      <th>support</th>\n",
       "      <th>score</th>\n",
       "      <th>gt-score</th>\n",
       "      <th>2nd-inf(%)</th>\n",
       "      <th>gt-inf(%)</th>\n",
       "      <th>new-acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"age=1\", \"gender=0\"]</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.985302</td>\n",
       "      <td>1.075691</td>\n",
       "      <td>0.518117</td>\n",
       "      <td>0.565648</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"age=1\", \"credit_hist=0\", \"gender=1\"]</td>\n",
       "      <td>6.250</td>\n",
       "      <td>0.621814</td>\n",
       "      <td>0.657572</td>\n",
       "      <td>0.408723</td>\n",
       "      <td>0.432227</td>\n",
       "      <td>0.755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"credit_amt=0\", \"install_rate=4\", \"install_plans=0\", \"status=0\"]</td>\n",
       "      <td>6.500</td>\n",
       "      <td>0.419609</td>\n",
       "      <td>0.410323</td>\n",
       "      <td>0.286844</td>\n",
       "      <td>0.280497</td>\n",
       "      <td>0.755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"duration=1\", \"employment=4\", \"num_liable=1\", \"residence=4\"]</td>\n",
       "      <td>5.375</td>\n",
       "      <td>0.405261</td>\n",
       "      <td>0.400088</td>\n",
       "      <td>0.229088</td>\n",
       "      <td>0.226164</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"credit_hist=2\", \"gender=0\", \"housing_A152=1\", \"savings=0\"]</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.393449</td>\n",
       "      <td>0.382634</td>\n",
       "      <td>0.331030</td>\n",
       "      <td>0.321931</td>\n",
       "      <td>0.760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        explanations  support  \\\n",
       "0                                              [\"age=1\", \"gender=0\"]    5.000   \n",
       "1                             [\"age=1\", \"credit_hist=0\", \"gender=1\"]    6.250   \n",
       "2  [\"credit_amt=0\", \"install_rate=4\", \"install_plans=0\", \"status=0\"]    6.500   \n",
       "3      [\"duration=1\", \"employment=4\", \"num_liable=1\", \"residence=4\"]    5.375   \n",
       "4       [\"credit_hist=2\", \"gender=0\", \"housing_A152=1\", \"savings=0\"]    8.000   \n",
       "\n",
       "      score  gt-score  2nd-inf(%)  gt-inf(%)  new-acc  \n",
       "0  0.985302  1.075691    0.518117   0.565648    0.745  \n",
       "1  0.621814  0.657572    0.408723   0.432227    0.755  \n",
       "2  0.419609  0.410323    0.286844   0.280497    0.755  \n",
       "3  0.405261  0.400088    0.229088   0.226164    0.740  \n",
       "4  0.393449  0.382634    0.331030   0.321931    0.760  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations = list(topk.top_explanations.keys())\n",
    "idxs = [v[1] for v in topk.top_explanations.values()]\n",
    "supports = list()\n",
    "scores = list()\n",
    "gt_scores = list()\n",
    "infs = list()\n",
    "gts = list()\n",
    "new_accs = list()\n",
    "for e in explanations:\n",
    "    idx = get_subset(json.loads(e))\n",
    "    X = np.delete(X_train, idx, 0)\n",
    "    y = y_train.drop(index=idx, inplace=False)\n",
    "    clf.fit(np.array(X), np.array(y))\n",
    "    y_pred = clf.predict_proba(np.array(X_test))\n",
    "    new_acc = computeAccuracy(y_test, y_pred)\n",
    "    inf_gt = computeFairness(y_pred, X_test_orig, y_test, 0, dataset) - spd_0\n",
    "    \n",
    "    condition = candidates_df_3_compact.predicates.apply(lambda x: x==json.loads(e))\n",
    "    supports.append(float(candidates_df_3_compact[condition]['support']))\n",
    "    scores.append(float(candidates_df_3_compact[condition]['score']))\n",
    "    infs.append(float(candidates_df_3_compact[condition]['2nd-inf']))\n",
    "    gts.append(inf_gt/(-spd_0))\n",
    "    gt_scores.append(inf_gt*100/float(candidates_df_3_compact[condition]['support']))\n",
    "    new_accs.append(new_acc)\n",
    "\n",
    "\n",
    "expl = [explanations, supports, scores, gt_scores, infs, gts, new_accs]\n",
    "expl = (np.array(expl).T).tolist()\n",
    "\n",
    "explanations = pd.DataFrame(expl, columns=[\"explanations\", \"support\", \"score\", \"gt-score\", \"2nd-inf(%)\", \"gt-inf(%)\", \"new-acc\"])\n",
    "explanations['score'] = explanations['score'].astype(float)\n",
    "explanations['gt-score'] = explanations['gt-score'].astype(float)\n",
    "explanations['support'] = explanations['support'].astype(float)\n",
    "explanations['2nd-inf(%)'] = explanations['2nd-inf(%)'].astype(float)/(-spd_0)\n",
    "explanations['gt-inf(%)'] = explanations['gt-inf(%)'].astype(float)\n",
    "explanations['new-acc'] = explanations['new-acc'].astype(float)\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "explanations.sort_values(by=['score'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
