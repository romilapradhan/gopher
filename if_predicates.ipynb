{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital', 'occupation', 'relationship', 'race', 'gender', 'capgain', 'caploss', 'hours', 'country', 'income']\n",
    "df_train = pd.read_csv('adult.data', names=cols, sep=\",\")\n",
    "df_test = pd.read_csv('adult.test', names=cols, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def one_hot_encode(df):\n",
    "    df.isin(['?']).sum(axis=0)\n",
    "\n",
    "    # replace missing values (?) to nan and then drop the columns\n",
    "    df['country'] = df['country'].replace('?',np.nan)\n",
    "    df['workclass'] = df['workclass'].replace('?',np.nan)\n",
    "    df['occupation'] = df['occupation'].replace('?',np.nan)\n",
    "\n",
    "    # dropping the NaN rows now\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    df['income'] = df['income'].map({'<=50K': 0, '>50K': 1}).astype(int)\n",
    "    df = pd.concat([df, pd.get_dummies(df['gender'], prefix='gender')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['race'], prefix='race')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['marital'], prefix='marital')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['workclass'], prefix='workclass')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['relationship'], prefix='relationship')],axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df['occupation'], prefix='occupation')],axis=1)\n",
    "\n",
    "    df = df.drop(columns=['workclass', 'gender', 'fnlwgt', 'education', 'occupation', \\\n",
    "                      'relationship', 'marital', 'race', 'country', 'capgain', \\\n",
    "                      'caploss'])\n",
    "    return df\n",
    "\n",
    "# one-hot encoding (for regression mdoels)\n",
    "df_train = one_hot_encode(df_train)\n",
    "df_test = one_hot_encode(df_test)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Protected, privileged**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protected: 'gender_Female'=1\n",
    "# privileged: 'gender_Male'=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns='income')\n",
    "y_train = df_train['income']\n",
    "\n",
    "X_test = df_test.drop(columns='income')\n",
    "y_test = df_test['income']\n",
    "\n",
    "size=500\n",
    "X_train = X_train[0:size]\n",
    "y_train = y_train[0:size]\n",
    "\n",
    "X_train_orig = copy.deepcopy(X_train)\n",
    "X_test_orig = copy.deepcopy(X_test)\n",
    "\n",
    "# Scale data: regularization penalty default: ‘l2’, ‘lbfgs’ solvers support only l2 penalties. \n",
    "# Regularization makes the predictor dependent on the scale of the features.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute statistical parity difference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFairness(y_pred, X_test): \n",
    "    protected_idx = X_test[X_test['gender_Female']==1].index\n",
    "    numProtected = len(protected_idx)\n",
    "    privileged_idx = X_test[X_test['gender_Male']==1].index\n",
    "    numPrivileged = len(privileged_idx)\n",
    "    \n",
    "    p_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        p_protected += y_pred[protected_idx[i]][1]\n",
    "    p_protected /= len(protected_idx)\n",
    "    \n",
    "    p_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        p_privileged += y_pred[privileged_idx[i]][1]\n",
    "    p_privileged /= len(privileged_idx)\n",
    "    \n",
    "    spd = p_protected - p_privileged\n",
    "    return spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Influence of points computed using ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_influence(X_train, y_train, X_test, X_test_orig):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    spd_0 = computeFairness(y_pred, X_test_orig)\n",
    "\n",
    "    delta_spd = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_removed = np.delete(X_train, i, 0)\n",
    "        y_removed = y_train.drop(index=i, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        delta_spd_i = computeFairness(y_pred, X_test_orig) - spd_0\n",
    "        delta_spd.append(delta_spd_i)\n",
    "    \n",
    "    return delta_spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function** (Log loss for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if (y_pred[i][1] != 0 and y_pred[i][0] != 0):\n",
    "            loss += - y_true[i] * math.log(y_pred[i][1]) - (1 - y_true[i]) * math.log(y_pred[i][0])\n",
    "    loss /= len(y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Accuracy** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAccuracy(y_true, y_pred):\n",
    "    accuracy = 0\n",
    "    for i in range(len(y_true)):\n",
    "        idx = y_true[i]\n",
    "        accuracy += y_pred[i][idx]\n",
    "    accuracy /= len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of loss function at z with respect to model parameters**\n",
    "\n",
    "(Pre-computed for all training points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_L_del_theta_i(num_params, y_true, x, y_pred):\n",
    "#     del_L_del_theta = np.ones((num_params, 1)) * ((1 - y_true) * y_pred[1] - y_true * y_pred[0])\n",
    "    del_L_del_theta = np.ones((num_params, 1)) * (- y_true + y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_L_del_theta[j] *=  x[j-1]\n",
    "    return del_L_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessian: Second-order partial derivative of loss function with respect to model parameters**\n",
    "\n",
    "(Pre-computed for all training points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_one_point(num_params, x, y_pred):\n",
    "    H = np.ones((num_params, num_params)) * (y_pred[0] * y_pred[1])\n",
    "    for i in range(1, num_params):\n",
    "        for j in range(i + 1):\n",
    "            if j == 0:\n",
    "                H[i][j] *= x[i-1]\n",
    "            else:\n",
    "                H[i][j] *= x[i-1] * x[j-1] \n",
    "    i_lower = np.tril_indices(num_params, -1)\n",
    "    H.T[i_lower] = H[i_lower]     \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of $P(y \\mid \\textbf{x})$ with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_f_del_theta_i(num_params, x, y_pred):\n",
    "    del_f_del_theta = np.ones((num_params, 1)) * (y_pred[0] * y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_f_del_theta[j] *=  x[j-1]\n",
    "    return del_f_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($Statistical parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(SPD)/del(theta)\n",
    "def del_spd_del_theta(num_params, X_test_orig, X_test, y_pred):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    numProtected = X_test_orig['gender_Female'].sum()\n",
    "    numPrivileged = X_test_orig['gender_Male'].sum()\n",
    "    for i in range(len(X_test)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[i], y_pred[i])\n",
    "        if X_test_orig.iloc[i]['gender_Male'] == 1: #privileged\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "        elif X_test_orig.iloc[i]['gender_Female'] == 1:\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_privileged /= numPrivileged\n",
    "    del_f_protected /= numProtected\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic estimation of Hessian vector product (involving del fairness): $H_{\\theta}^{-1}v = H_{\\theta}^{-1}\\nabla_{\\theta}f(z, \\theta) = v + [I - \\nabla_{\\theta}^2L(z_{s_j}, \\theta^*)]H_{\\theta}^{-1}v$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformly sample t points from training data \n",
    "def hessian_vector_product(num_params, n, size, v, hessian_all_points):\n",
    "    if (size > n):\n",
    "        size = n\n",
    "    sample = random.sample(range(n), size)\n",
    "    hinv_v = copy.deepcopy(v)\n",
    "    for idx in range(size):\n",
    "        i = sample[idx]\n",
    "        hessian_i = hessian_all_points[i]\n",
    "        hinv_v = np.matmul(np.subtract(np.identity(num_params), hessian_i), hinv_v)\n",
    "        hinv_v = np.add(hinv_v, v)\n",
    "    return hinv_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order_influence(del_L_del_theta, hinv_v, n):\n",
    "    infs = []\n",
    "    for i in range(n):\n",
    "        inf = -np.dot(del_L_del_theta[i].transpose(), hinv_v)\n",
    "        inf *= -1/n\n",
    "        infs.append(inf[0][0].tolist())\n",
    "    return infs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics: Initial state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial fairness:  -0.17171987868277863\n",
      "Initial loss:  0.40860436582736925\n",
      "Initial accuracy:  0.7721868753956858\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.0001\n",
    "clf.fit(X_train, y_train)\n",
    "num_params = len(clf.coef_.transpose()) + 1 #weights and intercept; params: clf.coef_, clf.intercept_\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "y_pred_train = clf.predict_proba(X_train)\n",
    "    \n",
    "spd_0 = computeFairness(y_pred_test, X_test_orig)\n",
    "print(\"Initial fairness: \", spd_0)\n",
    "\n",
    "loss_0 = logistic_loss(y_test, y_pred_test)\n",
    "print(\"Initial loss: \", loss_0)\n",
    "\n",
    "accuracy_0 = computeAccuracy(y_test, y_pred_test)\n",
    "print(\"Initial accuracy: \", accuracy_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-compute: (1) Hessian (2) del_L_del_theta for each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_L_del_theta = []\n",
    "for i in range(int(len(X_train))):\n",
    "    del_L_del_theta.insert(i, del_L_del_theta_i(num_params, y_train[i], X_train[i], y_pred_train[i]))\n",
    "\n",
    "hessian_all_points = []\n",
    "for i in range(len(X_train)):\n",
    "    hessian_all_points.insert(i, hessian_one_point(num_params, X_train[i], y_pred_train[i])\n",
    "                              /len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*H^{-1} computation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexact = 1\n",
    "v1 = del_spd_del_theta(num_params, X_test_orig, X_test, y_pred_test)\n",
    "# ix = 203\n",
    "# v1 = del_L_del_theta_i(num_params, y_test[ix], X_test[ix], y_pred_test[ix])\n",
    "if hexact == 1: \n",
    "    H_exact = np.zeros((num_params, num_params))\n",
    "    for i in range(len(X_train)):\n",
    "        H_exact = np.add(H_exact, hessian_all_points[i])\n",
    "    hinv_exact = np.linalg.pinv(H_exact) \n",
    "    hinv_v = np.matmul(hinv_exact, v1)\n",
    "else: #using Hessian vector product\n",
    "    size_hvp = int(len(X_train) * .01)\n",
    "    hinv_v = hessian_vector_product(num_params, len(X_train), size_hvp, v1, hessian_all_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ground truth influence of each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth influence\n",
    "# spdgt = ground_truth_influence(X_train, y_train, X_test, X_test_orig)\n",
    "# with open('delta_spd_ground_truth_v0.txt', 'w') as filehandle:\n",
    "#     for listitem in delta_spd:\n",
    "#         filehandle.write('%s\\n' % listitem)\n",
    "gt_spd = pd.read_csv('delta_spd_ground_truth_v0.txt', names=[\"Values\"], sep=\",\")\n",
    "gt_spd = gt_spd.values.tolist()\n",
    "spdgt=[]\n",
    "for i in range(len(gt_spd)):\n",
    "    spdgt.append(gt_spd[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence of each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "infs_1 = first_order_influence(del_L_del_theta, hinv_v, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification of first-order influence (implementation) on log-loss of a single point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "9\n",
      "19\n",
      "20\n",
      "47\n",
      "69\n",
      "72\n",
      "78\n",
      "88\n",
      "95\n",
      "107\n",
      "116\n",
      "123\n",
      "127\n",
      "134\n",
      "141\n",
      "146\n",
      "160\n",
      "165\n",
      "166\n",
      "175\n",
      "182\n",
      "184\n",
      "188\n",
      "190\n",
      "192\n",
      "198\n",
      "203\n",
      "206\n",
      "207\n",
      "210\n",
      "212\n",
      "213\n",
      "214\n",
      "228\n",
      "239\n",
      "244\n",
      "247\n",
      "248\n",
      "257\n",
      "272\n",
      "274\n",
      "276\n",
      "288\n",
      "290\n",
      "295\n",
      "299\n",
      "302\n",
      "304\n",
      "310\n",
      "312\n",
      "317\n",
      "318\n",
      "319\n",
      "324\n",
      "325\n",
      "335\n",
      "338\n",
      "347\n",
      "350\n",
      "352\n",
      "354\n",
      "360\n",
      "366\n",
      "367\n",
      "371\n",
      "378\n",
      "381\n",
      "385\n",
      "390\n",
      "396\n",
      "398\n",
      "400\n",
      "408\n",
      "429\n",
      "433\n",
      "443\n",
      "455\n",
      "460\n",
      "468\n",
      "475\n",
      "478\n",
      "481\n",
      "483\n",
      "484\n",
      "486\n",
      "491\n",
      "492\n",
      "494\n",
      "495\n",
      "499\n",
      "503\n",
      "506\n",
      "509\n",
      "510\n",
      "512\n",
      "515\n",
      "526\n",
      "531\n",
      "535\n",
      "536\n",
      "538\n",
      "540\n",
      "547\n",
      "553\n",
      "554\n",
      "559\n",
      "562\n",
      "600\n",
      "602\n",
      "607\n",
      "614\n",
      "634\n",
      "644\n",
      "645\n",
      "651\n",
      "656\n",
      "658\n",
      "662\n",
      "676\n",
      "677\n",
      "687\n",
      "691\n",
      "694\n",
      "698\n",
      "701\n",
      "702\n",
      "713\n",
      "724\n",
      "725\n",
      "736\n",
      "739\n",
      "740\n",
      "744\n",
      "746\n",
      "747\n",
      "750\n",
      "764\n",
      "766\n",
      "768\n",
      "769\n",
      "782\n",
      "783\n",
      "791\n",
      "801\n",
      "803\n",
      "807\n",
      "809\n",
      "817\n",
      "818\n",
      "820\n",
      "822\n",
      "823\n",
      "827\n",
      "830\n",
      "838\n",
      "841\n",
      "846\n",
      "849\n",
      "851\n",
      "853\n",
      "861\n",
      "863\n",
      "870\n",
      "872\n",
      "878\n",
      "881\n",
      "883\n",
      "886\n",
      "887\n",
      "903\n",
      "905\n",
      "906\n",
      "907\n",
      "910\n",
      "911\n",
      "913\n",
      "917\n",
      "918\n",
      "920\n",
      "921\n",
      "927\n",
      "941\n",
      "951\n",
      "952\n",
      "956\n",
      "960\n",
      "971\n",
      "982\n",
      "983\n",
      "984\n",
      "1005\n",
      "1006\n",
      "1012\n",
      "1017\n",
      "1018\n",
      "1021\n",
      "1026\n",
      "1027\n",
      "1029\n",
      "1031\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1040\n",
      "1045\n",
      "1054\n",
      "1063\n",
      "1069\n",
      "1088\n",
      "1089\n",
      "1091\n",
      "1094\n",
      "1105\n",
      "1111\n",
      "1116\n",
      "1119\n",
      "1123\n",
      "1126\n",
      "1127\n",
      "1138\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1145\n",
      "1149\n",
      "1154\n",
      "1162\n",
      "1164\n",
      "1167\n",
      "1171\n",
      "1178\n",
      "1182\n",
      "1206\n",
      "1213\n",
      "1218\n",
      "1219\n",
      "1227\n",
      "1230\n",
      "1234\n",
      "1237\n",
      "1238\n",
      "1243\n",
      "1254\n",
      "1261\n",
      "1267\n",
      "1273\n",
      "1277\n",
      "1280\n",
      "1291\n",
      "1292\n",
      "1302\n",
      "1303\n",
      "1308\n",
      "1310\n",
      "1327\n",
      "1330\n",
      "1331\n",
      "1336\n",
      "1338\n",
      "1344\n",
      "1359\n",
      "1366\n",
      "1369\n",
      "1373\n",
      "1374\n",
      "1385\n",
      "1394\n",
      "1399\n",
      "1403\n",
      "1419\n",
      "1434\n",
      "1445\n",
      "1449\n",
      "1476\n",
      "1489\n",
      "1490\n",
      "1522\n",
      "1533\n",
      "1538\n",
      "1547\n",
      "1550\n",
      "1551\n",
      "1559\n",
      "1562\n",
      "1578\n",
      "1585\n",
      "1589\n",
      "1593\n",
      "1595\n",
      "1596\n",
      "1600\n",
      "1608\n",
      "1613\n",
      "1615\n",
      "1627\n",
      "1633\n",
      "1636\n",
      "1648\n",
      "1649\n",
      "1656\n",
      "1666\n",
      "1675\n",
      "1677\n",
      "1684\n",
      "1685\n",
      "1713\n",
      "1723\n",
      "1724\n",
      "1730\n",
      "1731\n",
      "1748\n",
      "1750\n",
      "1752\n",
      "1763\n",
      "1768\n",
      "1769\n",
      "1778\n",
      "1781\n",
      "1787\n",
      "1788\n",
      "1790\n",
      "1804\n",
      "1815\n",
      "1817\n",
      "1822\n",
      "1823\n",
      "1825\n",
      "1830\n",
      "1838\n",
      "1841\n",
      "1847\n",
      "1849\n",
      "1858\n",
      "1865\n",
      "1877\n",
      "1880\n",
      "1883\n",
      "1888\n",
      "1890\n",
      "1896\n",
      "1916\n",
      "1917\n",
      "1920\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1939\n",
      "1957\n",
      "1966\n",
      "1975\n",
      "1984\n",
      "1986\n",
      "1992\n",
      "2002\n",
      "2012\n",
      "2025\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2046\n",
      "2050\n",
      "2051\n",
      "2055\n",
      "2060\n",
      "2063\n",
      "2067\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2081\n",
      "2092\n",
      "2098\n",
      "2100\n",
      "2106\n",
      "2111\n",
      "2115\n",
      "2123\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2146\n",
      "2150\n",
      "2152\n",
      "2157\n",
      "2161\n",
      "2164\n",
      "2166\n",
      "2173\n",
      "2175\n",
      "2177\n",
      "2187\n",
      "2191\n",
      "2192\n",
      "2196\n",
      "2206\n",
      "2209\n",
      "2210\n",
      "2215\n",
      "2220\n",
      "2221\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2250\n",
      "2260\n",
      "2268\n",
      "2272\n",
      "2278\n",
      "2280\n",
      "2283\n",
      "2285\n",
      "2286\n",
      "2306\n",
      "2310\n",
      "2324\n",
      "2328\n",
      "2340\n",
      "2349\n",
      "2352\n",
      "2354\n",
      "2356\n",
      "2364\n",
      "2372\n",
      "2373\n",
      "2377\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2393\n",
      "2397\n",
      "2412\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2430\n",
      "2434\n",
      "2435\n",
      "2444\n",
      "2445\n",
      "2452\n",
      "2457\n",
      "2459\n",
      "2464\n",
      "2466\n",
      "2467\n",
      "2478\n",
      "2488\n",
      "2491\n",
      "2504\n",
      "2505\n",
      "2516\n",
      "2525\n",
      "2529\n",
      "2531\n",
      "2534\n",
      "2536\n",
      "2537\n",
      "2540\n",
      "2548\n",
      "2551\n",
      "2552\n",
      "2564\n",
      "2570\n",
      "2577\n",
      "2578\n",
      "2583\n",
      "2584\n",
      "2588\n",
      "2592\n",
      "2595\n",
      "2599\n",
      "2606\n",
      "2608\n",
      "2620\n",
      "2626\n",
      "2630\n",
      "2639\n",
      "2649\n",
      "2650\n",
      "2657\n",
      "2667\n",
      "2668\n",
      "2679\n",
      "2680\n",
      "2682\n",
      "2684\n",
      "2691\n",
      "2694\n",
      "2695\n",
      "2700\n",
      "2719\n",
      "2721\n",
      "2723\n",
      "2736\n",
      "2739\n",
      "2741\n",
      "2750\n",
      "2758\n",
      "2759\n",
      "2763\n",
      "2766\n",
      "2789\n",
      "2792\n",
      "2802\n",
      "2803\n",
      "2816\n",
      "2817\n",
      "2825\n",
      "2828\n",
      "2842\n",
      "2844\n",
      "2848\n",
      "2864\n",
      "2877\n",
      "2887\n",
      "2902\n",
      "2908\n",
      "2918\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2930\n",
      "2935\n",
      "2937\n",
      "2946\n",
      "2948\n",
      "2953\n",
      "2970\n",
      "2978\n",
      "2979\n",
      "2991\n",
      "2993\n",
      "2997\n",
      "2998\n",
      "3003\n",
      "3014\n",
      "3019\n",
      "3021\n",
      "3037\n",
      "3044\n",
      "3046\n",
      "3058\n",
      "3066\n",
      "3069\n",
      "3071\n",
      "3072\n",
      "3080\n",
      "3087\n",
      "3094\n",
      "3098\n",
      "3099\n",
      "3106\n",
      "3110\n",
      "3124\n",
      "3135\n",
      "3138\n",
      "3139\n",
      "3148\n",
      "3151\n",
      "3152\n",
      "3166\n",
      "3168\n",
      "3171\n",
      "3185\n",
      "3192\n",
      "3193\n",
      "3195\n",
      "3198\n",
      "3202\n",
      "3208\n",
      "3215\n",
      "3226\n",
      "3228\n",
      "3245\n",
      "3252\n",
      "3254\n",
      "3265\n",
      "3269\n",
      "3271\n",
      "3275\n",
      "3279\n",
      "3282\n",
      "3290\n",
      "3300\n",
      "3304\n",
      "3305\n",
      "3307\n",
      "3308\n",
      "3310\n",
      "3337\n",
      "3340\n",
      "3345\n",
      "3348\n",
      "3350\n",
      "3351\n",
      "3354\n",
      "3360\n",
      "3371\n",
      "3376\n",
      "3379\n",
      "3383\n",
      "3387\n",
      "3395\n",
      "3411\n",
      "3416\n",
      "3417\n",
      "3418\n",
      "3421\n",
      "3423\n",
      "3424\n",
      "3436\n",
      "3441\n",
      "3442\n",
      "3446\n",
      "3447\n",
      "3454\n",
      "3458\n",
      "3460\n",
      "3465\n",
      "3473\n",
      "3485\n",
      "3486\n",
      "3489\n",
      "3506\n",
      "3507\n",
      "3518\n",
      "3519\n",
      "3521\n",
      "3527\n",
      "3536\n",
      "3537\n",
      "3540\n",
      "3548\n",
      "3555\n",
      "3560\n",
      "3563\n",
      "3569\n",
      "3574\n",
      "3578\n",
      "3579\n",
      "3586\n",
      "3590\n",
      "3591\n",
      "3594\n",
      "3599\n",
      "3612\n",
      "3616\n",
      "3621\n",
      "3623\n",
      "3637\n",
      "3642\n",
      "3645\n",
      "3646\n",
      "3647\n",
      "3656\n",
      "3665\n",
      "3675\n",
      "3677\n",
      "3678\n",
      "3684\n",
      "3687\n",
      "3689\n",
      "3695\n",
      "3702\n",
      "3708\n",
      "3715\n",
      "3722\n",
      "3729\n",
      "3735\n",
      "3736\n",
      "3740\n",
      "3746\n",
      "3749\n",
      "3750\n",
      "3753\n",
      "3756\n",
      "3770\n",
      "3773\n",
      "3793\n",
      "3794\n",
      "3795\n",
      "3811\n",
      "3835\n",
      "3836\n",
      "3851\n",
      "3868\n",
      "3879\n",
      "3895\n",
      "3920\n",
      "3927\n",
      "3934\n",
      "3947\n",
      "3954\n",
      "3956\n",
      "3958\n",
      "3960\n",
      "3961\n",
      "3964\n",
      "3969\n",
      "3974\n",
      "3982\n",
      "3986\n",
      "3995\n",
      "3998\n",
      "4008\n",
      "4016\n",
      "4021\n",
      "4025\n",
      "4029\n",
      "4035\n",
      "4036\n",
      "4058\n",
      "4060\n",
      "4064\n",
      "4069\n",
      "4085\n",
      "4086\n",
      "4093\n",
      "4096\n",
      "4100\n",
      "4103\n",
      "4105\n",
      "4114\n",
      "4122\n",
      "4123\n",
      "4132\n",
      "4139\n",
      "4150\n",
      "4167\n",
      "4188\n",
      "4200\n",
      "4207\n",
      "4208\n",
      "4220\n",
      "4224\n",
      "4227\n",
      "4232\n",
      "4237\n",
      "4249\n",
      "4251\n",
      "4257\n",
      "4264\n",
      "4268\n",
      "4269\n",
      "4275\n",
      "4284\n",
      "4289\n",
      "4294\n",
      "4305\n",
      "4307\n",
      "4309\n",
      "4314\n",
      "4315\n",
      "4328\n",
      "4339\n",
      "4358\n",
      "4364\n",
      "4370\n",
      "4376\n",
      "4381\n",
      "4385\n",
      "4388\n",
      "4408\n",
      "4417\n",
      "4418\n",
      "4423\n",
      "4424\n",
      "4429\n",
      "4430\n",
      "4441\n",
      "4442\n",
      "4446\n",
      "4451\n",
      "4452\n",
      "4464\n",
      "4466\n",
      "4468\n",
      "4469\n",
      "4473\n",
      "4474\n",
      "4476\n",
      "4477\n",
      "4482\n",
      "4489\n",
      "4490\n",
      "4493\n",
      "4499\n",
      "4501\n",
      "4502\n",
      "4509\n",
      "4510\n",
      "4511\n",
      "4513\n",
      "4520\n",
      "4521\n",
      "4528\n",
      "4542\n",
      "4546\n",
      "4547\n",
      "4559\n",
      "4560\n",
      "4565\n",
      "4566\n",
      "4569\n",
      "4573\n",
      "4576\n",
      "4593\n",
      "4607\n",
      "4609\n",
      "4615\n",
      "4632\n",
      "4641\n",
      "4642\n",
      "4652\n",
      "4657\n",
      "4660\n",
      "4667\n",
      "4668\n",
      "4670\n",
      "4671\n",
      "4673\n",
      "4680\n",
      "4681\n",
      "4682\n",
      "4688\n",
      "4691\n",
      "4692\n",
      "4693\n",
      "4694\n",
      "4700\n",
      "4703\n",
      "4704\n",
      "4705\n",
      "4707\n",
      "4722\n",
      "4731\n",
      "4732\n",
      "4736\n",
      "4737\n",
      "4739\n",
      "4743\n",
      "4753\n",
      "4762\n",
      "4764\n",
      "4765\n",
      "4775\n",
      "4777\n",
      "4781\n",
      "4784\n",
      "4786\n",
      "4788\n",
      "4790\n",
      "4793\n",
      "4798\n",
      "4802\n",
      "4803\n",
      "4811\n",
      "4814\n",
      "4823\n",
      "4826\n",
      "4827\n",
      "4831\n",
      "4840\n",
      "4842\n",
      "4851\n",
      "4852\n",
      "4853\n",
      "4865\n",
      "4873\n",
      "4874\n",
      "4875\n",
      "4881\n",
      "4908\n",
      "4909\n",
      "4911\n",
      "4912\n",
      "4924\n",
      "4947\n",
      "4953\n",
      "4955\n",
      "4957\n",
      "4962\n",
      "4963\n",
      "4977\n",
      "4985\n",
      "4987\n",
      "4989\n",
      "4991\n",
      "5004\n",
      "5007\n",
      "5009\n",
      "5010\n",
      "5012\n",
      "5015\n",
      "5019\n",
      "5023\n",
      "5029\n",
      "5045\n",
      "5050\n",
      "5051\n",
      "5052\n",
      "5053\n",
      "5057\n",
      "5073\n",
      "5074\n",
      "5079\n",
      "5086\n",
      "5090\n",
      "5092\n",
      "5097\n",
      "5098\n",
      "5102\n",
      "5104\n",
      "5107\n",
      "5113\n",
      "5115\n",
      "5128\n",
      "5131\n",
      "5139\n",
      "5140\n",
      "5151\n",
      "5154\n",
      "5157\n",
      "5159\n",
      "5172\n",
      "5176\n",
      "5178\n",
      "5184\n",
      "5185\n",
      "5190\n",
      "5204\n",
      "5211\n",
      "5215\n",
      "5225\n",
      "5231\n",
      "5233\n",
      "5244\n",
      "5252\n",
      "5257\n",
      "5260\n",
      "5261\n",
      "5262\n",
      "5270\n",
      "5275\n",
      "5280\n",
      "5288\n",
      "5290\n",
      "5297\n",
      "5299\n",
      "5301\n",
      "5308\n",
      "5310\n",
      "5318\n",
      "5319\n",
      "5333\n",
      "5351\n",
      "5352\n",
      "5353\n",
      "5354\n",
      "5358\n",
      "5369\n",
      "5393\n",
      "5396\n",
      "5398\n",
      "5400\n",
      "5404\n",
      "5413\n",
      "5420\n",
      "5427\n",
      "5429\n",
      "5447\n",
      "5448\n",
      "5460\n",
      "5465\n",
      "5489\n",
      "5490\n",
      "5492\n",
      "5496\n",
      "5509\n",
      "5512\n",
      "5515\n",
      "5517\n",
      "5536\n",
      "5549\n",
      "5554\n",
      "5558\n",
      "5568\n",
      "5569\n",
      "5579\n",
      "5580\n",
      "5589\n",
      "5601\n",
      "5602\n",
      "5612\n",
      "5620\n",
      "5624\n",
      "5625\n",
      "5632\n",
      "5642\n",
      "5649\n",
      "5650\n",
      "5657\n",
      "5664\n",
      "5666\n",
      "5669\n",
      "5672\n",
      "5675\n",
      "5686\n",
      "5690\n",
      "5692\n",
      "5693\n",
      "5698\n",
      "5709\n",
      "5713\n",
      "5718\n",
      "5720\n",
      "5721\n",
      "5729\n",
      "5730\n",
      "5739\n",
      "5740\n",
      "5754\n",
      "5758\n",
      "5760\n",
      "5762\n",
      "5763\n",
      "5764\n",
      "5765\n",
      "5805\n",
      "5808\n",
      "5815\n",
      "5818\n",
      "5826\n",
      "5831\n",
      "5834\n",
      "5845\n",
      "5847\n",
      "5850\n",
      "5854\n",
      "5859\n",
      "5860\n",
      "5861\n",
      "5866\n",
      "5870\n",
      "5872\n",
      "5879\n",
      "5881\n",
      "5891\n",
      "5892\n",
      "5903\n",
      "5914\n",
      "5921\n",
      "5922\n",
      "5929\n",
      "5934\n",
      "5940\n",
      "5946\n",
      "5957\n",
      "5960\n",
      "5965\n",
      "5968\n",
      "5973\n",
      "5977\n",
      "5982\n",
      "5984\n",
      "5988\n",
      "5994\n",
      "5997\n",
      "6005\n",
      "6011\n",
      "6013\n",
      "6036\n",
      "6040\n",
      "6041\n",
      "6042\n",
      "6062\n",
      "6063\n",
      "6070\n",
      "6087\n",
      "6096\n",
      "6098\n",
      "6099\n",
      "6106\n",
      "6107\n",
      "6115\n",
      "6124\n",
      "6125\n",
      "6126\n",
      "6128\n",
      "6133\n",
      "6139\n",
      "6141\n",
      "6143\n",
      "6146\n",
      "6151\n",
      "6153\n",
      "6160\n",
      "6173\n",
      "6174\n",
      "6183\n",
      "6186\n",
      "6189\n",
      "6192\n",
      "6194\n",
      "6200\n",
      "6204\n",
      "6220\n",
      "6234\n",
      "6236\n",
      "6247\n",
      "6248\n",
      "6252\n",
      "6259\n",
      "6260\n",
      "6272\n",
      "6276\n",
      "6278\n",
      "6279\n",
      "6290\n",
      "6301\n",
      "6304\n",
      "6318\n",
      "6331\n",
      "6335\n",
      "6340\n",
      "6341\n",
      "6344\n",
      "6355\n",
      "6358\n",
      "6360\n",
      "6364\n",
      "6373\n",
      "6375\n",
      "6378\n",
      "6380\n",
      "6381\n",
      "6383\n",
      "6387\n",
      "6395\n",
      "6396\n",
      "6397\n",
      "6399\n",
      "6405\n",
      "6409\n",
      "6410\n",
      "6414\n",
      "6417\n",
      "6428\n",
      "6432\n",
      "6440\n",
      "6441\n",
      "6449\n",
      "6451\n",
      "6457\n",
      "6465\n",
      "6466\n",
      "6474\n",
      "6489\n",
      "6490\n",
      "6493\n",
      "6500\n",
      "6509\n",
      "6526\n",
      "6528\n",
      "6533\n",
      "6535\n",
      "6541\n",
      "6549\n",
      "6552\n",
      "6572\n",
      "6575\n",
      "6577\n",
      "6596\n",
      "6601\n",
      "6616\n",
      "6618\n",
      "6630\n",
      "6631\n",
      "6634\n",
      "6649\n",
      "6666\n",
      "6672\n",
      "6673\n",
      "6677\n",
      "6680\n",
      "6682\n",
      "6698\n",
      "6706\n",
      "6709\n",
      "6723\n",
      "6728\n",
      "6746\n",
      "6747\n",
      "6752\n",
      "6759\n",
      "6766\n",
      "6767\n",
      "6771\n",
      "6796\n",
      "6798\n",
      "6801\n",
      "6805\n",
      "6806\n",
      "6808\n",
      "6811\n",
      "6812\n",
      "6819\n",
      "6823\n",
      "6826\n",
      "6827\n",
      "6830\n",
      "6836\n",
      "6845\n",
      "6847\n",
      "6855\n",
      "6857\n",
      "6861\n",
      "6866\n",
      "6868\n",
      "6869\n",
      "6870\n",
      "6880\n",
      "6890\n",
      "6894\n",
      "6908\n",
      "6909\n",
      "6913\n",
      "6920\n",
      "6922\n",
      "6928\n",
      "6931\n",
      "6932\n",
      "6946\n",
      "6961\n",
      "6962\n",
      "6965\n",
      "6967\n",
      "6978\n",
      "6980\n",
      "6984\n",
      "6998\n",
      "7009\n",
      "7014\n",
      "7022\n",
      "7023\n",
      "7025\n",
      "7028\n",
      "7043\n",
      "7044\n",
      "7051\n",
      "7061\n",
      "7064\n",
      "7069\n",
      "7071\n",
      "7073\n",
      "7076\n",
      "7078\n",
      "7079\n",
      "7083\n",
      "7092\n",
      "7095\n",
      "7105\n",
      "7125\n",
      "7141\n",
      "7143\n",
      "7144\n",
      "7146\n",
      "7164\n",
      "7165\n",
      "7167\n",
      "7172\n",
      "7176\n",
      "7180\n",
      "7182\n",
      "7215\n",
      "7218\n",
      "7220\n",
      "7221\n",
      "7229\n",
      "7231\n",
      "7240\n",
      "7245\n",
      "7246\n",
      "7247\n",
      "7256\n",
      "7261\n",
      "7264\n",
      "7270\n",
      "7271\n",
      "7274\n",
      "7282\n",
      "7287\n",
      "7295\n",
      "7299\n",
      "7302\n",
      "7303\n",
      "7305\n",
      "7307\n",
      "7312\n",
      "7315\n",
      "7319\n",
      "7329\n",
      "7330\n",
      "7347\n",
      "7350\n",
      "7353\n",
      "7355\n",
      "7356\n",
      "7359\n",
      "7362\n",
      "7370\n",
      "7371\n",
      "7373\n",
      "7374\n",
      "7381\n",
      "7391\n",
      "7407\n",
      "7409\n",
      "7412\n",
      "7419\n",
      "7424\n",
      "7425\n",
      "7430\n",
      "7434\n",
      "7437\n",
      "7444\n",
      "7449\n",
      "7456\n",
      "7458\n",
      "7464\n",
      "7465\n",
      "7466\n",
      "7477\n",
      "7485\n",
      "7490\n",
      "7515\n",
      "7522\n",
      "7527\n",
      "7529\n",
      "7531\n",
      "7532\n",
      "7542\n",
      "7551\n",
      "7568\n",
      "7569\n",
      "7577\n",
      "7581\n",
      "7589\n",
      "7599\n",
      "7602\n",
      "7604\n",
      "7607\n",
      "7612\n",
      "7625\n",
      "7626\n",
      "7627\n",
      "7628\n",
      "7631\n",
      "7637\n",
      "7638\n",
      "7642\n",
      "7647\n",
      "7653\n",
      "7655\n",
      "7661\n",
      "7662\n",
      "7664\n",
      "7666\n",
      "7672\n",
      "7678\n",
      "7683\n",
      "7684\n",
      "7686\n",
      "7694\n",
      "7699\n",
      "7704\n",
      "7705\n",
      "7707\n",
      "7709\n",
      "7712\n",
      "7729\n",
      "7730\n",
      "7731\n",
      "7735\n",
      "7736\n",
      "7737\n",
      "7739\n",
      "7753\n",
      "7756\n",
      "7758\n",
      "7759\n",
      "7760\n",
      "7761\n",
      "7768\n",
      "7771\n",
      "7781\n",
      "7788\n",
      "7794\n",
      "7795\n",
      "7798\n",
      "7809\n",
      "7812\n",
      "7817\n",
      "7819\n",
      "7821\n",
      "7823\n",
      "7827\n",
      "7839\n",
      "7840\n",
      "7849\n",
      "7861\n",
      "7863\n",
      "7873\n",
      "7878\n",
      "7879\n",
      "7898\n",
      "7909\n",
      "7910\n",
      "7920\n",
      "7922\n",
      "7933\n",
      "7948\n",
      "7954\n",
      "7958\n",
      "7959\n",
      "7962\n",
      "7963\n",
      "7969\n",
      "7973\n",
      "7974\n",
      "7987\n",
      "7988\n",
      "7991\n",
      "7994\n",
      "8003\n",
      "8014\n",
      "8016\n",
      "8032\n",
      "8034\n",
      "8035\n",
      "8038\n",
      "8041\n",
      "8046\n",
      "8049\n",
      "8052\n",
      "8076\n",
      "8085\n",
      "8102\n",
      "8107\n",
      "8108\n",
      "8113\n",
      "8116\n",
      "8117\n",
      "8119\n",
      "8136\n",
      "8147\n",
      "8150\n",
      "8160\n",
      "8173\n",
      "8179\n",
      "8187\n",
      "8188\n",
      "8198\n",
      "8199\n",
      "8200\n",
      "8201\n",
      "8205\n",
      "8206\n",
      "8208\n",
      "8211\n",
      "8214\n",
      "8217\n",
      "8222\n",
      "8223\n",
      "8224\n",
      "8231\n",
      "8243\n",
      "8249\n",
      "8250\n",
      "8269\n",
      "8271\n",
      "8273\n",
      "8275\n",
      "8283\n",
      "8285\n",
      "8287\n",
      "8289\n",
      "8300\n",
      "8303\n",
      "8305\n",
      "8308\n",
      "8309\n",
      "8314\n",
      "8318\n",
      "8321\n",
      "8323\n",
      "8324\n",
      "8325\n",
      "8331\n",
      "8334\n",
      "8344\n",
      "8349\n",
      "8353\n",
      "8354\n",
      "8356\n",
      "8359\n",
      "8363\n",
      "8366\n",
      "8372\n",
      "8386\n",
      "8396\n",
      "8404\n",
      "8408\n",
      "8410\n",
      "8419\n",
      "8421\n",
      "8427\n",
      "8430\n",
      "8431\n",
      "8441\n",
      "8443\n",
      "8462\n",
      "8466\n",
      "8488\n",
      "8495\n",
      "8496\n",
      "8500\n",
      "8505\n",
      "8507\n",
      "8509\n",
      "8510\n",
      "8518\n",
      "8522\n",
      "8528\n",
      "8531\n",
      "8535\n",
      "8541\n",
      "8559\n",
      "8560\n",
      "8569\n",
      "8592\n",
      "8594\n",
      "8602\n",
      "8612\n",
      "8613\n",
      "8618\n",
      "8624\n",
      "8632\n",
      "8644\n",
      "8648\n",
      "8664\n",
      "8666\n",
      "8673\n",
      "8674\n",
      "8677\n",
      "8678\n",
      "8680\n",
      "8682\n",
      "8685\n",
      "8686\n",
      "8687\n",
      "8690\n",
      "8702\n",
      "8708\n",
      "8711\n",
      "8718\n",
      "8720\n",
      "8721\n",
      "8722\n",
      "8724\n",
      "8728\n",
      "8731\n",
      "8734\n",
      "8736\n",
      "8740\n",
      "8741\n",
      "8746\n",
      "8747\n",
      "8752\n",
      "8756\n",
      "8761\n",
      "8763\n",
      "8766\n",
      "8779\n",
      "8793\n",
      "8797\n",
      "8806\n",
      "8815\n",
      "8820\n",
      "8821\n",
      "8834\n",
      "8852\n",
      "8868\n",
      "8871\n",
      "8875\n",
      "8876\n",
      "8878\n",
      "8879\n",
      "8886\n",
      "8887\n",
      "8888\n",
      "8892\n",
      "8895\n",
      "8900\n",
      "8908\n",
      "8910\n",
      "8912\n",
      "8917\n",
      "8924\n",
      "8928\n",
      "8938\n",
      "8943\n",
      "8945\n",
      "8946\n",
      "8949\n",
      "8951\n",
      "8956\n",
      "8967\n",
      "8970\n",
      "8975\n",
      "8979\n",
      "8983\n",
      "8989\n",
      "8991\n",
      "8993\n",
      "9001\n",
      "9007\n",
      "9013\n",
      "9016\n",
      "9018\n",
      "9020\n",
      "9029\n",
      "9035\n",
      "9039\n",
      "9042\n",
      "9043\n",
      "9044\n",
      "9051\n",
      "9060\n",
      "9078\n",
      "9091\n",
      "9094\n",
      "9097\n",
      "9112\n",
      "9116\n",
      "9120\n",
      "9122\n",
      "9134\n",
      "9136\n",
      "9139\n",
      "9143\n",
      "9151\n",
      "9156\n",
      "9159\n",
      "9179\n",
      "9182\n",
      "9184\n",
      "9188\n",
      "9195\n",
      "9201\n",
      "9203\n",
      "9208\n",
      "9209\n",
      "9215\n",
      "9220\n",
      "9236\n",
      "9237\n",
      "9240\n",
      "9248\n",
      "9258\n",
      "9262\n",
      "9268\n",
      "9272\n",
      "9278\n",
      "9285\n",
      "9293\n",
      "9300\n",
      "9308\n",
      "9310\n",
      "9320\n",
      "9332\n",
      "9346\n",
      "9349\n",
      "9351\n",
      "9352\n",
      "9355\n",
      "9356\n",
      "9364\n",
      "9371\n",
      "9377\n",
      "9378\n",
      "9379\n",
      "9399\n",
      "9403\n",
      "9405\n",
      "9407\n",
      "9415\n",
      "9426\n",
      "9427\n",
      "9445\n",
      "9450\n",
      "9452\n",
      "9469\n",
      "9475\n",
      "9478\n",
      "9479\n",
      "9480\n",
      "9488\n",
      "9489\n",
      "9493\n",
      "9498\n",
      "9499\n",
      "9508\n",
      "9511\n",
      "9542\n",
      "9548\n",
      "9553\n",
      "9558\n",
      "9569\n",
      "9572\n",
      "9573\n",
      "9578\n",
      "9587\n",
      "9588\n",
      "9590\n",
      "9596\n",
      "9601\n",
      "9607\n",
      "9617\n",
      "9622\n",
      "9626\n",
      "9643\n",
      "9645\n",
      "9647\n",
      "9650\n",
      "9653\n",
      "9656\n",
      "9665\n",
      "9668\n",
      "9675\n",
      "9687\n",
      "9689\n",
      "9695\n",
      "9698\n",
      "9699\n",
      "9704\n",
      "9714\n",
      "9717\n",
      "9720\n",
      "9725\n",
      "9726\n",
      "9731\n",
      "9733\n",
      "9736\n",
      "9738\n",
      "9740\n",
      "9743\n",
      "9744\n",
      "9748\n",
      "9749\n",
      "9753\n",
      "9755\n",
      "9758\n",
      "9760\n",
      "9769\n",
      "9779\n",
      "9786\n",
      "9795\n",
      "9810\n",
      "9817\n",
      "9826\n",
      "9836\n",
      "9837\n",
      "9839\n",
      "9846\n",
      "9857\n",
      "9866\n",
      "9869\n",
      "9873\n",
      "9878\n",
      "9891\n",
      "9896\n",
      "9901\n",
      "9905\n",
      "9913\n",
      "9917\n",
      "9921\n",
      "9922\n",
      "9932\n",
      "9934\n",
      "9936\n",
      "9944\n",
      "9953\n",
      "9961\n",
      "9975\n",
      "9980\n",
      "9981\n",
      "9986\n",
      "9987\n",
      "9994\n",
      "9995\n",
      "9998\n",
      "10003\n",
      "10008\n",
      "10014\n",
      "10016\n",
      "10025\n",
      "10027\n",
      "10037\n",
      "10039\n",
      "10040\n",
      "10046\n",
      "10056\n",
      "10057\n",
      "10061\n",
      "10082\n",
      "10091\n",
      "10095\n",
      "10105\n",
      "10107\n",
      "10115\n",
      "10120\n",
      "10127\n",
      "10128\n",
      "10133\n",
      "10135\n",
      "10139\n",
      "10140\n",
      "10153\n",
      "10155\n",
      "10166\n",
      "10176\n",
      "10181\n",
      "10185\n",
      "10187\n",
      "10204\n",
      "10206\n",
      "10214\n",
      "10215\n",
      "10218\n",
      "10229\n",
      "10236\n",
      "10247\n",
      "10249\n",
      "10258\n",
      "10265\n",
      "10266\n",
      "10274\n",
      "10275\n",
      "10279\n",
      "10280\n",
      "10292\n",
      "10296\n",
      "10309\n",
      "10310\n",
      "10315\n",
      "10318\n",
      "10321\n",
      "10340\n",
      "10343\n",
      "10348\n",
      "10351\n",
      "10354\n",
      "10355\n",
      "10359\n",
      "10362\n",
      "10364\n",
      "10365\n",
      "10370\n",
      "10381\n",
      "10382\n",
      "10389\n",
      "10391\n",
      "10399\n",
      "10409\n",
      "10417\n",
      "10419\n",
      "10420\n",
      "10425\n",
      "10447\n",
      "10456\n",
      "10462\n",
      "10464\n",
      "10465\n",
      "10469\n",
      "10470\n",
      "10472\n",
      "10475\n",
      "10488\n",
      "10494\n",
      "10505\n",
      "10513\n",
      "10515\n",
      "10522\n",
      "10524\n",
      "10526\n",
      "10532\n",
      "10547\n",
      "10550\n",
      "10554\n",
      "10562\n",
      "10565\n",
      "10573\n",
      "10575\n",
      "10595\n",
      "10606\n",
      "10616\n",
      "10617\n",
      "10620\n",
      "10622\n",
      "10626\n",
      "10631\n",
      "10637\n",
      "10656\n",
      "10661\n",
      "10665\n",
      "10672\n",
      "10677\n",
      "10687\n",
      "10688\n",
      "10690\n",
      "10691\n",
      "10692\n",
      "10697\n",
      "10698\n",
      "10701\n",
      "10717\n",
      "10722\n",
      "10723\n",
      "10727\n",
      "10729\n",
      "10731\n",
      "10738\n",
      "10742\n",
      "10743\n",
      "10751\n",
      "10755\n",
      "10759\n",
      "10763\n",
      "10767\n",
      "10774\n",
      "10775\n",
      "10785\n",
      "10800\n",
      "10807\n",
      "10819\n",
      "10827\n",
      "10838\n",
      "10839\n",
      "10840\n",
      "10861\n",
      "10874\n",
      "10878\n",
      "10891\n",
      "10902\n",
      "10905\n",
      "10918\n",
      "10919\n",
      "10921\n",
      "10925\n",
      "10926\n",
      "10944\n",
      "10947\n",
      "10949\n",
      "10951\n",
      "10960\n",
      "10968\n",
      "10975\n",
      "10977\n",
      "10981\n",
      "10984\n",
      "10991\n",
      "10997\n",
      "10999\n",
      "11001\n",
      "11004\n",
      "11005\n",
      "11006\n",
      "11009\n",
      "11015\n",
      "11016\n",
      "11023\n",
      "11038\n",
      "11052\n",
      "11063\n",
      "11081\n",
      "11092\n",
      "11095\n",
      "11103\n",
      "11111\n",
      "11114\n",
      "11115\n",
      "11119\n",
      "11120\n",
      "11127\n",
      "11135\n",
      "11148\n",
      "11158\n",
      "11164\n",
      "11167\n",
      "11182\n",
      "11183\n",
      "11185\n",
      "11196\n",
      "11197\n",
      "11199\n",
      "11206\n",
      "11215\n",
      "11220\n",
      "11222\n",
      "11236\n",
      "11240\n",
      "11241\n",
      "11242\n",
      "11249\n",
      "11259\n",
      "11261\n",
      "11267\n",
      "11273\n",
      "11275\n",
      "11289\n",
      "11291\n",
      "11295\n",
      "11314\n",
      "11316\n",
      "11320\n",
      "11323\n",
      "11325\n",
      "11330\n",
      "11332\n",
      "11333\n",
      "11336\n",
      "11343\n",
      "11350\n",
      "11353\n",
      "11357\n",
      "11365\n",
      "11366\n",
      "11367\n",
      "11369\n",
      "11375\n",
      "11379\n",
      "11393\n",
      "11398\n",
      "11402\n",
      "11409\n",
      "11410\n",
      "11415\n",
      "11434\n",
      "11438\n",
      "11441\n",
      "11445\n",
      "11455\n",
      "11458\n",
      "11467\n",
      "11468\n",
      "11476\n",
      "11478\n",
      "11496\n",
      "11497\n",
      "11508\n",
      "11522\n",
      "11532\n",
      "11534\n",
      "11535\n",
      "11536\n",
      "11539\n",
      "11543\n",
      "11554\n",
      "11558\n",
      "11563\n",
      "11564\n",
      "11565\n",
      "11569\n",
      "11580\n",
      "11584\n",
      "11591\n",
      "11595\n",
      "11598\n",
      "11603\n",
      "11604\n",
      "11605\n",
      "11609\n",
      "11621\n",
      "11622\n",
      "11625\n",
      "11639\n",
      "11644\n",
      "11646\n",
      "11654\n",
      "11671\n",
      "11673\n",
      "11674\n",
      "11681\n",
      "11691\n",
      "11692\n",
      "11695\n",
      "11699\n",
      "11707\n",
      "11713\n",
      "11728\n",
      "11747\n",
      "11748\n",
      "11752\n",
      "11754\n",
      "11755\n",
      "11760\n",
      "11769\n",
      "11772\n",
      "11773\n",
      "11776\n",
      "11778\n",
      "11789\n",
      "11790\n",
      "11791\n",
      "11793\n",
      "11801\n",
      "11804\n",
      "11806\n",
      "11815\n",
      "11827\n",
      "11830\n",
      "11841\n",
      "11849\n",
      "11856\n",
      "11857\n",
      "11859\n",
      "11860\n",
      "11869\n",
      "11875\n",
      "11884\n",
      "11890\n",
      "11893\n",
      "11895\n",
      "11902\n",
      "11903\n",
      "11910\n",
      "11914\n",
      "11918\n",
      "11919\n",
      "11920\n",
      "11938\n",
      "11940\n",
      "11946\n",
      "11952\n",
      "11954\n",
      "11959\n",
      "11962\n",
      "11970\n",
      "11982\n",
      "11983\n",
      "11989\n",
      "11998\n",
      "12001\n",
      "12002\n",
      "12004\n",
      "12007\n",
      "12008\n",
      "12010\n",
      "12013\n",
      "12014\n",
      "12023\n",
      "12027\n",
      "12029\n",
      "12030\n",
      "12044\n",
      "12046\n",
      "12051\n",
      "12067\n",
      "12072\n",
      "12078\n",
      "12085\n",
      "12089\n",
      "12102\n",
      "12105\n",
      "12107\n",
      "12108\n",
      "12110\n",
      "12111\n",
      "12114\n",
      "12116\n",
      "12128\n",
      "12135\n",
      "12139\n",
      "12140\n",
      "12142\n",
      "12143\n",
      "12149\n",
      "12150\n",
      "12153\n",
      "12158\n",
      "12169\n",
      "12170\n",
      "12180\n",
      "12181\n",
      "12182\n",
      "12188\n",
      "12191\n",
      "12192\n",
      "12196\n",
      "12204\n",
      "12208\n",
      "12212\n",
      "12214\n",
      "12220\n",
      "12222\n",
      "12232\n",
      "12235\n",
      "12238\n",
      "12242\n",
      "12243\n",
      "12244\n",
      "12246\n",
      "12259\n",
      "12265\n",
      "12272\n",
      "12275\n",
      "12278\n",
      "12279\n",
      "12285\n",
      "12286\n",
      "12290\n",
      "12295\n",
      "12296\n",
      "12325\n",
      "12326\n",
      "12337\n",
      "12338\n",
      "12343\n",
      "12345\n",
      "12346\n",
      "12347\n",
      "12355\n",
      "12362\n",
      "12367\n",
      "12368\n",
      "12381\n",
      "12384\n",
      "12391\n",
      "12394\n",
      "12400\n",
      "12403\n",
      "12422\n",
      "12425\n",
      "12426\n",
      "12428\n",
      "12429\n",
      "12434\n",
      "12435\n",
      "12440\n",
      "12449\n",
      "12484\n",
      "12519\n",
      "12525\n",
      "12528\n",
      "12535\n",
      "12538\n",
      "12540\n",
      "12541\n",
      "12543\n",
      "12544\n",
      "12545\n",
      "12551\n",
      "12552\n",
      "12557\n",
      "12559\n",
      "12569\n",
      "12571\n",
      "12573\n",
      "12576\n",
      "12577\n",
      "12579\n",
      "12580\n",
      "12581\n",
      "12591\n",
      "12609\n",
      "12611\n",
      "12621\n",
      "12632\n",
      "12638\n",
      "12652\n",
      "12660\n",
      "12663\n",
      "12670\n",
      "12679\n",
      "12689\n",
      "12692\n",
      "12697\n",
      "12702\n",
      "12704\n",
      "12709\n",
      "12718\n",
      "12726\n",
      "12730\n",
      "12732\n",
      "12744\n",
      "12749\n",
      "12755\n",
      "12762\n",
      "12765\n",
      "12766\n",
      "12769\n",
      "12772\n",
      "12777\n",
      "12780\n",
      "12789\n",
      "12795\n",
      "12800\n",
      "12801\n",
      "12806\n",
      "12813\n",
      "12814\n",
      "12815\n",
      "12818\n",
      "12829\n",
      "12837\n",
      "12838\n",
      "12845\n",
      "12847\n",
      "12850\n",
      "12851\n",
      "12853\n",
      "12855\n",
      "12860\n",
      "12861\n",
      "12863\n",
      "12864\n",
      "12867\n",
      "12873\n",
      "12881\n",
      "12886\n",
      "12889\n",
      "12896\n",
      "12904\n",
      "12919\n",
      "12927\n",
      "12933\n",
      "12939\n",
      "12948\n",
      "12951\n",
      "12958\n",
      "12961\n",
      "12967\n",
      "12968\n",
      "12971\n",
      "12973\n",
      "12988\n",
      "12989\n",
      "12991\n",
      "13006\n",
      "13009\n",
      "13015\n",
      "13017\n",
      "13019\n",
      "13023\n",
      "13026\n",
      "13039\n",
      "13046\n",
      "13047\n",
      "13053\n",
      "13057\n",
      "13060\n",
      "13062\n",
      "13065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13066\n",
      "13069\n",
      "13075\n",
      "13078\n",
      "13084\n",
      "13085\n",
      "13089\n",
      "13095\n",
      "13096\n",
      "13099\n",
      "13101\n",
      "13104\n",
      "13106\n",
      "13122\n",
      "13131\n",
      "13135\n",
      "13136\n",
      "13148\n",
      "13153\n",
      "13157\n",
      "13176\n",
      "13190\n",
      "13196\n",
      "13199\n",
      "13209\n",
      "13229\n",
      "13230\n",
      "13238\n",
      "13245\n",
      "13246\n",
      "13254\n",
      "13255\n",
      "13257\n",
      "13259\n",
      "13262\n",
      "13273\n",
      "13287\n",
      "13304\n",
      "13309\n",
      "13311\n",
      "13315\n",
      "13320\n",
      "13321\n",
      "13324\n",
      "13339\n",
      "13343\n",
      "13347\n",
      "13349\n",
      "13357\n",
      "13358\n",
      "13366\n",
      "13374\n",
      "13376\n",
      "13382\n",
      "13391\n",
      "13397\n",
      "13402\n",
      "13405\n",
      "13406\n",
      "13407\n",
      "13409\n",
      "13422\n",
      "13427\n",
      "13431\n",
      "13438\n",
      "13441\n",
      "13444\n",
      "13456\n",
      "13457\n",
      "13458\n",
      "13465\n",
      "13479\n",
      "13486\n",
      "13491\n",
      "13496\n",
      "13501\n",
      "13506\n",
      "13510\n",
      "13519\n",
      "13526\n",
      "13547\n",
      "13554\n",
      "13559\n",
      "13562\n",
      "13568\n",
      "13572\n",
      "13577\n",
      "13588\n",
      "13589\n",
      "13593\n",
      "13594\n",
      "13602\n",
      "13606\n",
      "13621\n",
      "13622\n",
      "13630\n",
      "13633\n",
      "13634\n",
      "13635\n",
      "13645\n",
      "13646\n",
      "13652\n",
      "13656\n",
      "13661\n",
      "13667\n",
      "13682\n",
      "13688\n",
      "13689\n",
      "13691\n",
      "13693\n",
      "13694\n",
      "13700\n",
      "13716\n",
      "13728\n",
      "13740\n",
      "13746\n",
      "13751\n",
      "13765\n",
      "13774\n",
      "13790\n",
      "13806\n",
      "13809\n",
      "13826\n",
      "13836\n",
      "13844\n",
      "13855\n",
      "13857\n",
      "13862\n",
      "13874\n",
      "13884\n",
      "13885\n",
      "13892\n",
      "13893\n",
      "13894\n",
      "13899\n",
      "13905\n",
      "13911\n",
      "13912\n",
      "13913\n",
      "13921\n",
      "13922\n",
      "13928\n",
      "13930\n",
      "13933\n",
      "13939\n",
      "13940\n",
      "13944\n",
      "13952\n",
      "13956\n",
      "13960\n",
      "13962\n",
      "13965\n",
      "13972\n",
      "13973\n",
      "13978\n",
      "13995\n",
      "13996\n",
      "14002\n",
      "14010\n",
      "14012\n",
      "14013\n",
      "14016\n",
      "14017\n",
      "14022\n",
      "14025\n",
      "14031\n",
      "14034\n",
      "14039\n",
      "14046\n",
      "14055\n",
      "14061\n",
      "14070\n",
      "14075\n",
      "14076\n",
      "14077\n",
      "14080\n",
      "14090\n",
      "14091\n",
      "14094\n",
      "14101\n",
      "14109\n",
      "14118\n",
      "14124\n",
      "14125\n",
      "14129\n",
      "14139\n",
      "14144\n",
      "14153\n",
      "14157\n",
      "14167\n",
      "14169\n",
      "14181\n",
      "14182\n",
      "14183\n",
      "14188\n",
      "14191\n",
      "14193\n",
      "14196\n",
      "14201\n",
      "14202\n",
      "14206\n",
      "14217\n",
      "14218\n",
      "14221\n",
      "14223\n",
      "14229\n",
      "14231\n",
      "14235\n",
      "14238\n",
      "14243\n",
      "14245\n",
      "14255\n",
      "14258\n",
      "14259\n",
      "14261\n",
      "14269\n",
      "14281\n",
      "14286\n",
      "14293\n",
      "14300\n",
      "14315\n",
      "14319\n",
      "14344\n",
      "14346\n",
      "14351\n",
      "14352\n",
      "14366\n",
      "14370\n",
      "14379\n",
      "14384\n",
      "14389\n",
      "14394\n",
      "14395\n",
      "14414\n",
      "14416\n",
      "14419\n",
      "14423\n",
      "14425\n",
      "14427\n",
      "14428\n",
      "14434\n",
      "14444\n",
      "14454\n",
      "14461\n",
      "14477\n",
      "14488\n",
      "14490\n",
      "14519\n",
      "14532\n",
      "14534\n",
      "14541\n",
      "14551\n",
      "14552\n",
      "14561\n",
      "14562\n",
      "14576\n",
      "14577\n",
      "14587\n",
      "14589\n",
      "14593\n",
      "14601\n",
      "14605\n",
      "14617\n",
      "14621\n",
      "14622\n",
      "14624\n",
      "14629\n",
      "14632\n",
      "14636\n",
      "14637\n",
      "14644\n",
      "14650\n",
      "14652\n",
      "14655\n",
      "14661\n",
      "14662\n",
      "14674\n",
      "14675\n",
      "14677\n",
      "14698\n",
      "14701\n",
      "14702\n",
      "14707\n",
      "14711\n",
      "14715\n",
      "14723\n",
      "14725\n",
      "14728\n",
      "14734\n",
      "14736\n",
      "14739\n",
      "14745\n",
      "14760\n",
      "14769\n",
      "14775\n",
      "14780\n",
      "14781\n",
      "14789\n",
      "14790\n",
      "14793\n",
      "14799\n",
      "14813\n",
      "14820\n",
      "14842\n",
      "14848\n",
      "14852\n",
      "14859\n",
      "14865\n",
      "14866\n",
      "14870\n",
      "14883\n",
      "14885\n",
      "14886\n",
      "14889\n",
      "14893\n",
      "14899\n",
      "14901\n",
      "14924\n",
      "14926\n",
      "14931\n",
      "14934\n",
      "14936\n",
      "14938\n",
      "14939\n",
      "14941\n",
      "14943\n",
      "14945\n",
      "14946\n",
      "14947\n",
      "14963\n",
      "14964\n",
      "14969\n",
      "14974\n",
      "14978\n",
      "14989\n",
      "15001\n",
      "15007\n",
      "15009\n",
      "15011\n",
      "15026\n",
      "15027\n",
      "15038\n",
      "15057\n"
     ]
    }
   ],
   "source": [
    "# Misclassified points\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "for i in range(len(X_test)):\n",
    "    if (y_pred[i][y_test[i]] < y_pred[i][1 - y_test[i]]):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_influence_loss(X_train, y_train, X_test, y_test, idx):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba([X_test[idx]])[0]\n",
    "    loss_0 = - y_test[idx] * math.log(y_pred[1]) - (1 - y_test[idx]) * math.log(y_pred[0])\n",
    "\n",
    "    delta_loss = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_removed = np.delete(X_train, i, 0)\n",
    "        y_removed = y_train.drop(index=i, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "        y_pred = clf.predict_proba([X_test[idx]])[0]\n",
    "        loss_i = - y_test[idx] * math.log(y_pred[1]) - (1 - y_test[idx]) * math.log(y_pred[0])\n",
    "        delta_loss_i = loss_i - loss_0\n",
    "        delta_loss.append(delta_loss_i)\n",
    "    \n",
    "    return delta_loss\n",
    "\n",
    "# ix = 203 #misprediction\n",
    "# v1 = del_L_del_theta_i(num_params, y_test[ix], X_test[ix], y_pred_test[ix])\n",
    "infs_1 = first_order_influence(del_L_del_theta, hinv_v, len(X_train))\n",
    "# delta_loss_gt = ground_truth_influence_loss(X_train, y_train, X_test, y_test, ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARoAAAEGCAYAAAC6p1paAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlfklEQVR4nO3deXxV9Z3/8debsCRsQQSNBSnoBBmlVWnUoVpFRAe1gmuVupS64FZHx6mtU+dXrXam1qUPN9qKjnVrXceFuoAiIK2CEndBFEStoBEQJIAJEPL5/fE90UtIwgm5554k9/N8PO4j95z7zTmfQPjwPd9zvp+vzAznnEtSh7QDcM61f55onHOJ80TjnEucJxrnXOI80TjnEtcx7QCyrU+fPjZw4MC0w3AuL73yyisrzKxv/f3tLtEMHDiQ8vLytMNwLi9J+qih/X7p5JxLnCca51ziPNE45xLnicY5lzhPNM65xKWaaCTdIWmZpLcb+VySbpK0SNKbkoblOkbnXMul3aO5ExjdxOeHA6XRawLwhxzE5JzLslQTjZnNAlY20WQscLcFc4BeknbKTXTOuWxJu0ezNf2AjzO2l0T7NiNpgqRySeXLly/PWXDOtWsbNmTtUK090cRiZpPMrMzMyvr23eLpZ+dcc61YAcOHwy23ZOVwrX0KwlJg54zt/tE+51xSPvsMRo2CRYtg112zcsjW3qOZDJwW3X36F2C1mX2adlDOtVtLl8JBB8HixfDkk3D44Vk5bKo9Gkn3ASOAPpKWAJcDnQDM7I/AU8ARwCLgS+DH6UTqXB748suQZJYtg6lT4YADsnboVBONmY3byucGnJ+jcJzLb127wsUXw3e+A/vtl9VDt/YxGudc0hYsCIO/BxwA552XyCk80TiXz956Kwz8FhfD/PnQMZmU0NoHg51zSXnlFRgxAjp1gr/+NbEkA55onMtPc+bAIYdAjx4waxbstluip/NE41w+uuMO6Ns3JJlddkn8dD5G41w+2bQJCgrg97+HlSthhx1yclrv0TiXL558EoYNg4qKMB6ToyQDnmicyw+PPgrHHBMGfjt1yvnpPdE4197ddx+ccAKUlcG0abD99jkPwRONc+3Zo4/CySfD/vuHaQW9eqUShica59qzAw6Ac86Bp58Ot7JT4onGufbo8cdD4aq+fcMdpq5dUw3HE41z7c3VV8PRR8PEiWlH8hVPNM61F2ZwxRXwn/8J48bBBRekHdFX/IE959oDM7j0UrjmGvjxj+G228KDea2E92icaw8+/hhuvRXOPRduv71VJRnwHo1zbZsZSDBgALz6KgwaFLZbGe/RONdWbdoE48eHyyUIkyNbYZIBTzTOtU0bN4YH8e6+G9avTzuarfJLJ+famvXr4aST4LHHQm/mkkvSjmirPNE415aYwfHHwxNPwE03tapb2E3xRONcWyLBUUeF14QJaUcTmyca59qCNWvg7bfDMrVtKMHUSXUwWNJoSe9KWiTp0gY+HyBphqTXJL0p6Yg04nQuVV98AYceCqNHw6pVaUezTVJLNJIKgInA4cDuwDhJu9dr9l/Ag2a2N3AS8PvcRulcylasgJEj4bXXwh2m7bZLO6JtkmaPZl9gkZktNrMNwP3A2HptDOgZvS8GPslhfM6l67PP4OCD4Z13wmzssfX/ebQdaY7R9AM+ztheAtRfh/MK4BlJFwDdgFENHUjSBGACwIABA7IeqHOpmDgRFi8OtX5Hjkw7mhZp7Q/sjQPuNLP+wBHAPZK2iNnMJplZmZmV9e3bN+dBOpeIyy+HuXPbfJKBdBPNUmDnjO3+0b5MZwAPApjZbKAQ6JOT6JxLw6JFYfXIjz8OEyN3rz9s2TalmWjmAqWSBknqTBjsnVyvzT+AQwAk/TMh0SzPaZTO5co778CBB4bb2CtXph1NVqWWaMysBvgJMBV4h3B3aZ6kKyWNiZr9B3CWpDeA+4DxZmbpROxcgt58Ew46CGprYeZM2HPPtCPKqlQf2DOzp4Cn6u37Zcb7+cD+uY7LuZx6881wd6moCJ57LvF1sNPQ2geDnWv/+veH730vrIPdDpMMeKJxLj2vvgrV1dC7d5iJvcsuaUeUGE80zqXh2WfDmks//3nakeSEJxrncu2JJ8Ls69JSuOyytKPJCU80zuXSI4/AscfC0KEwYwbssEPaEeWEJxrncmXtWjjvPCgrC3eXevdOO6KciXV7W9I3gVIzmyapCOhoZmuSDc25dqZ7d5g2Db75zVTXwU7DVns0ks4CHgZujXb1Bx5LMCbn2pc//hF+9avwfujQvEsyEO/S6XzCQ3OVAGa2EMiPC0vnWuqGG8KibnPnQk1N2tGkJk6iWR/ViwFAUkdCnRjnXFN+8xv493+H444Lg8Ad87dybpxE87ykXwBFkg4FHgL+mmxYzrVxv/oV/OIX8MMfwv33Q+fOaUeUqjiJ5lLCjOm3gLMJc5P+K8mgnGvzBg2CM88M5TfzuCdTR1ubDC2pG1BtZpui7QKgi5l9mYP4mq2srMzKy8vTDsPlI7NQ4uFb30o7ktRIesXMyurvj9OjeQ4oytguAqZlKzDn2oXaWjjnnPCMzIIFaUfT6sRJNIVmtrZuI3rfNbmQnGtjamrgxz+GSZPgpz9ttzOwWyJOolknaVjdhqTvAFXJheRcG7JxI5x8chiLueoq+O//DqtJus3EGaW6CHhI0ieAgBLgxCSDcq7NuOsuePBBuPba0JtxDdpqojGzuZKGAHX9wXfNbGOyYTnXRpx+erjDdMghaUfSqsWdVLkP8G1gGGFFydOSC8m5Vm7dOjjtNPjgA+jQwZNMDFvt0Ui6B9gVeB3YFO024O7kwnKulaqshCOPhBdfhDFjQm/GbVWcMZoyYHdffcDlvVWrYPToUILz/vvh+OPTjqjNiHPp9DZhANi5/LViRVgx8vXX4f/+D044Ie2I2pQ4PZo+wHxJLwPr63aa2ZjGv8W5dqZzZ+jVCx5/PPRqXLPESTRXJHVySaOBG4EC4HYzu7qBNj+IYjDgDTP7YVLxOLeFTz6B4mLo2ROmT/dnZLZRnNvbz9ersNeVkBhaJJozNRE4FFgCzJU0OVo0rq5NKfCfwP5mtkqS18FxufPhh+Fyae+9w+WSJ5ltti0V9vqRnQp7+wKLzGxxVO/mfmBsvTZnARPNbBWAmS3Lwnmd27pFi8I62KtW5c2SKElKs8JeP+DjjO0l0b5Mg4HBkl6QNCe61NqCpAmSyiWVL1++PAuhubz2zjshyVRVhZUK9t037YjavNZeYa8jUAqMAMYBt0nqVb+RmU0yszIzK+vbt2+OQnPtUm0tnHhiKPkwcybstVfaEbULcQaD61fYO4/sVNhbCuycsd0/2pdpCfBSNOXhA0nvERLP3Cyc37ktdegAf/4zdOkCgwenHU27kWaFvblAqaRBkjoDJwGT67V5jNCbQVIfwqXU4iyc27nNzZkTym+ahcJVnmSyKs5dp1rgtuiVNWZWI+knwFTCXaw7zGyepCuBcjObHH12mKT5hOkPl5jZ59mMwzlmzQrTCkpK4MILw/MyLqvilPL8gAbGZMxsl6SCagkv5ema5dlnYexYGDgwLO72jW+kHVGb1lgpz7hzneoUAicA+bOWp2u/nngizFfabbeQcPJkHew0bHWMxsw+z3gtNbMbgCOTD825hFVVhYfxZszwJJOwOGUihmVsdiD0cHz9CNd2LV0K/fqFiZHHHgsFLX7Q3W1FnIRxfcb7GuBD4AeJRONc0v70p7BE7TPPhIfyPMnkRJy7TgfnIhDnEvfHP4Ykc9hhYVkUlzONJhpJFzf1jWb2u+yH41xCbrghrIN91FGhmHhhYdoR5ZWmejQ9chaFc0maMSMkmeOOg7/8Je/XwU5DU4mmq5n9XNIJZvZQziJyLttGjAjrLo0b5+tgp6Sp29tHSBKhHoxzbYsZ/PrX8N57oY7Mqad6kklRU3/yU4BVQHdJlRn7BZiZ9Uw0Mue2VW0tXHQR3HwzbNgAV16ZdkR5r9EejZldYma9gCfNrGfGq4cnGddq1dbCOeeEJHPxxWGipEtdnCeD61e9c651qqmB8ePhttvgssvguuu8/GYrEaeU57GSFkpaLalS0pp6l1LOtQ4bNoQ6v1ddFcZnPMm0GnFGx64BjjKzd5IOxrltsn59SDI9eoQZ2H77utWJk2g+8yTjWq2qqjBf6csvw3IonmRapTiJplzSA4Rqd5kLyD2SVFDOxbJuXVj/esYMmDTJ5y21YnESTU/gS+CwjH0GeKJx6amsDFXxXnwxPIx3yilpR+SaEGdS5Y9zEYhzzTJ+fKjze//9vg52G9DUpMqfmdk1km6m4VKe/5ZoZM415Te/gdNPh+9/P+1IXAxN9WjqBoC9AK9rHSoqwmXSJZeE8pu77ZZ2RC6mRhONmf01+npX7sJxrhFLlsAhh4SvxxwDpaVpR+SawWeZudbvww9h5EhYsSJUxvMk0+Z4onGt28KFoSezZk14GM/XwW6T4qxUmRhJoyW9K2mRpEubaHecJJPk9RfzzcKFYQ7TjBmeZNqwOKsg9AXOAgZmtjez01tyYkkFwETgUMIa23MlTTaz+fXa9QAuBF5qyflcG7NmTZhScMQRsGgRdO2adkSuBeL0aB4HioFpwJMZr5baF1hkZovNbANwP9DQTPGrgN8C1Vk4p2sLXnkFdt0VHnssbHuSafPijNF0NbOfJ3DufsDHGdtLgP0yG0RrSu1sZk9KuqSxA0maAEwAGDBgQAKhupyZPRtGj4bevWHPPdOOxmVJnB7NE5KOSDySeiR1AH4H/MfW2prZJDMrM7Oyvn37Jh+cS8bzz8Ohh4ZVI2fNgkGD0o7IZUlTTwavITwRLOAXktYDG8leKc+lwM4Z2/2jfXV6AEOBmaF0MSXAZEljzMwfImxvFi2Cww+HgQPhuedgp53SjshlUVMP7CW93MpcoFTSIEKCOQn4Ycb5VwN96rYlzQR+6kmmndp11zCtYNw4Xwe7HYpTYe+5OPuay8xqgJ8AUwnTHR40s3mSrpQ0pqXHd23EY4/B22+HangXXuhJpp1q6tKpEOgG9JG0HeGSCULZiH7ZOLmZPQU8VW/fLxtpOyIb53StyF/+AqedFlaPfPTRtKNxCWrqrtPZwEXAN4BXM/ZXArckGJPLB3/6E5xxBhx4YJgo6dq1psZobgRulHSBmd2cw5hce/eHP8B558Fhh4WejD8n0+7FeY5mtaTT6u80M/9vyDVfbW1ILkcdBQ8+CIWFaUfkciBOotkn430hcAjhUsoTjWue6uqQWB57LCxP64XE80acUp4XZG5L6kWYLuBcPGZw+eWhxMO0adC9e9oRuRzbltnb6wB/ZNPFYwY/+1lY1G3oUCgqSjsil4I4s7f/ytc1gzsAuwMPJhmUaydqa8OzMbfcEgZ/b74ZOqRamcSlJM4YzXUZ72uAj8xsSULxuPbk8stDkrn4Yl8HO881mWiimjFXmNnBOYrHpahiYQULpi9gdcVqikuKGTJyCCWlJdt+wNNPDzVlLrnEk0yea7Ifa2abgFpJxTmKx6WkYmEFs++ZTVVlFT136ElVZRWz75lNxcKK5h1o40a47bZw2TRoUBif8SST9+JcOq0F3pL0LGEgGPB1ndqbBdMXUNijkKKeYbC27uuC6Qvi92rWr4cTT4THH4dddgm1fp0jXqJ5hC2Xv91iQTnXtq2uWE3PHTav/FHYvZDVFavjHaCqCo49FqZMCeMynmRchjiJplc0HeErki5MKB6XkuKSYqoqq77qyQBUr62muCTGVfPatTBmDMycCbffHuYwOZchzr3GHzWwb3yW43ApGzJyCNVrqqmqrMJqjarKKqrXVDNk5JCtf/Mbb8BLL4XJkZ5kXAOaKhMxjlCIapCkyRkf9QBWJh2Yy62S0hKGnzp8s7tOex+9d9PjMzU1YSrB/vvDBx94LRnXqKYunV4EPiVUubs+Y/8a4M0kg3LpKCktiT/wu3x5KCJ+0UVw6qmeZFyTmioT8RHwETA8d+G4NqGiIgz2Ll7sCcbF4kviuuZZsiQkmaVL4amn4GB/ltNtnScaF9/q1aEi3ooVMHVqGJtxLgZPNC6+4mI4+2wYORL22Wfr7Z2LNHXX6S2aeDDPzL6dSESu9Zk/PxStGjYMfp7EoqWuvWuqR/P96Ov50dd7oq8nJxeOy4VmTZ584w0YNQpKSsJ7L/PgtkGjvzVm9lF05+lQM/uZmb0VvS4FDstdiC6bmjV5srw8DPYWFsIjj3iScdsszm+OJO2fsfHdmN8X58CjJb0raZGkSxv4/GJJ8yW9Kek5Sd/MxnnzVcXCCqZcO4VP5n/CZws/Y92qdRT1LKKwRyELpi/YvPHs2eHuUnFxWAe7tDSdoF27ECdhnAH8XtKHkj4Cfg+c3tITR7VuJgKHE6r2jZO0e71mrwFl0XjQw8A1LT1vvqrryaxbtY6iXkVsXL+RJW8uYe3naxuePHnjjbDjjiHJDPLKra5l4hQnfwXYs64mTbQmdjbsCywys8UAku4HxgLzM849I6P9HOCULJ0779SVgei2XTc2rt9Ipy6dAFjx4QoKOhV8PXmytjZcIt15Z7idveOO6QXt2o04a293kfRDwqDwhZJ+KanBZWubqR/wccb2EppeavcM4OlGYpwgqVxS+fLly7MQWvuzumI1hd0L6TOoDzXra9i4fiMFnQpYt3Ld15Mnn3giPBuzalUYl/Ek47IkzqXT44SeRg2h8FXdK2cknQKUAdc29LmZTTKzMjMr69u3by5DazOKS4qpXltN997d2XnPnenUpRNVq6vo1rsbw08dTsmbL8Axx4SJkublhlx2xXlgr7+ZjU7g3EuBnTPPE+3bjKRRwGXAQWa2PoE48sKQkUOYfc9sALr16kZBaQHVa6pDkpk7HU47DfbbL0wrKPbKrS674vRoXpT0rQTOPRcolTRIUmfgJCCzHAWS9gZuBcaY2bIEYsgbdWUginoWUbmskqKeRSHJvPo8nHJKmFowdaonGZeIOD2aA4Dxkj4A1gMCrKVPBptZjaSfAFOBAuAOM5sn6Uqg3MwmEy6VugMPKRS4/oeZjWnJefNZg2UgCr8bilXddJMv7uYSI9vK9Xhjz65ED/O1OmVlZVZeXp52GK3fU0/Bv/4rFBSkHYlrRyS9YmZl9ffHuXSyRl6urfqf/4Ejjwz1fZ3LgTiXTk8SEouAQsK62+8CeyQYl0uCWVg98qqr4OSTvb6vy5k4D+xtNhAsaRhwXmIRuWSYhcXcrrsuJJhbb/XLJpczzZ6zZGavAvslEItL0sKFMHEinHceTJrkScbl1FZ7NJIuztjsAAwDPkksIpddZmFJ2sGD4dVXYbfdfIlal3NxejQ9Ml5dCGM2Y5MMymVJTQ2MHx/WwgYYMsSTjEtFnDGaXwFI6h5tr006KJcFGzeGAd+HHgq9GOdSFGdS5VBJrwHzgHmSXpE0NPnQ3DZbvx5OOCEkmeuvh1/8Iu2IXJ6Lc3t7EnBxXckGSSOifd9NLiy3zTZtgqOPhilT4JZb4Pzzt/otziUtTqLpllkXxsxmSuqWYEyuJQoKQvnN44/352RcqxEn0SyW9P/4ujj5KcDi5EJy26SyEt5/H/beOzwv41wrEueu0+lAX+AR4P8Ia3G3uJSny6KVK8NKBYcdBmvWpB2Nc1toskcT1fV9xMx83dPWavnykGDmz4eHH4YePdKOyLktNNmjMbNNQG1dvWDXylRUwIgRsGABTJ4MRx2VdkTONSjOGM1a4C1Jz5JRwtPM/i2xqFw811wDH30ETz8dEo5zrVScRPNI9HIpqVhYwazbZ/GP1/7BpppN9PpGLw4YfwB7XH11ePL32746sWvd4jwZfFcuAnGbq1u29tN3P+XDVz9kw9oNAPSu/pxDXvtfnvm0AjiePUZ5knGtX6NjNJLGSjo/Y/slSYuj1/G5CS8/1S32tuKjFSx8ceFXSaZP1TLGv/cndlq7lE6rPmfug3NTjtS5eJrq0fyMUDC8ThdgH6Ab8CfCypEuAQumL2DlkpUseXPJV/t2/LKCUxfeTS0duHPweFYUbE/v5X4r27UNTSWazmaWucDb383sc+BzfzI4WQv+voDP3vnsq+2SLz/ltPfuYkOHztw9+EesLNweDHr09VvZrm1oKtFsl7lhZj/J2PRV2hIy5XdTNksyAGs6dWdpt348OeD7fNHl67+WfX6wT67Dc26bNJVoXpJ0lpndlrlT0tnAy8mGlV/qBn7r92R2/LKCZUV9WdepB38uPXWz7xl6+FD2GOVlm13b0FSi+XfgsWjd7Vejfd8hjNUcnXBceaNu4LewR+FmSWaXykWctOh+5uz4L0zvN2qz79l1/1057qrjch2qc9us0btOZrbMzL4LXAV8GL2uNLPhZvZZY9/XHJJGS3pX0iJJlzbweRdJD0SfvyRpYDbO25osmL6Awh6FvPzA153E0i/eZdyi+/i8cHvm7DB8s/bbD9qeU248JddhOtcicZ6jmQ5Mz/aJo3lUE4FDgSXAXEmTzWx+RrMzgFVm9k+STgJ+C5yY7VjStLpiNa//9fWvtv951XyOW/wwFV1LuLf0FKo7dv26cQc4/n/8yQLX9jR7FYQs2hdYZGaLzWwDcD9b1iIeC9Q9MPgwcIjUvoreZiaZLjVVHPXRZJZ268c9g0/bPMkAoy4YteWSts61AXGmICSlH5B5+3wJWy7j8lWbaK3u1cD2wIrMRpImABMABgwYkFS8WfeHcX/YbHt9xyLuKT2VFYV92FjQZbPP9vvhfux/6v65DM+5rEmzR5M1ZjbJzMrMrKxv37Zx5/3eC+9l2cJlW+z/tFu/LZLMrvvvyuiLR+cqNOeyLs1EsxTYOWO7f7SvwTaSOgLFwOc5iS5B9154L++/8H7s9j7469q6NBPNXKBU0iBJnQnTHSbXazMZ+FH0/nhguplZDmPMunnT5jUryVxefnmC0TiXG6mN0URjLj8BpgIFwB1mNk/SlUC5mU0G/he4R9IiYCWbz71qk5658ZnYbT3JuPYizcFgzOwp4Kl6+36Z8b4aOCHXcSXlhXteoPLTyq037ASXz/Yk49qPVBNNPpk3bR4zb5251XY9duzBsDHDkg/IuRzyRJMDFQsr+Nvtf6O2phYENDLK1P/b/endvzdDRg7JaXzOJc0TTcIqFlYw5doprKpYBYIOHTtQu6kWajMaCQYMG8CgYYMYMnKIP5Tn2h1PNAmaN20es26fxeqK1VALHQo6ULO+hg4dO2AybJNR0LmAg8892B/Gc+2aJ5qEVCysYPrvp1O9tppNNZuwWsNqjY5dOlK7qRZJFBQWMOLsEZ5kXLvniSYh5Q+Xs/bztXQu6kznrp1Zv249dY8AdezSke1KtuN7Z37Pa8q4vOCJJovmTZvH3Afnsmb5GtYsX4MKBIJOnTshxIaqDdRuqmWnwTsx+pLRPhbj8oYnmiyZN20eU66fEgZ6gZoNNZgZZkaXbl0o6FxAJ+tEbU2tJxmXd9rFpMrW4O93/p2N1Rsp6FBAx04dKehUAMCmDZvo0KEDG6s2YrXGoH0GeZJxecd7NFnyxSdf0KlLJzp0DLm7sEch675Yh9UaRT2L6N6nO92268aBZx2YcqTO5Z4nmiwp6FjApk2bKOhY8NV256LO1NbUstOQnSguKfZnZFze8kSTJQP2HsD7c95nozbSsVNHajbWYLXGP333nxh7ef3Cgc7lFx+jyZIDzzyQksElFHQoYP2X6ynoUEDJ4BIOPNMvlZzzHs02qFuHaXXF6s0uiQ7/2eEN7ncu33miaabMdZh67tCTqsoqZt8zm+GnDqektMQTi3MN8EunZqpbh6moZxHqIIp6FlHYo5AF0xekHZpzrZYnmmZaXbGawu6Fm+0r7F4YJk465xrkiaaZikuKqV5bvdm+6rXVFJcUpxSRc62fJ5pmGjJyCNVrqqmqrMJqjarKKqrXVHuxKuea4ImmmUpKSxh+6nCKehZRuaySop5FXw0EO+ca5nedtoHfXXKuebxH45xLnCca51ziUkk0knpLelbSwujrdg202UvSbEnzJL0p6cQ0YnXOtVxaPZpLgefMrBR4Ltqu70vgNDPbAxgN3CCpV+5CdM5lS1qJZixwV/T+LuDo+g3M7D0zWxi9/wRYBvTNVYDOuexJK9HsaGafRu8rgB2baixpX6Az8H4jn0+QVC6pfPny5dmN1DnXYond3pY0DWjoHvBlmRtmZpIaWbsRJO0E3AP8yMxqG2pjZpOASQBlZWWNHss5l47EEo2ZjWrsM0mfSdrJzD6NEsmyRtr1BJ4ELjOzOQmF6pxLWFoP7E0GfgRcHX19vH4DSZ2BR4G7zezhbJ68sXoyzrlkpDVGczVwqKSFwKhoG0llkm6P2vwAOBAYL+n16LVXS09cV0+mqrJqs3oyFQsrWnpo51wjUunRmNnnwCEN7C8Hzoze3wvcm+1zZ9aTAb76umD6Au/VOJeQvHsy2OvJOJd7eZdovJ6Mc7mXd4nG68k4l3t5l2i8noxzuZeX9Wi8noxzuZV3PRrnXO55onHOJc4TjXMucZ5onHOJ80TjnEuczNpXVQVJy4GPtvHb+wArshiOx+AxtIcYIH4c3zSzLQrUtbtE0xKSys2szGPwGDyG7Mbhl07OucR5onHOJc4TzeYmpR0AHkMdjyFoDTFAC+PwMRrnXOK8R+OcS5wnGudc4vI60aS5NK+k0ZLelbRI0hYrdUrqIumB6POXJA3MxnmbGcPFkuZHP/dzkr6Z6xgy2h0nySRl/VZvnBgk/SD6s5gn6S+5jkHSAEkzJL0W/X0ckUAMd0haJuntRj6XpJuiGN+UNCz2wc0sb1/ANcCl0ftLgd820GYwUBq9/wbwKdCrhectICyGtwthYbw3gN3rtTkP+GP0/iTggSz/7HFiOBjoGr0/N40YonY9gFnAHKAshT+HUuA1YLtoe4cUYpgEnBu93x34MJsxRMc9EBgGvN3I50cATwMC/gV4Ke6x87pHQ3pL8+4LLDKzxWa2Abg/iqWx2B4GDpGkFp63WTGY2Qwz+zLanAP0z+L5Y8UQuQr4LVDdwGe5iOEsYKKZrQIwswbXIUs4BgN6Ru+LgU+yHANmNgtY2USTsYTlj8zCOmu9onXZtirfE01Wl+Zthn7AxxnbS6J9DbYxsxpgNbB9C8/b3BgynUH43yybthpD1D3f2cyezPK5Y8dA6NUOlvSCpDmSRqcQwxXAKZKWAE8BF2Q5hjia+zvzlXZfYS+XS/O2V5JOAcqAg3J83g7A74DxuTxvAzoSLp9GEHp1syR9y8y+yGEM44A7zex6ScOBeyQNbSu/i+0+0VjrXJp3KbBzxnb/aF9DbZZI6kjoLn+ehXM3JwYkjSIk5YPMbH0Wzx8nhh7AUGBmdNVYAkyWNMbCGmC5iAHC/9wvmdlG4ANJ7xESz9wcxnAGMBrAzGZLKiRMdMz2ZVxTYv3ONCjbA0pt6QVcy+aDwdc00KYz8BxwURbP2xFYDAzi68G/Peq1OZ/NB4MfzPLPHieGvQmXiaUJ/flvNYZ67WeS/cHgOH8Oo4G7ovd9CJcP2+c4hqeB8dH7fyaM0SiBv5OBND4YfCSbDwa/HPu4SfwCtZUXYczjOWAhMA3oHe0vA26P3p8CbARez3jtlYVzHwG8F/1DvizadyUwJnpfCDwELAJeBnZJ4OffWgzTgM8yfu7JuY6hXtusJ5qYfw4iXMLNB94CTkohht2BF6Ik9DpwWAIx3Ee4q7qR0Is7AzgHOCfjz2FiFONbzfm78CkIzrnE5ftdJ+dcDniicc4lzhONcy5xnmicc4nzROOcS5wnmnZE0iZJr2e8Bkp6sZnHuEhS1yzHdaek41vw/WOamtmd0e7aaHb1tZKukPTTbT2nyy6/vd2OSFprZt1jtOtoYf5UQ599SHg+YpuW+Gjo2JLuBJ4ws4e39Rgxv2814VmoTZKuANaa2XXNPY7LPu/RtHOS1kZfR0j6m6TJwHxJ3SQ9KekNSW9LOlHSvxFKYcyQNKOBYynqLbwt6a262jwNHFuSbonqq0wDdsg4xnckPS/pFUlT62b/Spop6QZJ5cCF9c47XtIt0fs7o5ooL0paXNdTis7dHXhF9WoGRccui973iZIpkgqin2duVF/l7IyfZ6akhyUtkPTnupnzkvaJzv2GpJcl9WjsOO5r7X6uU54pkvR69P4DMzum3ufDgKFm9oGk44BPzOxIAEnFZrZa0sXAwY30aI4F9gL2JDyKP1fSrAaOfSywG+Fp1h0JT9TeIakTcDMw1syWRwnhv4HTo2N0tnhrB+0EHAAMASYDD5vZmKhHt1f081wR4zhnAKvNbB9JXYAXJD0TfbY3sAfhUf8XgP0lvQw8AJxoZnOjOXBVjR3HzD6IEUNe8ETTvlTV/UNrxMsZv/xvAddL+i3hsuZvMY5/AHCfmW0CPpP0PLAPUFnv2AdmtPtE0vRo/26ESZLPRh2EAsIj73UeiBEDwGMWZi3Pl9RkaY+tOAz4dsb4UTFhsuQGws+zBCBK3gMJpTo+NbO5AGZWGX3e2HE80UQ80eSXdXVvzOw9hVovRwC/lvScmV2Z2VjSMcDl0eaZcY/dBAHzzGx4C44BkDmLPE4xsBq+HiYorPe9F5jZ1M2ClEbUO8cmmv630uBx3Nd8jCZPSfoG8KWZ3UuYxV5X/3UNoTwDZvaome0VvcqBvwEnRmMSfQk9l5cbOPysjHY7EUqCArwL9FWop4KkTpL2SOpnzPAh8J3ofebdr6nAudElHZIGS+rWxHHeBXaStE/UvodCCY/mHifveI8mf30LuFZSLWG27rnR/knAFEmfmNnB9b7nUWA4YQaxAT8zswpJQxpoN5IwNvMPYDaAmW2ILi9uklRM+P27AZiX7R+unuuAByVNINQVqnM74ZLo1WiwdzkNlHOtE8V/InCzpCLC+Myo5h4nH/ntbedc4vzSyTmXOE80zrnEeaJxziXOE41zLnGeaJxzifNE45xLnCca51zi/j/VwA6AE6T3JgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = (0.5,0.2,0.5)\n",
    "# fig = plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "plt.scatter(infs_1, delta_loss_gt, c=colors, alpha=0.5)\n",
    "plt.xlabel('First-order influence')\n",
    "plt.ylabel('Ground truth influence')\n",
    "ax.plot((0,1), 'r--')\n",
    "# ax.plot([0,1],[0,1], transform=ax.transAxes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation between first-order and ground truth influences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 30162 and the array at index 1 has size 500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d46aab0f573b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m print(\"Spearman rank correlation between 1st order inf and ground truth inf: \", \n\u001b[0;32m----> 2\u001b[0;31m       stats.spearmanr(spdgt, infs_1)[0])\n\u001b[0m\u001b[1;32m      3\u001b[0m print(\"Pearson correlation coefficient between 1st order inf and ground truth inf: \", \n\u001b[1;32m      4\u001b[0m       stats.pearsonr(spdgt, infs_1)[0])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36mspearmanr\u001b[0;34m(a, b, axis, nan_policy)\u001b[0m\n\u001b[1;32m   4181\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_chk_asarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxisout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4183\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4184\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4185\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcolumn_stack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mcolumn_stack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 30162 and the array at index 1 has size 500"
     ]
    }
   ],
   "source": [
    "print(\"Spearman rank correlation between 1st order inf and ground truth inf: \", \n",
    "      stats.spearmanr(spdgt, infs_1)[0])\n",
    "print(\"Pearson correlation coefficient between 1st order inf and ground truth inf: \", \n",
    "      stats.pearsonr(spdgt, infs_1)[0])\n",
    "\n",
    "colors = (0.5,0.2,0.5)\n",
    "fig = plt.figure()\n",
    "plt.scatter(infs_1, spdgt, c=colors, alpha=0.5)\n",
    "plt.xlabel('First-order influence')\n",
    "plt.ylabel('Ground truth influence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Space Partitioner for reducing bias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital', 'occupation', 'relationship', 'race', 'gender', 'capgain', 'caploss', 'hours', 'country', 'income']\n",
    "df_train = pd.read_csv('adult.data', names=cols, sep=\",\")\n",
    "df_test = pd.read_csv('adult.test', names=cols, sep=\",\")\n",
    "\n",
    "def preprocess(df):\n",
    "    df.isin(['?']).sum(axis=0)\n",
    "\n",
    "    # replace missing values (?) to nan and then drop the columns\n",
    "    df['country'] = df['country'].replace('?',np.nan)\n",
    "    df['workclass'] = df['workclass'].replace('?',np.nan)\n",
    "    df['occupation'] = df['occupation'].replace('?',np.nan)\n",
    "\n",
    "    # dropping the NaN rows now\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    df['income'] = df['income'].map({'<=50K': 0, '>50K': 1}).astype(int)\n",
    "    df = df.drop(columns=['fnlwgt', 'education.num', 'country', 'capgain', 'caploss'])\n",
    "    return df\n",
    "\n",
    "df_train = preprocess(df_train)\n",
    "df_test = preprocess(df_test)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "X_train_ = df_train.drop(columns='income')\n",
    "y_train_ = df_train['income']\n",
    "\n",
    "X_test_ = df_test.drop(columns='income')\n",
    "y_test_ = df_test['income']\n",
    "\n",
    "size=100\n",
    "X_train = X_train[0:size]\n",
    "y_train = y_train[0:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column passed:  workclass\n",
      "Val:  State-gov\n",
      "Column passed:  relationship\n",
      "Val:  Husband\n",
      "{'splitCol': 'relationship', 'numRows': 30162, 'infs': [-0.0021402477705084583, 0.00020016262055667553, 2.8032298761815304e-05, -0.0009761525574180404, 0.0031632358498914466, -0.0002699674198300297], 'vals': ['Not-in-family', 'Husband', 'Wife', 'Own-child', 'Unmarried', 'Other-relative'], 'idxs': [22436, 17699, 28756, 25696, 26950, 29273]}\n",
      "Depth:  1\n",
      "Column passed:  workclass\n",
      "Val:  Self-emp-not-inc\n",
      "Column passed:  occupation\n",
      "Val:  Exec-managerial\n",
      "Column passed:  education\n",
      "Val:  Bachelors\n",
      "Depth:  2\n",
      "Column passed:  workclass\n",
      "Val:  State-gov\n",
      "Column passed:  workclass\n",
      "Val:  Private\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Depth:  2\n",
      "Column passed:  workclass\n",
      "Val:  State-gov\n",
      "Column passed:  gender\n",
      "Val:  Male\n",
      "Column passed:  race\n",
      "Val:  White\n",
      "Depth:  2\n",
      "Column passed:  workclass\n",
      "Val:  State-gov\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Column passed:  marital\n",
      "Val:  Married-civ-spouse\n",
      "Depth:  2\n",
      "Column passed:  workclass\n",
      "Val:  State-gov\n",
      "Column passed:  workclass\n",
      "Val:  Private\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Column passed:  occupation\n",
      "Val:  Handlers-cleaners\n",
      "Depth:  2\n",
      "Column passed:  workclass\n",
      "Val:  State-gov\n",
      "Column passed:  occupation\n",
      "Val:  Adm-clerical\n",
      "Column passed:  marital\n",
      "Val:  Married-civ-spouse\n",
      "Depth:  2\n"
     ]
    }
   ],
   "source": [
    "def computeFairness(y_pred, X_test): \n",
    "    protected_idx = X_test[X_test['gender']=='Female'].index\n",
    "    numProtected = len(protected_idx)\n",
    "    privileged_idx = X_test[X_test['gender']=='Male'].index\n",
    "    numPrivileged = len(privileged_idx)\n",
    "    \n",
    "    p_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        p_protected += y_pred[protected_idx[i]][1]\n",
    "    p_protected /= len(protected_idx)\n",
    "    \n",
    "    p_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        p_privileged += y_pred[privileged_idx[i]][1]\n",
    "    p_privileged /= len(privileged_idx)\n",
    "    \n",
    "    spd = p_protected - p_privileged\n",
    "    return spd\n",
    "\n",
    "def getInfluenceOfSet(indices, f, X_train, y_train, X_test, X_test_, method): \n",
    "    del_f = 0\n",
    "    if (method == 1):\n",
    "        X = X_train.drop(index=indices, inplace=False)\n",
    "        y = y_train.drop(index=indices, inplace=False)\n",
    "        if len(y.unique()) < 2:\n",
    "            return 0\n",
    "        clf.fit(X, y)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        del_f = computeFairness(y_pred, X_test_)\n",
    "    elif (method == 2):\n",
    "        for i in range(len(indices)):\n",
    "            del_f += infs_1[indices[i]]\n",
    "    elif (method == 3):\n",
    "        del_f = second_order_influence(X_train, v1, indices, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "#     del_f = del_f * 100/f\n",
    "    return  del_f\n",
    "\n",
    "def getSplitVal(infs):\n",
    "    return (np.argmax(np.asarray(infs)))\n",
    "\n",
    "def getSplitAttribute(cols, cols_continuous, X_train, y_train, X_test, X_train_, X_test_, method):\n",
    "    splitCol, numRows, score = None, len(X_train_), np.Inf\n",
    "    infs = []\n",
    "    vals = []\n",
    "    idxs = []\n",
    "    for i in range(len(cols)):\n",
    "        col = cols[i]\n",
    "        infs_i = []\n",
    "        vals_i = []\n",
    "        idxs_i = []\n",
    "        if col not in cols_continuous:\n",
    "            colVals = X_train_[col].unique()\n",
    "            for val in colVals:\n",
    "                idx = X_train_[X_train_[col] == val].index\n",
    "                idxs_i.append(len(X_train)-len(idx))\n",
    "                infs_i.append(getInfluenceOfSet(idx, spd_0, X_train, y_train, X_test, X_test_, method))\n",
    "                vals_i.append(val)\n",
    "                ix = getSplitVal(infs_i)\n",
    "                if (abs(infs_i[ix]) < abs(spd_0)) and (abs(infs_i[ix]) < score):\n",
    "                    print(\"Column passed: \", col)\n",
    "                    print(\"Val: \", val)\n",
    "                    splitCol = col\n",
    "                    splitIdx = i\n",
    "                    score = abs(infs_i[ix])\n",
    "        infs.append(infs_i)\n",
    "        vals.append(vals_i)\n",
    "        idxs.append(idxs_i)\n",
    "    return {'splitCol':splitCol, 'numRows':numRows, \n",
    "            'infs': infs[splitIdx], 'vals': vals[splitIdx], 'idxs': idxs[splitIdx]}\n",
    "\n",
    "def partition(node, maxDepth, minSize, depth, cols, cols_continuous, \n",
    "              X_train_, y_train_, X_train, X_test_, X_test, method):\n",
    "    print(\"Depth: \", depth)\n",
    "    if depth >= maxDepth or node['numRows'] < minSize:\n",
    "        node['children'] = None\n",
    "        return\n",
    "    col = node['splitCol']\n",
    "    if col not in cols_continuous:\n",
    "        vals = X_train_[col].unique()\n",
    "        child = [None] * len(vals)\n",
    "        for i in range(len(vals)):\n",
    "            idx = X_train_[X_train_[col] == vals[i]].index \n",
    "            X = X_train.drop(index=idx, inplace=False)\n",
    "            y = y_train.drop(index=idx, inplace=False)\n",
    "            X_ = X_train_.drop(index=idx, inplace=False)\n",
    "            if len(X) < minSize:\n",
    "                node['children'] = None\n",
    "            else:\n",
    "                cols_ = copy.deepcopy(cols)\n",
    "                cols_.remove(col)\n",
    "                child[i] = getSplitAttribute(cols_, cols_continuous,\n",
    "                                             X, y, X_test, X_, X_test_, method)\n",
    "                child[i]['col'] = col\n",
    "                child[i]['val'] = vals[i]\n",
    "                partition(child[i], maxDepth, minSize, depth + 1, cols_, cols_continuous, \n",
    "                  X_, y, X, X_test_, X_test, method)\n",
    "    node['children'] = child\n",
    "\n",
    "def buildTree(X_train_, X_train, maxDepth, minSize, method):\n",
    "    cols = copy.deepcopy(X_train_.columns).tolist()\n",
    "    cols_continuous = ['age', 'hours']\n",
    "    X_train = pd.DataFrame(data=X_train, columns=X_train_orig.columns)\n",
    "    cols = list(set(cols) - set(cols_continuous))\n",
    "    root = getSplitAttribute(cols, cols_continuous,\n",
    "                             X_train, y_train, X_test, X_train_, X_test_, method)\n",
    "    print(root)\n",
    "    partition(root, maxDepth, minSize, 1, cols, cols_continuous,\n",
    "              X_train_, y_train_, X_train, X_test_, X_test, method)\n",
    "    return root\n",
    "\n",
    "method = 2\n",
    "dtree = buildTree(X_train_, X_train, 2, 100, method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking ground truth, first-order and second-order influences for a set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'education.num', 'hours', 'gender_Female', 'gender_Male',\n",
       "       'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black',\n",
       "       'race_Other', 'race_White', 'marital_Divorced',\n",
       "       'marital_Married-AF-spouse', 'marital_Married-civ-spouse',\n",
       "       'marital_Married-spouse-absent', 'marital_Never-married',\n",
       "       'marital_Separated', 'marital_Widowed', 'workclass_Federal-gov',\n",
       "       'workclass_Local-gov', 'workclass_Private', 'workclass_Self-emp-inc',\n",
       "       'workclass_Self-emp-not-inc', 'workclass_State-gov',\n",
       "       'workclass_Without-pay', 'relationship_Husband',\n",
       "       'relationship_Not-in-family', 'relationship_Other-relative',\n",
       "       'relationship_Own-child', 'relationship_Unmarried', 'relationship_Wife',\n",
       "       'occupation_Adm-clerical', 'occupation_Armed-Forces',\n",
       "       'occupation_Craft-repair', 'occupation_Exec-managerial',\n",
       "       'occupation_Farming-fishing', 'occupation_Handlers-cleaners',\n",
       "       'occupation_Machine-op-inspct', 'occupation_Other-service',\n",
       "       'occupation_Priv-house-serv', 'occupation_Prof-specialty',\n",
       "       'occupation_Protective-serv', 'occupation_Sales',\n",
       "       'occupation_Tech-support', 'occupation_Transport-moving'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-truth subset, Add 1st-order inf individual, Add ground-truth inf individual, Second-order subset influence\n",
      "-0.012317888893070866, -0.005774630703017444, 0, -0.010680725984522696\n",
      "0.00952634547908568, 0.006543852435894074, 0, 0.008479011774582136\n",
      "-3.3820441799892187e-07, -0.0011413329218123776, 0, -0.001073514495385736\n",
      "-0.004553130146451395, -0.0048770932021105756, 0, -0.004760677792922686\n",
      "-0.01248901276080941, -0.008781403243108075, 0, -0.009250741198171781\n",
      "-0.009277304028965289, -0.010472497579629148, 0, -0.011054044075865691\n",
      "-0.0015036286856289494, -0.0020227369035464033, 0, -0.0018726731170513673\n",
      "-0.00339556812189723, 0.0014816309586452368, 0, 0.0014824238552872979\n",
      "0.007309984763309824, 0.005526037294540163, 0, 0.007310177495959154\n",
      "-0.006599345171166976, -0.006057932833891032, 0, -0.0077428490498831935\n",
      "-0.02663702700573739, -0.022016371804897916, 0, -0.02638564402060233\n",
      "0.013524398191424508, 0.010320528889637779, 0, 0.013994549338615207\n",
      "0.019595244135551443, 0.012383200113762205, 0, 0.01918362734828579\n",
      "0.0232493360662758, 0.015285305831629835, 0, 0.022594439619195404\n",
      "0.014094640320290203, 0.008605890774995965, 0, 0.01287184484774402\n",
      "0.003948057122120346, 0.0006193330791706721, 0, 0.0027021615751603245\n",
      "-0.011879625493491447, -0.010898508330427903, 0, -0.012335399803803682\n",
      "0.0014852529302175344, -0.0036142013048099794, 0, -0.001554370839050104\n",
      "-0.005115566769769125, -0.0015732795433429194, 0, -0.002826575088557733\n",
      "-0.00018965408825424857, -0.007430506872544716, 0, -0.004334086911108374\n",
      "0.03292477271942307, 0.023659669334321284, 0, 0.03324839722928219\n",
      "0.020730553356165388, 0.011588980785092136, 0, 0.020052850863299575\n",
      "-0.00714232795248812, -0.009172810067823724, 0, -0.007404050495251446\n",
      "0.0020199017898447635, -0.0009461715268572068, 0, -0.0005916861556319534\n",
      "0.006278784936998322, 0.0071010594210508616, 0, 0.0067990414029039554\n",
      "0.001681443224673923, 0.00011894226967411988, 0, 0.0010645465934803094\n",
      "-0.0031800316453070576, 0.0006620750490976489, 0, -0.0012373139286106865\n",
      "-0.020655944912776253, -0.016898615161243868, 0, -0.020310102353579548\n",
      "-0.015214637389992541, -0.01006706043939674, 0, -0.013664194042672322\n",
      "0.008335680688071417, 0.008465927899786485, 0, 0.009504028522091583\n",
      "-0.004515301455201515, -0.006805228301487607, 0, -0.0058095927911025825\n",
      "-0.01572580007974575, -0.012212100273223604, 0, -0.015868192660053314\n",
      "0.008236266426073918, 0.005500929635110002, 0, 0.007709664281869448\n",
      "0.015970764955807043, 0.009343869979198186, 0, 0.01526694257484651\n",
      "0.021854907811162783, 0.01423157255521729, 0, 0.02221687487795311\n",
      "0.00309805600970528, 0.0037752516966404327, 0, 0.004099061785169191\n",
      "-0.002429067592962325, 0.005100651223524978, 0, 0.0012533397642996566\n",
      "-0.014862060140173522, -0.012638822637601426, 0, -0.01445174569776648\n",
      "-0.009617881124503846, -0.008099518382334338, 0, -0.009878580587315965\n",
      "0.019222625451769143, 0.009514417735939565, 0, 0.016856853028079963\n",
      "0.010270722144084393, 0.008195598553831037, 0, 0.00966267627407487\n",
      "0.015672216682908552, 0.008279596863055818, 0, 0.014831568526839834\n",
      "-0.020194151198157678, -0.012251498910322749, 0, -0.01767289560815952\n",
      "0.0015279054501839695, -0.002012235243913231, 0, 0.0005758392579111854\n",
      "0.007079585343668926, 0.00707201746392724, 0, 0.007584558607119851\n",
      "0.023028826243993467, 0.017729565129147935, 0, 0.022367628591411504\n",
      "-0.01173307753958866, -0.00747866604282381, 0, -0.010689378924727263\n",
      "-0.001817668374049758, -0.0009003171871630561, 0, -0.0013588188509506777\n",
      "0.0021779810138204403, 0.001348267936973198, 0, 0.0020231391563045207\n",
      "0.021804776254051056, 0.014731381348490316, 0, 0.02108526043222608\n",
      "0.00834751626682459, 0.0051127416566608885, 0, 0.00892875939043048\n",
      "-0.005679830378490042, -0.003192941946277895, 0, -0.004074693557807781\n",
      "-0.006588770167732405, -0.00557707664443511, 0, -0.005918127496151899\n",
      "0.0020223572352603136, 0.0014746826313261116, 0, -0.00019108534661059774\n",
      "-0.008521938766191534, -0.00459547385263106, 0, -0.0073940151745069925\n",
      "0.0033230680167253557, 0.0015474511679955793, 0, 0.002767406636183595\n",
      "0.01772949621863834, 0.007332664287718192, 0, 0.014121347219527285\n",
      "0.012195999295249005, 0.010019866520382847, 0, 0.012351289874990685\n",
      "0.01233564245238436, 0.008244823016385565, 0, 0.012699571811637802\n",
      "-0.007080761163844346, -0.004532617819077983, 0, -0.006994662598762368\n",
      "0.005062447701129819, 0.0030144407663235864, 0, 0.005462827354756116\n",
      "-0.027193448177948526, -0.020826042377316507, 0, -0.02803924594699771\n",
      "0.020203427789246914, 0.0203330132863082, 0, 0.02359754929352644\n",
      "-0.02425427098388025, -0.016934111590261796, 0, -0.02347071259705787\n",
      "9.914148001341494e-05, 0.002479825138270525, 0, -0.0012488202663309253\n",
      "0.002349810967917282, 0.0023741432715478654, 0, 0.0008232416542609968\n",
      "-0.010447103320151668, -0.008673820163237263, 0, -0.011322046481773363\n",
      "-0.004294621589471942, -0.00498148964274598, 0, -0.00597711871467194\n",
      "-0.004470105777030026, -0.005192425910527853, 0, -0.0036028934803591685\n",
      "-0.006435852976877127, -0.002119843230938062, 0, -0.002862867988738978\n",
      "0.0031863715072442955, 0.0003943547330800817, 0, 0.0026696626640106097\n",
      "0.01505131386449568, 0.01027548572613134, 0, 0.014135898323686807\n",
      "-0.013976150963043915, -0.011022934709162014, 0, -0.014638174773436999\n",
      "0.0077769289436577516, 0.007526674662702769, 0, 0.008439038347846031\n",
      "0.0020467421983858813, 0.0016056854196328301, 0, 0.0006135248334536331\n",
      "-0.010974539939731243, -0.009997988333218402, 0, -0.009553632896758054\n",
      "0.0032780844257012665, 0.0030730861165122357, 0, 0.0022954193948002545\n",
      "0.03563111573785191, 0.02366887248440374, 0, 0.03576956296153673\n",
      "0.0269669532636089, 0.01950285055122412, 0, 0.026088317148089388\n",
      "0.012337625897343912, 0.00901728087651907, 0, 0.012453471810452635\n",
      "-0.0068587387495119, -0.00904446822285044, 0, -0.010314986246615468\n",
      "-0.00473397994878777, -0.0019686499485757764, 0, -0.005308632685751175\n",
      "-0.014002878696784987, -0.007911713141105765, 0, -0.012550291979357786\n",
      "-0.03771178210135731, -0.0291761078624725, 0, -0.03923976020607433\n",
      "0.009611599789944997, 0.006865523814525874, 0, 0.01049606995465674\n",
      "-0.06133078225699354, -0.04535257987006544, 0, -0.06103358118809751\n",
      "0.026974766691318597, 0.019704611029564396, 0, 0.02745945165531208\n",
      "-0.0054851906099111225, -0.0034632755780678436, 0, -0.005245046344727491\n",
      "-0.024352311893561157, -0.016988638628744637, 0, -0.022141700243151326\n",
      "0.01613997492123767, 0.01103986995943805, 0, 0.015592479968160787\n",
      "-0.012729283068969921, -0.009571177859003733, 0, -0.012326157651224844\n",
      "-0.02278402824271708, -0.016376549140588226, 0, -0.022788860436602786\n",
      "0.005587013157448667, -0.0033008739771669938, 0, 0.0032377359111441847\n",
      "-0.002313985161377069, -0.0037736765320749996, 0, -0.003107158640759901\n",
      "0.0060848945159071854, 0.003351459988270653, 0, 0.0067938679772952615\n",
      "0.000712075886776381, -2.7211147662274438e-05, 0, 0.001037152605405949\n",
      "0.0008774168189932818, 0.0042732461082994905, 0, 0.0008932397166046416\n",
      "-0.0029016251860577713, -0.005296892723821474, 0, -0.005221057238778092\n",
      "-0.005491118948482615, -0.0014083844521656072, 0, -0.0036945076926738524\n",
      "0.010068473014868379, 0.00802317214113978, 0, 0.008960954147238319\n"
     ]
    }
   ],
   "source": [
    "# ground truth predicates\n",
    "# predicates = ['marital_Married-civ-spouse']\n",
    "predicates = ['marital_Never-married']\n",
    "idx = X_train_orig[(X_train_orig[predicates[0]] == 1)\n",
    "#                    & (X_train_orig[predicates[1]] == 1) \n",
    "                  ].index \n",
    "# print(predicates[0])\n",
    "# print(\"#Rows removed: \", len(idx))\n",
    "\n",
    "print(\"Ground-truth subset, Add 1st-order inf individual, Add ground-truth inf individual, Second-order subset influence\")\n",
    "for i in range(100):\n",
    "    idx = random.sample(range(1, len(X_train)), 100)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict_proba([X_test[ix]])[0]\n",
    "#     loss_ix = - y_test[ix] * math.log(y_pred[1]) - (1 - y_test[ix]) * math.log(y_pred[0])\n",
    "\n",
    "    X = np.delete(X_train, idx, 0)\n",
    "    y = y_train.drop(index=idx, inplace=False)\n",
    "    clf.fit(X, y)\n",
    "    y_pred_test = clf.predict_proba(X_test)\n",
    "    # print(\"Ground truth influence of subset: \", computeFairness(y_pred_test, X_test_orig)-spd_0)\n",
    "    inf_gt = computeFairness(y_pred_test, X_test_orig)-spd_0\n",
    "#     y_pred = clf.predict_proba([X_test[ix]])[0]\n",
    "#     inf_gt = - y_test[ix] * math.log(y_pred[1]) - (1 - y_test[ix]) * math.log(y_pred[0]) - loss_ix\n",
    "    \n",
    "    del_f_gt = 0\n",
    "    del_f_1 = 0\n",
    "    diff = []\n",
    "    for i in range(len(idx)):\n",
    "        del_f_1 += infs_1[idx[i]]\n",
    "#         del_f_gt += spdgt[idx[i]] #delta_loss_gt[idx[i]]\n",
    "    \n",
    "    size_hvp = 1\n",
    "    params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "    del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "    \n",
    "    print(inf_gt, del_f_1, del_f_gt, del_f_2, sep=\", \")\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.intercept_ += del_f_2[0]\n",
    "# for i in range(1, num_params):\n",
    "#     clf.coef_.transpose()[i-1][0] += del_f_2[i]\n",
    "# # Mostly highly correlated, a few closer to zero have flipped signs for the actual\n",
    "# # and the computed numbers \n",
    "# gt_idx = []\n",
    "# infs_idx = []\n",
    "# tol = 0.00001\n",
    "# for i in range(len(idx)):\n",
    "#     gt_idx.append(spdgt[idx[i]])\n",
    "#     infs_idx.append(infs_1[idx[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-truth subset, Add 1st-order inf individual, Add ground-truth inf individual, Second-order subset influence\n",
      "-0.004836231102632177, -0.004067099800826107, 0, -0.004891873612879793, 8\n",
      "-0.0032456686746517305, -0.0022990364704849353, 0, -0.002819365152913278, 10\n",
      "0.0031240190584012895, 0.0026550639706884316, 0, 0.002967348969806111, 18\n",
      "-0.007052321904313874, -0.006559115278019833, 0, -0.007108155662394483, 8\n",
      "0.007294960646163112, 0.0063103668141702065, 0, 0.0072545763201799975, 16\n",
      "0.016972839671325873, 0.0156974081919933, 0, 0.017149853159910706, 13\n",
      "0.004074405271264098, 0.002902256290133017, 0, 0.003761645660257173, 10\n",
      "-0.0016071941676318036, -0.0014429001029589964, 0, -0.0015962119966324, 6\n",
      "0.0044186793346371045, 0.00411229704395347, 0, 0.004097005074633314, 14\n",
      "0.017615166302909574, 0.014672176952190061, 0, 0.017394065325646055, 11\n",
      "-0.006482957325324029, -0.005451695348386307, 0, -0.006211082978151962, 16\n",
      "-0.0014488125319288558, -0.0013528871276953368, 0, -0.0014754312013733552, 14\n",
      "-0.003563716707153136, -0.003596056659375411, 0, -0.003610539450752487, 10\n",
      "0.007091555319274939, 0.006450353357501409, 0, 0.007115220780880932, 16\n",
      "0.0004960540959720039, 0.0005416345115399974, 0, 0.0005401965807797221, 14\n",
      "-0.011661364169725824, -0.010541011818699982, 0, -0.011711389723975106, 15\n",
      "0.0036446680729480263, 0.0022865231280410825, 0, 0.0031047803411009553, 13\n",
      "0.0018913420992691665, 0.0014806685707696877, 0, 0.0017960813254229552, 5\n",
      "0.0041761801625348205, 0.002829581598701971, 0, 0.0034010645222103935, 13\n",
      "-0.00256117699492156, -0.0022072582757117496, 0, -0.0025247106679790058, 6\n",
      "-0.0008174057449369665, -0.0008085547041064367, 0, -0.0008992228448246302, 5\n",
      "0.00037171391020748845, 8.63200905854463e-05, 0, 0.00017689534099188373, 11\n",
      "-0.001851401277235848, -0.0017186341372059236, 0, -0.001872562950658089, 11\n",
      "-0.004989456381280205, -0.005260496479135399, 0, -0.005297489418770221, 12\n",
      "-7.115076524985553e-05, -4.607831634765644e-05, 0, -5.4760538727943824e-05, 10\n",
      "0.00042503920601164036, 0.00020922538254433058, 0, 0.00035881604252964525, 7\n",
      "8.580971438926221e-05, 8.468376866704439e-05, 0, 8.656187339496979e-05, 7\n",
      "-0.004594292867249022, -0.004359289036645479, 0, -0.004610366489829017, 15\n",
      "-0.004863897831309472, -0.004284358400356507, 0, -0.004191313730298388, 5\n",
      "-0.00042675611818940307, -0.0010123840651351634, 0, -0.0002515131398675264, 18\n",
      "0.0018375328457926055, 0.0012661917802551832, 0, 0.001917228085839649, 17\n",
      "0.0009861737152434547, 0.0009039154233577296, 0, 0.0009416599296291111, 15\n",
      "-0.004357197354705772, -0.004056228943627215, 0, -0.004478730019642398, 9\n",
      "0.0015857655021874195, 0.001316844285546695, 0, 0.001300610444678303, 14\n",
      "-0.008731767339932056, -0.00854373167595481, 0, -0.00881896759227413, 14\n",
      "1.080716319509789e-05, 0.00021692070160017583, 0, 0.0003196336451594684, 1\n",
      "0.0009388425222776331, 0.000864245063651007, 0, 0.0009185677489605902, 13\n",
      "-2.9140145722444943e-05, -2.795908955488432e-05, 0, -2.809840002829121e-05, 6\n",
      "-0.002385837585195283, -0.0025017455236089935, 0, -0.002535591168177206, 15\n",
      "-0.0018702191611350005, -0.0014987749048231626, 0, -0.0018069877621644907, 2\n",
      "0.001171696929917504, 0.0011074479770886239, 0, 0.0012022648636827882, 3\n",
      "-0.0018554957417042384, -0.001560271945185119, 0, -0.0018222466790392218, 2\n",
      "-9.816079453245363e-06, -7.819671419992696e-05, 0, -9.649888941485414e-05, 1\n",
      "-0.0002931470224546706, -0.0002690164460868369, 0, -0.0002907695179133553, 2\n",
      "8.312300837726161e-06, 6.041493832717174e-06, 0, 6.282428498512476e-06, 1\n",
      "-0.0008146697944430292, -0.0007435230460880297, 0, -0.0008584498080882134, 3\n",
      "0.003053409744806168, 0.0032153148564033856, 0, 0.0032692875377533077, 3\n",
      "0.00039912052161067413, 0.0005982796045690448, 0, 0.00048123755832999175, 6\n",
      "-0.0011136151933568705, -0.0010478824632108971, 0, -0.0011201843068100559, 11\n",
      "7.181071425749197e-06, -1.2187776648747101e-05, 0, -1.2261722967516748e-05, 7\n",
      "0.005427777114699994, 0.005032388235977607, 0, 0.005705145543627022, 4\n",
      "-9.03641252378573e-05, -9.013427180452934e-05, 0, -9.869130542097647e-05, 1\n",
      "3.482204321655913e-05, 4.0525486624190554e-06, 0, 4.3674686435617946e-06, 1\n",
      "0.0003009215224254458, 0.0002752469625146374, 0, 0.0002834602891282344, 3\n",
      "-0.0009882200157392551, -0.0008945559539833091, 0, -0.00098856531791066, 1\n",
      "0.00036554543277178553, 0.0003441637470905051, 0, 0.0003427254153716185, 3\n",
      "-0.00018943010947750083, -0.00011530706389005138, 0, -0.00016818068937588473, 2\n",
      "0.0008281134339803708, 0.0008059976814410513, 0, 0.0008382250735383958, 2\n",
      "0.00041264921942826827, 0.00039576906444473473, 0, 0.0004150013753801675, 1\n",
      "-0.004370968584770318, -0.003987270175392903, 0, -0.00523685858998597, 82\n",
      "-0.011652962226548325, -0.009042311827528291, 0, -0.012267967367593233, 157\n",
      "-0.002805995540433842, -0.0026239681044321426, 0, -0.002855017295764003, 21\n",
      "-0.005483127536896698, -0.00439784705493657, 0, -0.006833842694163661, 30\n",
      "-0.0015188494431877808, -0.0013950035216118036, 0, -0.001594661436207306, 9\n",
      "0.016351143920744687, 0.01268139324795294, 0, 0.0166833298122585, 118\n",
      "-0.00023188130464307788, -0.0006997253735880407, 0, -0.00034759251843927743, 25\n",
      "0.00515549228351822, 0.003997380335933692, 0, 0.004927683207599233, 7\n",
      "-0.0007826558233581249, -0.000645486530051936, 0, -0.0009481496599295597, 7\n",
      "0.0004969864564113025, -0.0005674996624040274, 0, -0.00014592182873710588, 20\n",
      "0.0036442583224471636, 0.0034534776959913903, 0, 0.0037758730145110043, 6\n",
      "-0.00028737113625246224, -0.00027747466475303674, 0, -0.00028413202247038475, 6\n",
      "0.00022059721450937153, 0.00018739364225155218, 0, 0.0001935435950688106, 6\n",
      "8.456387145364119e-05, 8.241179926515593e-05, 0, 8.362745340813592e-05, 1\n",
      "0.003365135965309435, 0.0035239039331557745, 0, 0.003574769199488674, 2\n",
      "-0.0004784002737387161, -6.436648169452547e-05, 0, 0.0003692873105337573, 3\n",
      "0.062445672053679346, 0.020071940273106624, 0, 0.05684880659752484, 256\n",
      "-0.002127315298702831, -0.0019046874200343426, 0, -0.002144686955779026, 1\n",
      "-0.0030666901871005126, -0.0026077751992519293, 0, -0.0029989071138362307, 5\n",
      "-0.0029216444772866557, -0.003418705718044496, 0, -0.0034155760625465105, 30\n",
      "-0.018897807569860398, -0.013874723889285618, 0, -0.018163223220290615, 39\n",
      "-0.0005125951868517886, -0.0005054798604996467, 0, -0.0006155657027163476, 4\n",
      "-0.0013632547589558508, -0.0008925482140656255, 0, -0.0005470803075040924, 12\n",
      "0.0022299571181844113, 0.0020507827247718533, 0, 0.0022361137229759483, 19\n",
      "0.0030415087246093153, 0.0025641455064481927, 0, 0.00293537502143843, 24\n",
      "-1.8924373253181859e-06, 5.5476019650728604e-05, 0, 0.00014599957750625748, 15\n",
      "-0.0004628381908329815, -0.00042014803993419415, 0, -0.0004656410936291004, 1\n",
      "-1.9712704598662256e-06, -1.4405756231297195e-06, 0, -1.4474979635518109e-06, 1\n",
      "0.0004998546118828717, 0.00046659829310323605, 0, 0.0004944571243321824, 5\n",
      "-0.0017628713160481035, -0.0017529503657210945, 0, -0.0019398363395700473, 11\n",
      "-0.0009233822040716488, -0.000916870239329675, 0, -0.0009669138265389211, 2\n",
      "-0.0002668799053483495, -0.00031951782976863896, 0, -0.0003494365131231367, 6\n",
      "0.0007826821488444435, 0.0008738607058105751, 0, 0.0007464751443235187, 8\n",
      "-0.0004459028361137596, -0.0002522232286334975, 0, -0.00045850468455329056, 6\n",
      "0.0031858517443732193, 0.0026674163904533634, 0, 0.0031586170798180552, 2\n",
      "-2.1289308750616076e-06, -5.813556557028377e-06, 0, -5.8869033061934406e-06, 1\n",
      "0.002936791750758505, 0.0025458574198460988, 0, 0.0028665281216665494, 2\n",
      "-1.126357635103381e-05, 1.972025486849152e-06, 0, 1.9750429828804903e-06, 1\n",
      "-0.0023209669345662653, -0.002139182686871953, 0, -0.0023287705006924835, 2\n",
      "-7.680399966861207e-05, -8.513028628648441e-05, 0, -8.589131641532114e-05, 1\n",
      "0.0006729073254701234, 0.0006359289589167764, 0, 0.0006755092835504673, 3\n",
      "0.002971973994171262, 0.0027798780779960375, 0, 0.0029726427365709394, 3\n",
      "-0.0011916804506310053, -0.0011107544938358943, 0, -0.0013020068425463353, 8\n",
      "-0.00155988095148199, -0.001489264941885895, 0, -0.001549944278116049, 2\n",
      "8.312300837726161e-06, 6.041493832717174e-06, 0, 6.282428498512476e-06, 1\n",
      "0.0019202793470987078, 0.0017135824218671642, 0, 0.0018656525641824896, 7\n",
      "1.5786201940648503e-06, -6.806522469293674e-07, 0, -6.84119219794984e-07, 3\n",
      "-0.0021312391432080913, -0.002024695437939552, 0, -0.002142685987448391, 2\n",
      "0.00014206212721093459, 0.00011983122213066691, 0, 0.00013871905673414035, 1\n",
      "6.549933731395119e-05, 6.901639467839853e-05, 0, 6.961514690143941e-05, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00023543209696372247, 0.00021464435240852418, 0, 0.0002284552884771403, 1\n",
      "-0.003268158261378745, -0.0020200701013168773, 0, -0.0026628014352210383, 1\n",
      "0.0005861547964124691, 0.0005716459108165312, 0, 0.0005938874704790451, 1\n",
      "0.0032109690333780028, 0.0024419179982419067, 0, 0.003132698634688322, 1\n",
      "-0.0020988959142932695, -0.0017395271739010186, 0, -0.0020430324118568743, 3\n",
      "-0.003811278113687505, -0.0033920443375536962, 0, -0.0036773671080611594, 2\n",
      "0.0009200509346797003, 0.0008682393588215706, 0, 0.0009409853035727282, 1\n",
      "3.4082896678555574e-06, 2.752145930703167e-06, 0, 2.75782179539664e-06, 1\n",
      "-1.2427743919712908e-05, -9.583061614831493e-08, 0, -9.586148674286677e-08, 1\n",
      "-9.247806100981526e-06, -1.025497856232271e-05, 0, -1.045224494603915e-05, 1\n",
      "-5.337632277280235e-06, -7.704442840356912e-06, 0, -7.789389006034371e-06, 1\n",
      "0.00041264921942826827, 0.00039576906444473473, 0, 0.0004150013753801675, 1\n",
      "0.13066127381493242, -2.9799049067642684e-06, 0, -0.0018488185147980027, 336\n",
      "0.04641795297010162, 0.00022798716306398953, 0, 0.000980477375165619, 164\n",
      "0.13066127381493242, -2.9799049067642684e-06, 0, -0.0018488185147980027, 336\n",
      "0.04641795297010162, 0.00022798716306398953, 0, 0.000980477375165619, 164\n",
      "-0.0007512653706045758, -0.00011426953560962158, 0, -0.000230158758963392, 4\n",
      "-0.3497113648854039, -0.011360782393965553, 0, 0.8988954187707108, 487\n",
      "0.014221278521613506, 0.01158578965212275, 0, 0.013475856753812717, 13\n",
      "-0.03963993240633393, -0.002628704593073904, 0, -0.0015696069991057654, 440\n",
      "0.0035880862745173436, 0.002853711851231185, 0, 0.004286415362255411, 60\n",
      "-0.0002832897088939357, -5.029098225368481e-06, 0, -1.0067680829429833e-05, 5\n",
      "-0.1025441279916304, -0.014095195611361702, 0, -0.07581451255207977, 418\n",
      "0.018491536030141448, 0.014320202869518947, 0, 0.01893363533964308, 82\n",
      "0.13300910949545422, -0.005358522514229703, 0, 0.09570582079810827, 428\n",
      "0.0313819526516505, 0.005583529772386926, 0, 0.00997836913148184, 72\n",
      "0.0003802687554091688, 8.641358182285452e-05, 0, 0.00017317351066704401, 1\n",
      "0.11028377542270182, -0.0004440515741287945, 0, -0.0013865942005661413, 269\n",
      "0.17015669577986925, 0.0006690588322860245, 0, -0.0008004650897509655, 231\n",
      "0.0005956682081095788, 3.8777435097759225e-05, 0, 7.866308946299131e-05, 9\n",
      "-0.0025732200473664035, -0.006697294265704798, 0, -0.00919661767545174, 165\n",
      "0.2099640660510823, 0.00692230152386201, 0, 0.023683643856166836, 335\n",
      "0.0021896067897498595, 0.00026781550994976175, 0, 0.0005493259983103737, 12\n",
      "0.007132271499162041, 0.00027670639231869537, 0, 0.0005669236026374643, 10\n",
      "0.08218960686440138, 0.006513321487382921, 0, -0.44981438430766385, 483\n",
      "-0.0034671640487741184, -0.006288314229225672, 0, -0.007489247181773885, 17\n",
      "-0.07717919453646896, -0.008280251989221367, 0, 0.031559926580606155, 460\n",
      "0.011186413857676797, 0.008505259247378622, 0, 0.01047661738637397, 40\n",
      "-0.00844054440018746, -0.011402777029872996, 0, -0.01643690977311287, 131\n",
      "0.01920764661601973, 0.011627784288030248, 0, 0.04102851726557894, 369\n",
      "-0.09158471123725373, -0.006880280619440823, 0, 1.4690825824600717, 484\n",
      "0.013738927813398166, 0.0071052878775980785, 0, 0.009067447196014431, 16\n",
      "0.2450423136413755, 0.019471827369040243, 0, 0.06758385632972148, 463\n",
      "-0.035191825071906796, -0.01924682011088301, 0, -0.022090962223439106, 37\n",
      "-0.0019789940496329206, -0.0014781898147410105, 0, -0.001873248097229619, 21\n",
      "0.036916799868631, 0.0017031970728982506, 0, -0.042110529507211385, 479\n",
      "0.17043255882254196, 0.0011552815310825065, 0, 0.0007647836143551493, 307\n",
      "0.11531842040359774, -0.0009302742729252781, 0, -0.002118490930192638, 193\n",
      "-0.0490172617817303, -0.014934544571720927, 0, -0.037641042898846344, 149\n",
      "0.20719614191748095, 0.015159551829878142, 0, -0.043081394322071725, 351\n",
      "0.11133922919743731, -0.000899595634790887, 0, -0.03687409940927202, 487\n",
      "0.002652460238696225, 0.0011246028929481381, 0, 0.0014891763414844983, 13\n",
      "0.0013471787127498358, 0.00016043634395131073, 0, 0.00036890639358709836, 62\n",
      "-0.0339546019416217, -0.013532074104007029, 0, 0.025584320394653217, 451\n",
      "0.018134954993868663, 0.013757081362164277, 0, 0.016794729029017234, 49\n",
      "0.06544081884556804, -0.0008226982455824107, 0, 0.1278401650227188, 466\n",
      "-0.029950864931762994, 0.001047705503739696, 0, 0.0013646910645843773, 34\n",
      "0.019839060502567646, 0.013785246505107651, 0, 0.01785931030080856, 51\n",
      "-0.015810608958589184, -0.013560239246950441, 0, 0.058199622152737336, 449\n",
      "-6.822935864145285e-05, -6.389896135456391e-05, 0, -0.00012805403077067014, 1\n",
      "0.14781847468172113, 0.008507754249325044, 0, 0.09358418233336724, 431\n",
      "-0.005079419760255416, -0.008282746991167803, 0, -0.008774291020549822, 69\n",
      "-0.1024354319333933, -0.009942019330078019, 0, 0.07219371677932282, 438\n",
      "0.03531061810445943, 0.01016702658823524, 0, 0.015105463229949176, 62\n",
      "0.10736738807549259, -0.0015943514848301818, 0, -0.05813667742339504, 486\n",
      "-0.0010860738436127282, 0.0018193587429874217, 0, 0.001804551531813142, 14\n",
      "-0.005663772271558315, -0.00048673873373473406, 0, -0.001021756064799685, 24\n",
      "0.17681216231216473, 0.0016332350390694844, 0, 0.13541903859154159, 469\n",
      "0.005045573177372115, -0.001408227780912241, 0, -0.0008934184044940681, 31\n",
      "0.09862325781593195, 0.0023906732946663897, 0, 0.018974418715407323, 441\n",
      "-0.0023087440141538462, -0.002165666036509139, 0, -0.002430389810353267, 59\n",
      "7.048922431243665e-05, 0.0004680407733891448, 0, 0.0009379574617016949, 1\n",
      "0.15859992596890113, 0.013885948612100152, 0, -0.014595548213452936, 435\n",
      "-0.009830348066764999, -0.013660941353942921, 0, -0.018364369981146527, 65\n",
      "-0.17992372256975295, -0.0007650495042483337, 0, 0.10218775954715452, 488\n",
      "0.0049723884127557405, 0.0009900567624055823, 0, 0.001052938631204482, 12\n",
      "0.058602585822804296, 0.00209636126885294, 0, 0.021904700694828583, 441\n",
      "-0.005916578493965291, -0.0018713540106956815, 0, -0.0020722529775321352, 59\n",
      "0.015203584690140187, 0.0006582994432563966, 0, 0.11325094622286289, 478\n",
      "0.0005429298877011879, -0.00043329218509914337, 0, -0.00029540696287084096, 22\n",
      "0.14357933899685826, -0.0011431366812911824, 0, -0.0039008592625750967, 470\n",
      "-0.010587203917795524, 0.0013681439394484338, 0, 0.0011626158350368876, 30\n"
     ]
    }
   ],
   "source": [
    "print(\"Ground-truth subset, Add 1st-order inf individual, Add ground-truth inf individual, Second-order subset influence\")\n",
    "clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict_proba([X_test[ix]])[0]\n",
    "# loss_ix = - y_test[ix] * math.log(y_pred[1]) - (1 - y_test[ix]) * math.log(y_pred[0])\n",
    "for col in X_train_orig.columns:\n",
    "    vals = X_train_orig[col].unique()\n",
    "    for val in vals:\n",
    "#         print(col, val, sep=\": \")\n",
    "        idx = X_train_orig[X_train_orig[col] == val].index \n",
    "        X = np.delete(X_train, idx, 0)\n",
    "        y = y_train.drop(index=idx, inplace=False)\n",
    "        if len(y.unique())>1:\n",
    "            clf.fit(X, y)\n",
    "            y_pred = clf.predict_proba(X_test)\n",
    "#             y_pred = clf.predict_proba([X_test[ix]])[0]\n",
    "#             inf_gt = - y_test[ix] * math.log(y_pred[1]) - (1 - y_test[ix]) * math.log(y_pred[0]) - loss_ix\n",
    "            inf_gt = computeFairness(y_pred, X_test_orig)-spd_0\n",
    "            del_f_gt = 0\n",
    "            del_f_1 = 0\n",
    "            diff = []\n",
    "            for i in range(len(idx)):\n",
    "                del_f_1 += infs_1[idx[i]]\n",
    "#                 del_f_gt += delta_loss_gt[idx[i]] #spdgt[idx[i]]\n",
    "\n",
    "            size_hvp = 1\n",
    "            params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "            del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "\n",
    "            print(inf_gt, del_f_1, del_f_gt, del_f_2, len(idx), sep=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_influence(X_train, U, size, del_L_del_theta, hessian_all_points):\n",
    "    u = len(U)\n",
    "    s = len(X_train)\n",
    "    p = u/s\n",
    "    c1 = (1 - 2*p)/(s * (1-p)**2)\n",
    "    c2 = 1/((s * (1-p))**2)\n",
    "    num_params = len(del_L_del_theta[0])\n",
    "    del_L_del_theta_hinv = np.zeros((num_params, 1))\n",
    "    del_L_del_theta_sum = np.zeros((num_params, 1))\n",
    "    hessian_U = np.zeros((num_params, num_params))\n",
    "    for i in range(u):\n",
    "        idx = U[i]\n",
    "        hessian_U = np.add(hessian_U, s * hessian_all_points[idx])\n",
    "        del_L_del_theta_sum = np.add(del_L_del_theta_sum, del_L_del_theta[idx])\n",
    "    \n",
    "    hinv_del_L_del_theta= np.matmul(hinv_exact, del_L_del_theta_sum)\n",
    "    hinv_hessian_U = np.matmul(hinv_exact, hessian_U)\n",
    "    term1 = c1 * hinv_del_L_del_theta\n",
    "    term2 = c2 * np.matmul(hinv_hessian_U, hinv_del_L_del_theta)\n",
    "    sum_term = np.add(term1, term2)\n",
    "    return sum_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
