{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['status', 'duration', 'credit_hist', 'purpose', 'credit_amt', 'savings', 'employment', 'install_rate', 'personal_status', 'debtors', 'residence', 'property', 'age', 'install_plans', 'housing', 'num_credits', 'job', 'num_liable', 'telephone', 'foreign_worker', 'credit']\n",
    "df = pd.read_table('german.data', names=cols, sep=\" \", index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing** (categorical to numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    " def preprocess(df):\n",
    "    df['status'] = df['status'].map({'A11': 0, 'A12': 1, 'A13': 2, 'A14': 3}).astype(int)\n",
    "    \n",
    "    df.loc[(df['duration'] <= 12), 'duration'] = 0\n",
    "    df.loc[(df['duration'] > 12) & (df['duration'] <= 24), 'duration'] = 1\n",
    "    df.loc[(df['duration'] > 24) & (df['duration'] <= 36), 'duration'] = 2\n",
    "    df.loc[(df['duration'] > 36), 'duration'] = 3    \n",
    "    \n",
    "    df['credit_hist'] = df['credit_hist'].map({'A34': 0, 'A33': 1, 'A32': 2, 'A31': 3, 'A30': 4}).astype(int)    \n",
    "    df = pd.concat([df, pd.get_dummies(df['purpose'], prefix='purpose')],axis=1)\n",
    "\n",
    "    df.loc[(df['credit_amt'] <= 2000), 'credit_amt'] = 0\n",
    "    df.loc[(df['credit_amt'] > 2000) & (df['credit_amt'] <= 5000), 'credit_amt'] = 1\n",
    "    df.loc[(df['credit_amt'] > 5000), 'credit_amt'] = 2    \n",
    "    \n",
    "    df['savings'] = df['savings'].map({'A61': 0, 'A62': 1, 'A63': 2, 'A64': 3, 'A65': 4}).astype(int)\n",
    "    df['employment'] = df['employment'].map({'A71': 0, 'A72': 1, 'A73': 2, 'A74': 3, 'A75': 4}).astype(int)    \n",
    "    df['gender'] = df['personal_status'].map({'A91': 1, 'A92': 0, 'A93': 1, 'A94': 1, 'A95': 0}).astype(int)\n",
    "    df['debtors'] = df['debtors'].map({'A101': 0, 'A102': 1, 'A103': 2}).astype(int)\n",
    "    df['property'] = df['property'].map({'A121': 3, 'A122': 2, 'A123': 1, 'A124': 0}).astype(int)        \n",
    "    df['age'] = df['age'].apply(lambda x : 1 if x >= 45 else 0) # 1 if old, 0 if young\n",
    "    df['install_plans'] = df['install_plans'].map({'A141': 1, 'A142': 1, 'A143': 0}).astype(int)\n",
    "    df = pd.concat([df, pd.get_dummies(df['housing'], prefix='housing')],axis=1)\n",
    "    df['job'] = df['job'].map({'A171': 0, 'A172': 1, 'A173': 2, 'A174': 3}).astype(int)    \n",
    "    df['telephone'] = df['telephone'].map({'A191': 0, 'A192': 1}).astype(int)\n",
    "    df['foreign_worker'] = df['foreign_worker'].map({'A201': 1, 'A202': 0}).astype(int)\n",
    "    \n",
    "    df['credit'] = df['credit'].replace(2, 0) #1 = Good, 2= Bad credit risk\n",
    "\n",
    "#     process age\n",
    "#     df.loc[(df['age'] >= 15) & (df['age'] <= 24) , 'age'] = 0\n",
    "#     df.loc[(df['age'] >= 25) & (df['age'] <= 34) , 'age'] = 1\n",
    "#     df.loc[(df['age'] >= 35) & (df['age'] <= 44) , 'age'] = 2\n",
    "#     df.loc[(df['age'] >= 45) & (df['age'] <= 54) , 'age'] = 3\n",
    "#     df.loc[(df['age'] >= 55) & (df['age'] <= 64) , 'age'] = 4\n",
    "#     df.loc[(df['age'] >= 65) , 'age'] = 5\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess(df)\n",
    "\n",
    "y = df['credit']\n",
    "df = df.drop(columns=['purpose', 'personal_status', 'housing', 'credit'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=1)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Protected, privileged**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protected: 'gender'=0\n",
    "# privileged: 'gender'=1\n",
    "\n",
    "# protected: 'age'=0\n",
    "# privileged: 'age'=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size=500\n",
    "# X_train = X_train[0:size]\n",
    "# y_train = y_train[0:size]\n",
    "\n",
    "X_train_orig = copy.deepcopy(X_train)\n",
    "X_test_orig = copy.deepcopy(X_test)\n",
    "\n",
    "# Scale data: regularization penalty default: ‘l2’, ‘lbfgs’ solvers support only l2 penalties. \n",
    "# Regularization makes the predictor dependent on the scale of the features.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute fairness metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFairness(y_pred, X_test, y_test, metric): \n",
    "    fairnessMetric = 0\n",
    "    protected_idx = X_test[X_test['age']==0].index\n",
    "    numProtected = len(protected_idx)\n",
    "    privileged_idx = X_test[X_test['age']==1].index\n",
    "    numPrivileged = len(privileged_idx)\n",
    "        \n",
    "    p_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        p_protected += y_pred[protected_idx[i]][1]\n",
    "    p_protected /= len(protected_idx)\n",
    "    \n",
    "    p_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        p_privileged += y_pred[privileged_idx[i]][1]\n",
    "    p_privileged /= len(privileged_idx)\n",
    "    \n",
    "    # statistical parity difference\n",
    "    statistical_parity = p_protected - p_privileged\n",
    "    \n",
    "    # equality of opportunity, or \n",
    "    # true positive rate parity\n",
    "    # P(Y=1 | Y=1, G=0)- P(Y=1 | Y=1, G=1)\n",
    "    true_positive_protected = 0\n",
    "    actual_positive_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            actual_positive_protected += 1\n",
    "#             if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "            true_positive_protected += y_pred[protected_idx[i]][1]\n",
    "    tpr_protected = true_positive_protected/actual_positive_protected\n",
    "    \n",
    "    true_positive_privileged = 0\n",
    "    actual_positive_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            actual_positive_privileged += 1\n",
    "#             if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "            true_positive_privileged += y_pred[privileged_idx[i]][1]\n",
    "    tpr_privileged = true_positive_privileged/actual_positive_privileged\n",
    "    \n",
    "    tpr_parity = tpr_protected - tpr_privileged\n",
    "    \n",
    "    # equalized odds or TPR parity + FPR parity\n",
    "    # false positive rate parity\n",
    "    \n",
    "    # predictive parity\n",
    "    p_o1_y1_s1 = 0\n",
    "    p_o1_s1 = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "#         if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "        p_o1_s1 += y_pred[protected_idx[i]][1]\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            p_o1_y1_s1 += y_pred[protected_idx[i]][1]\n",
    "    ppv_protected = p_o1_y1_s1/p_o1_s1\n",
    "    \n",
    "    p_o1_y1_s0 = 0\n",
    "    p_o1_s0 = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "#         if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "        p_o1_s0 += y_pred[privileged_idx[i]][1]\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            p_o1_y1_s0 += y_pred[privileged_idx[i]][1]\n",
    "    ppv_privileged = p_o1_y1_s0/p_o1_s0\n",
    "    \n",
    "    predictive_parity = ppv_protected - ppv_privileged\n",
    "    \n",
    "    if (metric == 0):\n",
    "        fairnessMetric = statistical_parity\n",
    "    elif (metric == 1):\n",
    "        fairnessMetric = tpr_parity\n",
    "    elif (metric == 2):\n",
    "        fairnessMetric = predictive_parity\n",
    "        \n",
    "    return fairnessMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Influence of points computed using ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_influence(X_train, y_train, X_test, X_test_orig, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    spd_0 = computeFairness(y_pred, X_test_orig, y_test, 0)\n",
    "\n",
    "    delta_spd = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_removed = np.delete(X_train, i, 0)\n",
    "        y_removed = y_train.drop(index=i, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        delta_spd_i = computeFairness(y_pred, X_test_orig, y_test, 0) - spd_0\n",
    "        delta_spd.append(delta_spd_i)\n",
    "    \n",
    "    return delta_spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function** (Log loss for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if (y_pred[i][1] != 0 and y_pred[i][0] != 0):\n",
    "            loss += - y_true[i] * math.log(y_pred[i][1]) - (1 - y_true[i]) * math.log(y_pred[i][0])\n",
    "    loss /= len(y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Accuracy** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def computeAccuracy(y_true, y_pred):\n",
    "    accuracy = 0\n",
    "    for i in range(len(y_true)):\n",
    "        idx = y_true[i]\n",
    "        if (y_pred[i][idx] > y_pred[i][1 - idx]):\n",
    "            accuracy += 1\n",
    "#         accuracy += y_pred[i][idx]\n",
    "    accuracy /= len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of loss function at z with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_L_del_theta_i(num_params, y_true, x, y_pred):\n",
    "#     del_L_del_theta = np.ones((num_params, 1)) * ((1 - y_true) * y_pred[1] - y_true * y_pred[0])\n",
    "    del_L_del_theta = np.ones((num_params, 1)) * (- y_true + y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_L_del_theta[j] *=  x[j-1]\n",
    "    return del_L_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessian: Second-order partial derivative of loss function with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_one_point(num_params, x, y_pred):\n",
    "    H = np.ones((num_params, num_params)) * (y_pred[0] * y_pred[1])\n",
    "    for i in range(1, num_params):\n",
    "        for j in range(i + 1):\n",
    "            if j == 0:\n",
    "                H[i][j] *= x[i-1]\n",
    "            else:\n",
    "                H[i][j] *= x[i-1] * x[j-1] \n",
    "    i_lower = np.tril_indices(num_params, -1)\n",
    "    H.T[i_lower] = H[i_lower]     \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of $P(y \\mid \\textbf{x})$ with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_f_del_theta_i(num_params, x, y_pred):\n",
    "    del_f_del_theta = np.ones((num_params, 1)) * (y_pred[0] * y_pred[1])\n",
    "    for j in range(1, num_params):\n",
    "            del_f_del_theta[j] *=  x[j-1]\n",
    "    return del_f_del_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($Statistical parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(SPD)/del(theta)\n",
    "def del_spd_del_theta(num_params, X_test_orig, X_test, y_pred):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    numPrivileged = X_test_orig['age'].sum()\n",
    "    numProtected = len(X_test_orig) - numPrivileged\n",
    "    for i in range(len(X_test)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[i], y_pred[i])\n",
    "        if X_test_orig.iloc[i]['age'] == 1: #privileged\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "        elif X_test_orig.iloc[i]['age'] == 0:\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_privileged /= numPrivileged\n",
    "    del_f_protected /= numProtected\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($TPR parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(TPR_parity)/del(theta)\n",
    "def del_tpr_parity_del_theta(num_params, X_test_orig, X_test, y_pred, y_test):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    \n",
    "    protected_idx = X_test_orig[X_test_orig['age']==0].index\n",
    "    privileged_idx = X_test_orig[X_test_orig['age']==1].index\n",
    "\n",
    "    actual_positive_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            actual_positive_privileged += 1\n",
    "#             if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "            del_f_i = del_f_del_theta_i(num_params, X_test[privileged_idx[i]], y_pred[privileged_idx[i]])\n",
    "            del_f_privileged = np.add(del_f_privileged, del_f_i)\n",
    "    del_f_privileged /= actual_positive_privileged\n",
    "    \n",
    "    actual_positive_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            actual_positive_protected += 1\n",
    "#             if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "            del_f_i = del_f_del_theta_i(num_params, X_test[protected_idx[i]], y_pred[protected_idx[i]])\n",
    "            del_f_protected = np.add(del_f_protected, del_f_i)\n",
    "    del_f_protected /= actual_positive_protected\n",
    "\n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing $v=\\nabla($Predictive parity difference$)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return v = del(Predictive_parity)/del(theta)\n",
    "def del_predictive_parity_del_theta(num_params, X_test_orig, X_test, y_pred, y_test):\n",
    "    del_f_protected = np.zeros((num_params, 1))\n",
    "    del_f_privileged = np.zeros((num_params, 1))\n",
    "    \n",
    "    protected_idx = X_test_orig[X_test_orig['age']==0].index\n",
    "    privileged_idx = X_test_orig[X_test_orig['age']==1].index\n",
    "\n",
    "    u_dash_protected = np.zeros((num_params, 1))\n",
    "    v_protected = 0\n",
    "    v_dash_protected = np.zeros((num_params, 1))\n",
    "    u_protected = 0\n",
    "    for i in range(len(protected_idx)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[protected_idx[i]], y_pred[protected_idx[i]])\n",
    "#         if (y_pred[protected_idx[i]][1] > y_pred[protected_idx[i]][0]):\n",
    "        v_protected += y_pred[protected_idx[i]][1]\n",
    "        v_dash_protected = np.add(v_dash_protected, del_f_i)\n",
    "        if (y_test[protected_idx[i]] == 1):\n",
    "            u_dash_protected = np.add(u_dash_protected, del_f_i)\n",
    "            u_protected += y_pred[protected_idx[i]][1]\n",
    "    del_f_protected = (u_dash_protected * v_protected - u_protected * v_dash_protected)/(v_protected * v_protected)\n",
    "    \n",
    "    u_dash_privileged = np.zeros((num_params, 1))\n",
    "    v_privileged = 0\n",
    "    v_dash_privileged = np.zeros((num_params, 1))\n",
    "    u_privileged = 0\n",
    "    for i in range(len(privileged_idx)):\n",
    "        del_f_i = del_f_del_theta_i(num_params, X_test[privileged_idx[i]], y_pred[privileged_idx[i]])\n",
    "#         if (y_pred[privileged_idx[i]][1] > y_pred[privileged_idx[i]][0]):\n",
    "        v_privileged += y_pred[privileged_idx[i]][1]\n",
    "        v_dash_privileged = np.add(v_dash_privileged, del_f_i)\n",
    "        if (y_test[privileged_idx[i]] == 1):\n",
    "            u_dash_privileged = np.add(u_dash_privileged, del_f_i)\n",
    "            u_privileged += y_pred[privileged_idx[i]][1]\n",
    "    del_f_privileged = (u_dash_privileged * v_privileged - u_privileged * v_dash_privileged)/(v_privileged * v_privileged)\n",
    "    \n",
    "    v = np.subtract(del_f_protected, del_f_privileged)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic estimation of Hessian vector product (involving del fairness): $H_{\\theta}^{-1}v = H_{\\theta}^{-1}\\nabla_{\\theta}f(z, \\theta) = v + [I - \\nabla_{\\theta}^2L(z_{s_j}, \\theta^*)]H_{\\theta}^{-1}v$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformly sample t points from training data \n",
    "def hessian_vector_product(num_params, n, size, v, hessian_all_points):\n",
    "    if (size > n):\n",
    "        size = n\n",
    "    sample = random.sample(range(n), size)\n",
    "    hinv_v = copy.deepcopy(v)\n",
    "    for idx in range(size):\n",
    "        i = sample[idx]\n",
    "        hessian_i = hessian_all_points[i]\n",
    "        hinv_v = np.matmul(np.subtract(np.identity(num_params), hessian_i), hinv_v)\n",
    "        hinv_v = np.add(hinv_v, v)\n",
    "    return hinv_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order_influence(del_L_del_theta, hinv_v, n):\n",
    "    infs = []\n",
    "    for i in range(n):\n",
    "        inf = -np.dot(del_L_del_theta[i].transpose(), hinv_v)\n",
    "        inf *= -1/n\n",
    "        infs.append(inf[0][0].tolist())\n",
    "    return infs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second-order influence computation for a group of points in subset U**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_influence(X_train, U, size, del_L_del_theta, hessian_all_points):\n",
    "    u = len(U)\n",
    "    s = len(X_train)\n",
    "    p = u/s\n",
    "    c1 = (1 - 2*p)/(s * (1-p)**2)\n",
    "    c2 = 1/((s * (1-p))**2)\n",
    "    num_params = len(del_L_del_theta[0])\n",
    "    del_L_del_theta_hinv = np.zeros((num_params, 1))\n",
    "    del_L_del_theta_sum = np.zeros((num_params, 1))\n",
    "    hessian_U = np.zeros((num_params, num_params))\n",
    "    for i in range(u):\n",
    "        idx = U[i]\n",
    "        hessian_U = np.add(hessian_U, s * hessian_all_points[idx])\n",
    "        del_L_del_theta_sum = np.add(del_L_del_theta_sum, del_L_del_theta[idx])\n",
    "    \n",
    "    hinv_del_L_del_theta= np.matmul(hinv_exact, del_L_del_theta_sum)\n",
    "    hinv_hessian_U = np.matmul(hinv_exact, hessian_U)\n",
    "    term1 = c1 * hinv_del_L_del_theta\n",
    "    term2 = c2 * np.matmul(hinv_hessian_U, hinv_del_L_del_theta)\n",
    "    sum_term = np.add(term1, term2)\n",
    "    return sum_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics: Initial state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial statistical parity:  -0.11218759952324076\n",
      "Initial TPR parity:  -0.08639444415122699\n",
      "Initial predictive parity:  -0.09396577739948753\n",
      "Initial loss:  0.5063649711386483\n",
      "Initial accuracy:  0.755\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "num_params = len(clf.coef_.transpose()) + 1 #weights and intercept; params: clf.coef_, clf.intercept_\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "y_pred_train = clf.predict_proba(X_train)\n",
    "\n",
    "spd_0 = computeFairness(y_pred_test, X_test_orig, y_test, 0)\n",
    "print(\"Initial statistical parity: \", spd_0)\n",
    "\n",
    "tpr_parity_0 = computeFairness(y_pred_test, X_test_orig, y_test, 1)\n",
    "print(\"Initial TPR parity: \", tpr_parity_0)\n",
    "\n",
    "predictive_parity_0 = computeFairness(y_pred_test, X_test_orig, y_test, 2)\n",
    "print(\"Initial predictive parity: \", predictive_parity_0)\n",
    "\n",
    "loss_0 = logistic_loss(y_test, y_pred_test)\n",
    "print(\"Initial loss: \", loss_0)\n",
    "\n",
    "accuracy_0 = computeAccuracy(y_test, y_pred_test)\n",
    "print(\"Initial accuracy: \", accuracy_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-compute: (1) Hessian (2) del_L_del_theta for each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_L_del_theta = []\n",
    "for i in range(int(len(X_train))):\n",
    "    del_L_del_theta.insert(i, del_L_del_theta_i(num_params, y_train[i], X_train[i], y_pred_train[i]))\n",
    "\n",
    "hessian_all_points = []\n",
    "for i in range(len(X_train)):\n",
    "    hessian_all_points.insert(i, hessian_one_point(num_params, X_train[i], y_pred_train[i])\n",
    "                              /len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Select delta fairness function depending on selected metric*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 0\n",
    "if metric == 0:\n",
    "    v1 = del_spd_del_theta(num_params, X_test_orig, X_test, y_pred_test)\n",
    "elif metric == 1:\n",
    "    v1 = del_tpr_parity_del_theta(num_params, X_test_orig, X_test, y_pred_test, y_test)\n",
    "elif metric == 2:\n",
    "    v1 = del_predictive_parity_del_theta(num_params, X_test_orig, X_test, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*H^{-1} computation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexact = 1\n",
    "if hexact == 1: \n",
    "    H_exact = np.zeros((num_params, num_params))\n",
    "    for i in range(len(X_train)):\n",
    "        H_exact = np.add(H_exact, hessian_all_points[i])\n",
    "    hinv_exact = np.linalg.pinv(H_exact) \n",
    "    hinv_v = np.matmul(hinv_exact, v1)\n",
    "else: #using Hessian vector product\n",
    "    size_hvp = int(len(X_train) * .01)\n",
    "    hinv_v = hessian_vector_product(num_params, len(X_train), size_hvp, v1, hessian_all_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence of each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "infs_1 = first_order_influence(del_L_del_theta, hinv_v, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking ground truth, first-order and second-order influences for a set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical parity \n",
      "#Rows removed:  67\n",
      "#Rows left:  733\n",
      "Ground truth influence of subset (on statistical parity):  0.0450982036707821\n",
      "Ground truth influence of subset (on tpr parity):  0.021053889053461172\n",
      "Ground truth influence of subset (on predictive parity):  0.019288838268330277\n",
      "First-order influence:  0.04166645112626196\n",
      "Second-order influence:  0.047501068997333265\n",
      "Ground truth statistical parity after removing subset:  -0.08610961744429924\n",
      "Ground truth tpr parity after removing subset:  -0.23207230495614894\n",
      "Ground truth predictive parity after removing subset:  -0.23207230495614894\n",
      "Loss after removing subset:  0.4968544758453933\n",
      "Accuracy after removing subset:  0.755\n"
     ]
    }
   ],
   "source": [
    "if metric == 0:\n",
    "    print(\"Statistical parity \")\n",
    "elif metric == 1:\n",
    "    print(\"True positive rate parity \")\n",
    "elif metric == 2:\n",
    "    print(\"Predictive parity\")\n",
    "    \n",
    "active = 1\n",
    "if active:\n",
    "    predicates = [ 'age']\n",
    "    idx=X_train_orig.index \n",
    "    for pred in predicates:\n",
    "       idx0 = X_train_orig[(X_train_orig[pred] == 1)].index \n",
    "       idx=idx.intersection(idx0)\n",
    "      \n",
    "    print(\"#Rows removed: \", len(idx))\n",
    "    print(\"#Rows left: \", len(X_train) - len(idx))\n",
    "    X = np.delete(X_train, idx, 0)\n",
    "    y = y_train.drop(index=idx, inplace=False)\n",
    "    clf.fit(X, y)\n",
    "    y_pred_test = clf.predict_proba(X_test)\n",
    "    print(\"Ground truth influence of subset (on statistical parity): \", computeFairness(y_pred_test, X_test_orig, y_test, 0) - spd_0)\n",
    "    print(\"Ground truth influence of subset (on tpr parity): \", computeFairness(y_pred_test, X_test_orig, y_test, 1) - tpr_parity_0)\n",
    "    print(\"Ground truth influence of subset (on predictive parity): \", computeFairness(y_pred_test, X_test_orig, y_test, 2) - predictive_parity_0)\n",
    "\n",
    "    del_f_1 = 0\n",
    "    for i in range(len(idx)):\n",
    "        del_f_1 += infs_1[idx[i]]\n",
    "    print(\"First-order influence: \", del_f_1)\n",
    "\n",
    "    size_hvp = 1\n",
    "    params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "    del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "    print(\"Second-order influence: \", del_f_2)\n",
    "    \n",
    "    spd_1 = computeFairness(y_pred_test, X_test_orig, y_test, 0)\n",
    "    print(\"Ground truth statistical parity after removing subset: \", spd_1)\n",
    "    \n",
    "    tpr_parity_1 = computeFairness(y_pred_test, X_test_orig, y_test, 1)\n",
    "    print(\"Ground truth tpr parity after removing subset: \", tpr_parity_1)\n",
    "\n",
    "    predictive_parity_1 = computeFairness(y_pred_test, X_test_orig, y_test, 1)\n",
    "    print(\"Ground truth predictive parity after removing subset: \", predictive_parity_1)\n",
    "\n",
    "    loss_1 = logistic_loss(y_test, y_pred_test)\n",
    "    print(\"Loss after removing subset: \", loss_1)\n",
    "\n",
    "    accuracy_1 = computeAccuracy(y_test, y_pred_test)\n",
    "    print(\"Accuracy after removing subset: \", accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.20044233691822827 -0.17393358242162493 0.3597551530603338 0.8310092961487384\n",
      "-0.1225835505238218 -0.01100904749147602 0.3789946670357515 0.8205179282868525\n"
     ]
    }
   ],
   "source": [
    "print(spd_0, tpr_parity_0, preditive_parity_0, loss_0, accuracy_0)\n",
    "print(spd_1, tpr_parity_1, predictive_parity_1, loss_1, accuracy_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fairness: Ground-truth subset influence vs. computed subset influences: Random subset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-truth subset, Add 1st-order inf individual, Second-order subset influence\n"
     ]
    }
   ],
   "source": [
    "print(\"Ground-truth subset, Add 1st-order inf individual, Second-order subset influence\")\n",
    "sampleSize = int(.2 * len(X_train))\n",
    "for i in range(100):\n",
    "    idx = random.sample(range(1, len(X_train)), sampleSize) \n",
    "    \n",
    "    # Ground truth subset influence\n",
    "    X = np.delete(X_train, idx, 0)\n",
    "    y = y_train.drop(index=idx, inplace=False)\n",
    "    clf.fit(X, y)\n",
    "    y_pred_test = clf.predict_proba(X_test)\n",
    "    inf_gt = computeFairness(y_pred_test, X_test_orig, y_test, 0) - spd_0\n",
    "    \n",
    "    # First-order subset influence\n",
    "    del_f_1 = 0\n",
    "    for i in range(len(idx)):\n",
    "        del_f_1 += infs_1[idx[i]]\n",
    "    \n",
    "    # Second-order subset influence\n",
    "    size_hvp = 1\n",
    "    params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "    del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "    \n",
    "#     print(inf_gt, del_f_1, del_f_2, sep=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fairness: Ground-truth subset influence vs. computed subset influences: Coherent subset** \n",
    "\n",
    "(by coherent, we mean group of data points that share some properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute, Value, Ground-truth subset, Add 1st-order inf individual, Second-order subset influence, %rowsRemoved, Accuracy\n"
     ]
    }
   ],
   "source": [
    "attributes = []\n",
    "attributeValues = []\n",
    "second_order_influences = []\n",
    "gt_influences = []\n",
    "fractionRows = []\n",
    "\n",
    "print(\"Attribute, Value, Ground-truth subset, Add 1st-order inf individual, \\\n",
    "Second-order subset influence, %rowsRemoved, Accuracy\")\n",
    "clf.fit(X_train, y_train)\n",
    "continuous_cols = ['duration', 'credit_amt', 'install_rate', 'num_credits', 'residence']\n",
    "for col in X_train_orig.columns:\n",
    "    if \"purpose\" in col or \"housing\" in col: #dummy variables purpose=0 doesn't make sense\n",
    "        vals = [1]\n",
    "    else:\n",
    "        vals = X_train_orig[col].unique()\n",
    "    for val in vals:\n",
    "#         print(col, val, sep=\": \")\n",
    "        idx = X_train_orig[X_train_orig[col] == val].index \n",
    "    \n",
    "        X = np.delete(X_train, idx, 0)\n",
    "        y = y_train.drop(index=idx, inplace=False)\n",
    "        inf_gt = 0\n",
    "        if len(y.unique()) > 1:\n",
    "            # Ground truth subset influence\n",
    "            clf.fit(X, y)\n",
    "            y_pred = clf.predict_proba(X_test)\n",
    "            if metric == 0:\n",
    "                inf_gt = computeFairness(y_pred, X_test_orig, y_test, 0) - spd_0\n",
    "            elif metric == 1:\n",
    "                inf_gt = computeFairness(y_pred, X_test_orig, y_test, 1) - tpr_parity_0\n",
    "            elif metric == 2:\n",
    "                inf_gt = computeFairness(y_pred, X_test_orig, y_test, 2) - predictive_parity_0\n",
    "            accuracy = computeAccuracy(y_test, y_pred)\n",
    "\n",
    "        # First-order subset influence\n",
    "        del_f_1 = 0            \n",
    "        for i in range(len(idx)):\n",
    "            del_f_1 += infs_1[idx[i]]\n",
    "\n",
    "        # Second-order subset influence\n",
    "        size_hvp = 1\n",
    "        params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "        del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "        \n",
    "        attributes.append(col)\n",
    "        attributeValues.append(val)\n",
    "        second_order_influences.append(del_f_2)\n",
    "        gt_influences.append(inf_gt)\n",
    "        fractionRows.append(len(idx)/len(X_train)*100)\n",
    "\n",
    "#         print(col, val, inf_gt, del_f_1, del_f_2, len(idx)/len(X_train), accuracy, sep=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl = [attributes, attributeValues, second_order_influences, gt_influences, fractionRows]\n",
    "expl = (np.array(expl).T).tolist()\n",
    "\n",
    "explanations = pd.DataFrame(expl, columns=[\"attributes\", \"attributeValues\", \"second_order_influences\", \"gt_influences\", \"fractionRows\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attributes</th>\n",
       "      <th>attributeValues</th>\n",
       "      <th>second_order_influences</th>\n",
       "      <th>gt_influences</th>\n",
       "      <th>fractionRows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>savings</td>\n",
       "      <td>0</td>\n",
       "      <td>0.07619667982380768</td>\n",
       "      <td>0.07478872604381981</td>\n",
       "      <td>60.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>num_liable</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07175791697987433</td>\n",
       "      <td>0.04176676869998219</td>\n",
       "      <td>85.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>0.04703390838653976</td>\n",
       "      <td>0.046539293316492425</td>\n",
       "      <td>32.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>residence</td>\n",
       "      <td>2</td>\n",
       "      <td>0.046417655184498426</td>\n",
       "      <td>0.047997849002136994</td>\n",
       "      <td>31.125000000000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>telephone</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040965146335371794</td>\n",
       "      <td>0.03787372419117385</td>\n",
       "      <td>39.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>install_rate</td>\n",
       "      <td>4</td>\n",
       "      <td>0.03456405068845926</td>\n",
       "      <td>0.035841080151672866</td>\n",
       "      <td>47.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>credit_amt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03425846721689816</td>\n",
       "      <td>0.034012275816776905</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>num_credits</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029161295948363305</td>\n",
       "      <td>0.03297224642708385</td>\n",
       "      <td>63.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>install_plans</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028751527341856944</td>\n",
       "      <td>0.023639925134487982</td>\n",
       "      <td>19.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>purpose_A43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02690725910373638</td>\n",
       "      <td>0.028026002147808815</td>\n",
       "      <td>28.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       attributes attributeValues second_order_influences  \\\n",
       "19        savings               0     0.07619667982380768   \n",
       "53     num_liable               1     0.07175791697987433   \n",
       "69         gender               0     0.04703390838653976   \n",
       "36      residence               2    0.046417655184498426   \n",
       "56      telephone               1    0.040965146335371794   \n",
       "26   install_rate               4     0.03456405068845926   \n",
       "13     credit_amt               0     0.03425846721689816   \n",
       "45    num_credits               1    0.029161295948363305   \n",
       "44  install_plans               1    0.028751527341856944   \n",
       "63    purpose_A43               1     0.02690725910373638   \n",
       "\n",
       "           gt_influences        fractionRows  \n",
       "19   0.07478872604381981               60.25  \n",
       "53   0.04176676869998219                85.5  \n",
       "69  0.046539293316492425              32.125  \n",
       "36  0.047997849002136994  31.125000000000004  \n",
       "56   0.03787372419117385              39.625  \n",
       "26  0.035841080151672866              47.625  \n",
       "13  0.034012275816776905                44.0  \n",
       "45   0.03297224642708385              63.125  \n",
       "44  0.023639925134487982                19.5  \n",
       "63  0.028026002147808815              28.125  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations.sort_values(by=['second_order_influences'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attributes</th>\n",
       "      <th>attributeValues</th>\n",
       "      <th>second_order_influences</th>\n",
       "      <th>gt_influences</th>\n",
       "      <th>fractionRows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>foreign_worker</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01475263293101025</td>\n",
       "      <td>0.08775761281533401</td>\n",
       "      <td>96.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>savings</td>\n",
       "      <td>0</td>\n",
       "      <td>0.07619667982380768</td>\n",
       "      <td>0.07478872604381981</td>\n",
       "      <td>60.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>residence</td>\n",
       "      <td>2</td>\n",
       "      <td>0.046417655184498426</td>\n",
       "      <td>0.047997849002136994</td>\n",
       "      <td>31.125000000000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>0.04703390838653976</td>\n",
       "      <td>0.046539293316492425</td>\n",
       "      <td>32.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>age</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.020617479129422172</td>\n",
       "      <td>0.0440812243156512</td>\n",
       "      <td>79.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>num_liable</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07175791697987433</td>\n",
       "      <td>0.04176676869998219</td>\n",
       "      <td>85.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>telephone</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040965146335371794</td>\n",
       "      <td>0.03787372419117385</td>\n",
       "      <td>39.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>install_rate</td>\n",
       "      <td>4</td>\n",
       "      <td>0.03456405068845926</td>\n",
       "      <td>0.035841080151672866</td>\n",
       "      <td>47.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>credit_amt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03425846721689816</td>\n",
       "      <td>0.034012275816776905</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>num_credits</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029161295948363305</td>\n",
       "      <td>0.03297224642708385</td>\n",
       "      <td>63.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        attributes attributeValues second_order_influences  \\\n",
       "57  foreign_worker               1     0.01475263293101025   \n",
       "19         savings               0     0.07619667982380768   \n",
       "36       residence               2    0.046417655184498426   \n",
       "69          gender               0     0.04703390838653976   \n",
       "41             age               0   -0.020617479129422172   \n",
       "53      num_liable               1     0.07175791697987433   \n",
       "56       telephone               1    0.040965146335371794   \n",
       "26    install_rate               4     0.03456405068845926   \n",
       "13      credit_amt               0     0.03425846721689816   \n",
       "45     num_credits               1    0.029161295948363305   \n",
       "\n",
       "           gt_influences        fractionRows  \n",
       "57   0.08775761281533401               96.25  \n",
       "19   0.07478872604381981               60.25  \n",
       "36  0.047997849002136994  31.125000000000004  \n",
       "69  0.046539293316492425              32.125  \n",
       "41    0.0440812243156512              79.875  \n",
       "53   0.04176676869998219                85.5  \n",
       "56   0.03787372419117385              39.625  \n",
       "26  0.035841080151672866              47.625  \n",
       "13  0.034012275816776905                44.0  \n",
       "45   0.03297224642708385              63.125  "
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations.sort_values(by=['gt_influences'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "explanations[\"gt_rank\"] = len(explanations) - ss.rankdata(explanations[\"gt_influences\"])\n",
    "explanations[\"so_rank\"] = len(explanations) - ss.rankdata(explanations[\"second_order_influences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.7993335801554979, pvalue=7.091127095427948e-24)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kendalltau(explanations[\"gt_rank\"], explanations[\"so_rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
